diff --git "a/C:\\amber\\amber_mint\\src\\pmemd\\src/.ford.md" "b/C:\\amber\\amber_mint\\src\\pmemd\\src/.ford.md"
new file mode 100644
index 0000000..1e7a694
--- /dev/null
+++ "b/C:\\amber\\amber_mint\\src\\pmemd\\src/.ford.md"
@@ -0,0 +1,7 @@
+project: MyFortranProject
+project_dir: .
+output_dir: ./ford-docs
+source_dir: .
+graph: true
+print_graph: true
+graph_format: dot
diff --git "a/C:\\amber\\amber_mint\\src\\pmemd\\src/cuda/amber_claude.code-workspace" "b/C:\\amber\\amber_mint\\src\\pmemd\\src/cuda/amber_claude.code-workspace"
new file mode 100644
index 0000000..ab3edf7
--- /dev/null
+++ "b/C:\\amber\\amber_mint\\src\\pmemd\\src/cuda/amber_claude.code-workspace"
@@ -0,0 +1,7 @@
+{
+	"folders": [
+		{
+			"path": "../../../.."
+		}
+	]
+}
\ No newline at end of file
diff --git "a/C:\\amber\\amber_master\\src\\pmemd\\src/cuda/base_gpuContext.cpp" "b/C:\\amber\\amber_mint\\src\\pmemd\\src/cuda/base_gpuContext.cpp"
index 17c1ae2..31f7a9b 100644
--- "a/C:\\amber\\amber_master\\src\\pmemd\\src/cuda/base_gpuContext.cpp"
+++ "b/C:\\amber\\amber_mint\\src\\pmemd\\src/cuda/base_gpuContext.cpp"
@@ -24,7 +24,7 @@ base_gpuContext::base_gpuContext() :
   ischeme(0), ithermostat(0), 
 
   // Atom data, in single and double precision
-  pbAtom(NULL), pbAtomXYSP(NULL), pbAtomZSP(NULL), pbAtomSigEps(NULL), pbAtomLJID(NULL),
+  pbAtom(NULL), pbAtomCoord_M32(NULL), pbAtomSigEps(NULL), pbAtomLJID(NULL),
   pbAtomCharge(NULL), pbAtomChargeSP(NULL), pbAtomChargeSPLJID(NULL), pbAtomRBorn(NULL),
   pbAtomS(NULL), pbAtomMass(NULL), pbCenter(NULL), pbR4sources(NULL),
   pbAtomIonMask(NULL),
@@ -127,7 +127,9 @@ base_gpuContext::base_gpuContext() :
 #endif
   pblliXYZ_q(NULL), pbXYZ_q(NULL), pbXYZ_qt(NULL),
   bNeighborList(false), bNeedNewNeighborList(true), bNewNeighborList(false), bSmallBox(false),
-  bOddNLCells(false), neighborListBits(32), pbAtomXYSaveSP(NULL), pbAtomZSaveSP(NULL),
+  bOddNLCells(false), neighborListBits(32), pbAtomCoordSave_M32(NULL), pbOldAtomCoord_M32(NULL),
+  pbOldAtomXYSP(NULL), pbOldAtomZSP(NULL), pbAtomXYSP(NULL), pbAtomZSP(NULL),
+  pbAtomCoordUnsorted_M32(NULL), pbOldAtomCoordUnsorted_M32(NULL),
   pbImage(NULL), pbImageIndex(NULL), pbSortTemp(NULL), pbSubImageLookup(NULL), pbImageVel(NULL),
   pbImageLVel(NULL), pbImageMass(NULL), pbImageCharge(NULL), pbImageSigEps(NULL),
   pbImageLJID(NULL), pbImageCellID(NULL), pbImageTIRegion(NULL), pbImageTILinearAtmID(NULL),
@@ -204,8 +206,7 @@ base_gpuContext::~base_gpuContext()
   int i;
   // Delete Atom data
   delete pbAtom;
-  delete pbAtomXYSP;
-  delete pbAtomZSP;
+  delete pbAtomCoord_M32;
   delete pbAtomSigEps;
   delete pbAtomLJID;
   delete pbAtomRBorn;
@@ -261,8 +262,8 @@ base_gpuContext::~base_gpuContext()
 #endif
 
   // Delete neighbor list stuff
-  delete pbAtomXYSaveSP;
-  delete pbAtomZSaveSP;
+  delete pbAtomCoordSave_M32;
+  delete pbOldAtomCoord_M32;
   delete pbImage;
   delete pbSortTemp;
   delete pbImageIndex;
@@ -614,10 +615,8 @@ base_gpuContext::~base_gpuContext()
   cudaDestroyTextureObject(sim.texXYZ_q);
   cudaDestroyTextureObject(sim.texOldAtomX);
 #if !defined(use_DPFP)
-  cudaDestroyTextureObject(sim.texAtomXYSP);
   cudaDestroyTextureObject(sim.texAtomChargeSPLJID);
   cudaDestroyTextureObject(sim.texLJTerm);
-  cudaDestroyTextureObject(sim.texAtomZSP);
   cudaDestroyTextureObject(sim.texErfcCoeffsTable);
 #endif
 }
diff --git "a/C:\\amber\\amber_master\\src\\pmemd\\src/cuda/base_gpuContext.h" "b/C:\\amber\\amber_mint\\src\\pmemd\\src/cuda/base_gpuContext.h"
index 9b7c51d..44c6667 100644
--- "a/C:\\amber\\amber_master\\src\\pmemd\\src/cuda/base_gpuContext.h"
+++ "b/C:\\amber\\amber_mint\\src\\pmemd\\src/cuda/base_gpuContext.h"
@@ -46,8 +46,9 @@ public:
 
   // Atom stuff
   GpuBuffer<double>*         pbAtom;              // Atom coordinates
-  GpuBuffer<PMEFloat2>*      pbAtomXYSP;          // Single Precision Atom X and Y coordinates
-  GpuBuffer<PMEFloat>*       pbAtomZSP;           // Single Precision Atom Z coordinate
+  GpuBuffer<int4>*           pbAtomCoord_M32;         // MINT32 Atom Coordinates (x, y, z, pad)
+  GpuBuffer<PMEFloat2>*      pbAtomXYSP;          // FP32 fractional coords XY (for NL build)
+  GpuBuffer<PMEFloat>*       pbAtomZSP;           // FP32 fractional coords Z (for NL build)
   GpuBuffer<PMEFloat2>*      pbAtomSigEps;        // Atom nonbond parameters
   GpuBuffer<unsigned int>*   pbAtomLJID;          // Atom Lennard Jone index
   GpuBuffer<PMEFloat>*       pbAtomS;             // Atom scaled Born Radius
@@ -143,8 +144,12 @@ public:
                                               //   NOT the pointers pQQSubImageSP and
                                               //   pLJSubImageSP (see above, in cudaSimulation)
                                               //   that point to each half of it!
-  GpuBuffer<PMEFloat2>*    pbAtomXYSaveSP;    // Saved neighbor list coordinates
-  GpuBuffer<PMEFloat>*     pbAtomZSaveSP;     // Saved neighbor list coordinates
+  GpuBuffer<int4>*         pbAtomCoordSave_M32;      // Saved neighbor list coordinates
+    GpuBuffer<int4>*         pbAtomCoordUnsorted_M32; // MINT32 coords in original atom order (Array A)
+    GpuBuffer<int4>*         pbOldAtomCoord_M32;       // Old MINT32 coordinate buffer
+    GpuBuffer<int4>*         pbOldAtomCoordUnsorted_M32; // Old MINT32 coords in original atom order (Array A)
+  GpuBuffer<PMEFloat2>*    pbOldAtomXYSP;     // Old FP32 coords XY (for NL build)
+  GpuBuffer<PMEFloat>*     pbOldAtomZSP;      // Old FP32 coords Z (for NL build)
   GpuBuffer<double>*       pbImage;           // Image coordinates
   GpuBuffer<double>*       pbImageVel;        // Image velocities
   GpuBuffer<double>*       pbImageLVel;       // Image last velocities
@@ -779,4 +784,5 @@ public:
   FILE* pPPVelocity;        // Pointer to velocity output file
 #endif
 };
+
 #endif //_BASE_GPU_CONTEXT
diff --git "a/C:\\amber\\amber_master\\src\\pmemd\\src/cuda/base_simulationConst.cpp" "b/C:\\amber\\amber_mint\\src\\pmemd\\src/cuda/base_simulationConst.cpp"
index 6ee2148..fa53229 100644
--- "a/C:\\amber\\amber_master\\src\\pmemd\\src/cuda/base_simulationConst.cpp"
+++ "b/C:\\amber\\amber_mint\\src\\pmemd\\src/cuda/base_simulationConst.cpp"
@@ -118,9 +118,8 @@ void base_simulationConst::InitSimulationConst()
   paddedNumberOfAtoms          = 0;
   pAtomX                       = NULL;
   pAtomY                       = NULL;
-  pAtomXYSP                    = NULL;
-  pAtomZ                       = NULL;
-  pAtomZSP                     = NULL;
+  pAtomCoord_M32                   = NULL;
+
   pAtomSigEps                  = NULL;
   pAtomLJID                    = NULL;
   pAtomRBorn                   = NULL;
@@ -507,8 +506,7 @@ void base_simulationConst::InitSimulationConst()
   pImageAtom2                  = NULL;
   pImageAtomLookup             = NULL;
   pImageHash                   = NULL;
-  pAtomXYSaveSP                = NULL;
-  pAtomZSaveSP                 = NULL;
+  pAtomCoordSave_M32               = NULL;
   pImageX                      = NULL;
   pImageY                      = NULL;
   pImageZ                      = NULL;
@@ -557,6 +555,7 @@ void base_simulationConst::InitSimulationConst()
   pOldAtomX                    = NULL;
   pOldAtomY                    = NULL;
   pOldAtomZ                    = NULL;
+  pOldAtomCoord_M32             = NULL;
   pShakeOldAtomX               = NULL;
   pShakeOldAtomY               = NULL;
   pShakeOldAtomZ               = NULL;
diff --git "a/C:\\amber\\amber_master\\src\\pmemd\\src/cuda/base_simulationConst.h" "b/C:\\amber\\amber_mint\\src\\pmemd\\src/cuda/base_simulationConst.h"
index c020749..2812aa0 100644
--- "a/C:\\amber\\amber_master\\src\\pmemd\\src/cuda/base_simulationConst.h"
+++ "b/C:\\amber\\amber_mint\\src\\pmemd\\src/cuda/base_simulationConst.h"
@@ -28,6 +28,7 @@ public:
   int igb;                          // Generalized Born overall setting
   int icnstph;                      // constant pH flag
   int icnste;                       // constant Redox potential flag
+  int bFrozenTest;                  // MINT32_FROZEN_TEST mode for debugging
   int hybridgb;                     // Hybrid Solvent REMD flag
   int ti_mode;                      // Softcore TI mode
   double scnb;                      // 1-4 nonbond scale factor
@@ -71,6 +72,7 @@ public:
   double c_explic;                  // Langevin integration parameter
   bool bUseVlimit;                  // Use vlimit flag
   double vlimit;                    // vlimit
+  int bFrozen;                      // MINT32 frozen mode flag
   double tol;                       // SHAKE tolerance
   double massH;                     // Hydrogen mass
   aligned_double invMassH;          // Inverse hydrogen mass
@@ -98,6 +100,8 @@ public:
   PMEFloat surften;                 // GB surface tension
   PMEFloat ala;			//TFE for alanine
   PMEFloat arg;			//TFE for arginine
+  double3 mint32_scale;     // MINT32 scaling factors (per dimension)
+  double3 mint32_inv_scale; // MINT32 inverse scaling factors (per dimension)
   PMEFloat asn;			//TFE for asparagine
   PMEFloat asp;			//TFE for aspartate
   PMEFloat cys;			//TFE for cysteine
@@ -416,11 +420,13 @@ public:
   double* pOldAtomX;               // Old Atom X position
   double* pOldAtomY;               // Old Atom Y position
   double* pOldAtomZ;               // Old Atom Z position
+  int4* pOldAtomCoord_M32;              // Old MINT32 atom coordinates (previous timestep for SHAKE)
   double* pShakeOldAtomX;          // Old Atom X position added by zhf, used by middle-scheme
   double* pShakeOldAtomY;          // Old Atom Y position velocity recalculation
   double* pShakeOldAtomZ;          // Old Atom Z position
-  PMEFloat2* pAtomXYSP;            // Single Precision Atom X and Y coordinates
-  PMEFloat* pAtomZSP;              // Single Precision Atom Z coordinates
+  int4* pAtomCoord_M32;                // MINT32 Atom Coordinates (x, y, z, pad)
+  PMEFloat2* pAtomXYSP;            // FP32 fractional coords XY (for NL build)
+  PMEFloat* pAtomZSP;              // FP32 fractional coords Z (for NL build)
   PMEFloat2* pAtomSigEps;          // Atom nonbond parameters
   unsigned int* pAtomLJID;         // Atom Lennard-Jones index
   PMEFloat* pAtomS;                // Atom S Parameter
@@ -472,8 +478,11 @@ public:
   unsigned int* pImageAtom;       // Image atom #
   unsigned int* pImageAtom2;      // Image atom #
   unsigned int* pImageAtomLookup; // Original atom lookup table
-  PMEFloat2* pAtomXYSaveSP;       // Saved atom coordinates from neighbor list generation
-  PMEFloat* pAtomZSaveSP;         // Saved atom coordinates from neighbor list generation
+  int4* pAtomCoordSave_M32;           // Saved MINT32 atom coordinates from neighbor list generation
+  int4* pAtomCoordUnsorted_M32;       // MINT32 coords in original atom order (Array A) - never sorted
+  int4* pOldAtomCoordUnsorted_M32;    // Old MINT32 coords in original atom order (Array A) - never sorted
+  PMEFloat2* pOldAtomXYSP;        // Old FP32 coords XY (for NL rebuild check)
+  PMEFloat* pOldAtomZSP;          // Old FP32 coords Z (for NL rebuild check)
   double* pImageX;                // Image x coordinates
   double* pImageY;                // Image y coordinates
   double* pImageZ;                // Image z coordinates
@@ -1385,4 +1394,11 @@ public:
   int* pImageIndexPHMD2;                      //Pointer to image indices
 };
 
+// Compatibility aliases for legacy names (pre-_M32 rename)
+#define pAtomCoord               pAtomCoord_M32
+#define pAtomCoordSave           pAtomCoordSave_M32
+#define pAtomCoordUnsorted       pAtomCoordUnsorted_M32
+#define pOldAtomCoord            pOldAtomCoord_M32
+#define pOldAtomCoordUnsorted    pOldAtomCoordUnsorted_M32
+
 #endif // _BASE_CUDA_SIMULATION
diff --git "a/C:\\amber\\amber_master\\src\\pmemd\\src/cuda/gpu.cpp" "b/C:\\amber\\amber_mint\\src\\pmemd\\src/cuda/gpu.cpp"
index f8bcc47..539e386 100644
--- "a/C:\\amber\\amber_master\\src\\pmemd\\src/cuda/gpu.cpp"
+++ "b/C:\\amber\\amber_mint\\src\\pmemd\\src/cuda/gpu.cpp"
@@ -12,6 +12,7 @@
 #include <iostream>
 #include <fstream>
 #include <ctime>
+#include <climits>
 #include <math.h>
 #include <string.h>
 #include <algorithm>
@@ -26,6 +27,20 @@
 #ifdef GTI
 #include "gti_cuda.cuh"
 #endif
+
+extern "C" void Mint32SampleInit(int enable);
+extern "C" void Mint32SampleDump();
+extern "C" void Mint32NLChecksumInit(int enable);
+extern "C" void Mint32NLChecksumDump();
+extern __device__ int dMint32BNLDebug[4];
+
+// MINT32 debugging instrumentation
+#include "mint32_debug.h"
+
+#ifndef MINT32_NL_CHECKSUM_ENABLED
+#define MINT32_NL_CHECKSUM_ENABLED 0
+#endif
+
 namespace gppImpl {
   static gpuContext gpu = NULL;
 }
@@ -33,6 +48,241 @@ namespace gppImpl {
 using namespace std;
 using namespace gppImpl;
 
+// Snapshot of pre-remap charge/LJ for mapping sanity checks
+static std::vector<PMEFloat2> gMint32OrigChargeLJ;
+
+static inline double wrap_centered(double x, double box)
+{
+  // Wrap to [-box/2, box/2)
+  // Use x - box * floor(x/box + 0.5) to avoid drifting due to rounding.
+  return x - box * floor(x / box + 0.5);
+}
+
+static inline int mint32_from_double(double v, double scale)
+{
+  long long raw = llround(v * scale);
+  if (raw <= (long long)INT_MIN) return INT_MIN + 1;
+  if (raw >= (long long)INT_MAX) return INT_MAX - 1;
+  return (int)raw;
+}
+
+//---------------------------------------------------------------------------------------------
+// Debug helper: sample a handful of atoms/pairs to verify MINT32 coordinate and charge scaling.
+// Controlled via env var MINT32_CHECK_PAIRS (set to nonzero to enable).
+//---------------------------------------------------------------------------------------------
+static void Mint32CheckPairs(gpuContext gpu)
+{
+  const char* checkEnv = getenv("MINT32_CHECK_PAIRS");
+  if (checkEnv == nullptr || atoi(checkEnv) == 0) {
+    return;
+  }
+  if (gpu == nullptr || gpu->sim.atoms < 2) {
+    MINT32_LOG("MINT32_CHECK_PAIRS enabled but not enough atoms to sample");
+    return;
+  }
+  // Host fetch the relevant buffers
+  gpu->pbAtomCoord_M32->Download();
+  gpu->pbAtom->Download();
+  if (gpu->pbImage != nullptr) {
+    gpu->pbImage->Download();
+  }
+  if (gpu->pbImageIndex != nullptr) {
+    gpu->pbImageIndex->Download();
+  }
+  gpu->pbAtomChargeSPLJID->Download();
+  if (gpu->pbLJTerm != nullptr) {
+    gpu->pbLJTerm->Download();
+  }
+
+  const int sampleAtoms = min(4, gpu->sim.atoms);
+  // Use image coordinates when available so indices match pbAtomCoord ordering.
+  double* pFloatCrd = (gpu->pbImage != nullptr) ? gpu->pbImage->_pSysData
+                                                : gpu->pbAtom->_pSysData;
+  if (gpu->pbImage != nullptr && gpu->sim.pImageX != gpu->pbImage->_pDevData) {
+    // Device image coords live in the second half of pbImage; mirror that on host.
+    pFloatCrd += gpu->sim.stride3;
+  }
+  // Also pull device image coordinates directly from active pointers to ensure the host
+  // mirror matches whichever half of the double buffer is currently live.
+  std::vector<double> devImg;
+  double* devX = nullptr;
+  double* devY = nullptr;
+  double* devZ = nullptr;
+  bool devImgValid = false;
+  if (gpu->sim.pImageX != nullptr && gpu->sim.pImageY != nullptr && gpu->sim.pImageZ != nullptr) {
+    devImg.resize(gpu->sim.stride * 3);
+    devX = devImg.data();
+    devY = devX + gpu->sim.stride;
+    devZ = devX + gpu->sim.stride2;
+    cudaError_t cstat = cudaMemcpy(devX, gpu->sim.pImageX,
+                                   gpu->sim.stride * sizeof(double),
+                                   cudaMemcpyDeviceToHost);
+    if (cstat == cudaSuccess) {
+      cstat = cudaMemcpy(devY, gpu->sim.pImageY,
+                         gpu->sim.stride * sizeof(double),
+                         cudaMemcpyDeviceToHost);
+    }
+    if (cstat == cudaSuccess) {
+      cstat = cudaMemcpy(devZ, gpu->sim.pImageZ,
+                         gpu->sim.stride * sizeof(double),
+                         cudaMemcpyDeviceToHost);
+    }
+    if (cstat != cudaSuccess) {
+      char buf[160];
+      snprintf(buf, sizeof(buf),
+               "MINT32_CHECK_PAIRS: cudaMemcpy device images failed: %s",
+               cudaGetErrorString(cstat));
+      MINT32_LOG(buf);
+    } else {
+      devImgValid = true;
+      char buf[200];
+      snprintf(buf, sizeof(buf),
+               "MINT32_CHECK_PAIRS: device image ptrs X=%p Y=%p Z=%p hostBase=%p hostUse=%p",
+               (void*)gpu->sim.pImageX, (void*)gpu->sim.pImageY, (void*)gpu->sim.pImageZ,
+               (void*)gpu->pbImage->_pDevData, (void*)pFloatCrd);
+      MINT32_LOG(buf);
+    }
+  }
+  char buf[256];
+  snprintf(buf, sizeof(buf),
+           "MINT32_CHECK_PAIRS: box=(%.6f,%.6f,%.6f) mint32_inv_scale=(%.6g,%.6g,%.6g) atoms=%d stride=%d",
+           gpu->sim.a, gpu->sim.b, gpu->sim.c,
+           gpu->sim.mint32_inv_scale.x, gpu->sim.mint32_inv_scale.y,
+           gpu->sim.mint32_inv_scale.z, gpu->sim.atoms, gpu->sim.stride);
+  MINT32_LOG(buf);
+
+  unsigned int* pImageAtom = nullptr;
+  unsigned int* pImageAtomLookup = nullptr;
+  if (gpu->pbImageIndex != nullptr) {
+    pImageAtom        = gpu->pbImageIndex->_pSysData + gpu->sim.imageStride;
+    pImageAtomLookup  = gpu->pbImageIndex->_pSysData + gpu->sim.imageStride * 2;
+  }
+
+  for (int i = 0; i < sampleAtoms; i++) {
+    int4 ic = gpu->pbAtomCoord_M32->_pSysData[i];
+    double fx = pFloatCrd[i];
+    double fy = pFloatCrd[i + gpu->sim.stride];
+    double fz = pFloatCrd[i + gpu->sim.stride2];
+    PMEFloat2 q = gpu->pbAtomChargeSPLJID->_pSysData[i];
+    double dx = devImgValid ? devX[i] : 0.0;
+    double dy = devImgValid ? devY[i] : 0.0;
+    double dz = devImgValid ? devZ[i] : 0.0;
+    double wx = wrap_centered(fx, gpu->sim.a);
+    double wy = wrap_centered(fy, gpu->sim.b);
+    double wz = wrap_centered(fz, gpu->sim.c);
+    int origAtom = (pImageAtom != nullptr) ? (int)pImageAtom[i] : -1;
+    int mappedBack = (pImageAtomLookup != nullptr && origAtom >= 0) ? (int)pImageAtomLookup[origAtom] : -1;
+    PMEFloat2 qOrig = (origAtom >= 0 && origAtom < (int)gMint32OrigChargeLJ.size())
+                        ? gMint32OrigChargeLJ[origAtom]
+                        : ((origAtom >= 0 && origAtom < gpu->sim.atoms)
+                             ? gpu->pbAtomChargeSPLJID->_pSysData[origAtom]
+                             : q);
+    snprintf(buf, sizeof(buf),
+             " atom %d: int=(%d,%d,%d) wrap=(%.6f,%.6f,%.6f) float=(%.6f,%.6f,%.6f) devImg=(%.6f,%.6f,%.6f) charge=%.6f LJID=%d origAtom=%d mapBack=%d origCharge=%.6f origLJID=%d",
+             i, ic.x, ic.y, ic.z, wx, wy, wz, fx, fy, fz, dx, dy, dz,
+             q.x, (int)q.y, origAtom, mappedBack, qOrig.x, (int)qOrig.y);
+    MINT32_LOG(buf);
+  }
+
+  // Sample first pair (0,1) to compare int-vs-float distances
+  int4 ia = gpu->pbAtomCoord_M32->_pSysData[0];
+  int4 ib = gpu->pbAtomCoord_M32->_pSysData[1];
+  double dx_i = (double)(ib.x - ia.x) * gpu->sim.mint32_inv_scale.x;
+  double dy_i = (double)(ib.y - ia.y) * gpu->sim.mint32_inv_scale.y;
+  double dz_i = (double)(ib.z - ia.z) * gpu->sim.mint32_inv_scale.z;
+  double r2_i = dx_i * dx_i + dy_i * dy_i + dz_i * dz_i;
+
+  double fx0 = pFloatCrd[0];
+  double fy0 = pFloatCrd[0 + gpu->sim.stride];
+  double fz0 = pFloatCrd[0 + gpu->sim.stride2];
+  double fx1 = pFloatCrd[1];
+  double fy1 = pFloatCrd[1 + gpu->sim.stride];
+  double fz1 = pFloatCrd[1 + gpu->sim.stride2];
+  double dx_f = fx1 - fx0;
+  double dy_f = fy1 - fy0;
+  double dz_f = fz1 - fz0;
+  double r2_f = dx_f * dx_f + dy_f * dy_f + dz_f * dz_f;
+  double r_f  = sqrt(r2_f);
+  double r2_dev = -1.0;
+  double r2_dev_wrap = -1.0;
+  if (devImgValid) {
+    double dx_d = devX[1] - devX[0];
+    double dy_d = devY[1] - devY[0];
+    double dz_d = devZ[1] - devZ[0];
+    r2_dev = dx_d*dx_d + dy_d*dy_d + dz_d*dz_d;
+    double dx_dw = wrap_centered(devX[1], gpu->sim.a) - wrap_centered(devX[0], gpu->sim.a);
+    double dy_dw = wrap_centered(devY[1], gpu->sim.b) - wrap_centered(devY[0], gpu->sim.b);
+    double dz_dw = wrap_centered(devZ[1], gpu->sim.c) - wrap_centered(devZ[0], gpu->sim.c);
+    r2_dev_wrap = dx_dw*dx_dw + dy_dw*dy_dw + dz_dw*dz_dw;
+  }
+
+  PMEFloat2 qa = gpu->pbAtomChargeSPLJID->_pSysData[0];
+  PMEFloat2 qb = gpu->pbAtomChargeSPLJID->_pSysData[1];
+  PMEFloat2 lj00 = {0.f, 0.f};
+  int ljIndex = -1;
+  if (gpu->pbLJTerm != nullptr) {
+    unsigned int ti = (unsigned int)qa.y;
+    unsigned int tj = (unsigned int)qb.y;
+    ljIndex = (int)(ti * gpu->sim.LJTypes + tj);
+    if (ljIndex >= 0 && ljIndex < gpu->sim.LJTerms) {
+      lj00 = gpu->pbLJTerm->_pSysData[ljIndex];
+    }
+  }
+  double hostEcoul = 0.0;
+  double hostElj   = 0.0;
+  double hostAlpha = gpu->sim.ew_coeff;
+  double hostCoulConst = 332.0636;
+  if (r_f > 0.0) {
+    double rinv = 1.0 / r_f;
+    double erfcTerm = erfc(hostAlpha * r_f);
+    hostEcoul = hostCoulConst * qa.x * qb.x * erfcTerm * rinv;
+    double r2inv = rinv * rinv;
+    double r6inv = r2inv * r2inv * r2inv;
+    hostElj = lj00.x * r6inv * r6inv - lj00.y * r6inv;
+  }
+  snprintf(buf, sizeof(buf),
+           " pair 0-1: r_int=%.6f r_float=%.6f r_dev=%.6f r_dev_wrap=%.6f r2_int=%.6f r2_float=%.6f r2_dev=%.6f r2_dev_wrap=%.6f q=(%.6f,%.6f) LJID=(%d,%d)",
+           sqrt(r2_i), sqrt(r2_f),
+           (devImgValid && r2_dev >= 0.0) ? sqrt(r2_dev) : -1.0,
+           (devImgValid && r2_dev_wrap >= 0.0) ? sqrt(r2_dev_wrap) : -1.0,
+           r2_i, r2_f, r2_dev, r2_dev_wrap, qa.x, qb.x, (int)qa.y, (int)qb.y);
+  MINT32_LOG(buf);
+  snprintf(buf, sizeof(buf),
+           " LJTerm index=%d value=(%.6f,%.6f) LJTypes=%d LJTerms=%d alpha=%.6f CoulConst=%.3f hostEcoul=%.6f hostElj=%.6f",
+           ljIndex, lj00.x, lj00.y, gpu->sim.LJTypes, gpu->sim.LJTerms,
+           hostAlpha, hostCoulConst, hostEcoul, hostElj);
+  MINT32_LOG(buf);
+}
+
+static int Mint32ChecksumEnvValue()
+{
+  static int cached = -1;
+  if (cached == -1) {
+#if !MINT32_NL_CHECKSUM_ENABLED
+    const char* env = getenv("MINT32_NL_CHECKSUM");
+    cached = 0;
+    if (env && atoi(env) != 0) {
+      char buf[180];
+      snprintf(buf, sizeof(buf),
+               "MINT32_NL_CHECKSUM requested (%s) but checksum code is compiled out; rebuild with MINT32_NL_CHECKSUM_ENABLED=1 to enable",
+               env);
+      MINT32_LOG(buf);
+    }
+    else {
+      MINT32_LOG("MINT32_NL_CHECKSUM disabled at compile time");
+    }
+#else
+    const char* env = getenv("MINT32_NL_CHECKSUM");
+    cached = (env != nullptr) ? atoi(env) : 0;
+    char buf[200];
+    snprintf(buf, sizeof(buf), "MINT32_NL_CHECKSUM env=%s parsed=%d",
+             env ? env : "unset", cached);
+    MINT32_LOG(buf);
+#endif
+  }
+  return cached;
+}
+
 #include "matrix.h"
 #include "bondRemap.h"
 
@@ -308,7 +558,9 @@ extern "C" void gpu_init_(void)
   // Finally set CUDA device
   status = cudaSetDevice(device);
   RTERROR(status, "Error setting CUDA device");
+  MINT32_LOG("BEFORE cudaDeviceSynchronize in gpu_init");
   cudaDeviceSynchronize();
+  MINT32_LOG("AFTER cudaDeviceSynchronize in gpu_init");
 
   // Test for universal P2P access
 #if defined(MPI)
@@ -678,12 +930,14 @@ extern "C" void gpu_send_slave_device_info_()
 //---------------------------------------------------------------------------------------------
 extern "C" void gpu_get_memory_info_(int* gpumemory, int* cpumemory)
 {
+  MINT32_LOG("ENTRY gpu_get_memory_info");
   // The "ll" suffix denotes that the integer be formulated as a long long int
   //*gpumemory  = (int)(gpu->totalGPUMemory / 1024ll);
   //*cpumemory  = (int)(gpu->totalCPUMemory / 1024ll);
 
   *gpumemory = gpuMemoryInfo::Instance().totalGPUMemory / 1024ll;
   *cpumemory = gpuMemoryInfo::Instance().totalCPUMemory / 1024ll;
+  MINT32_LOG("EXIT gpu_get_memory_info");
   return;
 }
 
@@ -748,6 +1002,7 @@ extern "C" void gpu_setup_system_(int *atoms, int *imin, double* tol, int* ntf,
 				                  int *ti_mode, double* surften,
                                   int *ischeme, int *ithermostat, int *ntc)
 {
+  MINT32_LOG("ENTRY gpu_setup_system");
   PRINTMETHOD("gpu_setup_system");
 
   // Grab simulation parameters;
@@ -810,6 +1065,12 @@ extern "C" void gpu_setup_system_(int *atoms, int *imin, double* tol, int* ntf,
 
   // Clear any previous stuff
   delete gpu->pbAtom;
+  delete gpu->pbOldAtomCoord_M32;
+  delete gpu->pbOldAtomXYSP;
+  delete gpu->pbOldAtomZSP;
+  delete gpu->pbAtomCoord_M32;
+  delete gpu->pbAtomCoordUnsorted_M32;
+  delete gpu->pbOldAtomCoordUnsorted_M32;
   delete gpu->pbAtomXYSP;
   delete gpu->pbAtomZSP;
   delete gpu->pbAtomSigEps;
@@ -836,8 +1097,14 @@ extern "C" void gpu_setup_system_(int *atoms, int *imin, double* tol, int* ntf,
   delete gpu->pbBarLambda;
   delete gpu->pbBarTot;
   gpu->pbAtom               = new GpuBuffer<double>(gpu->sim.stride3 * 3); //fixed to 3 to store SHAKEOLDATOM for middle-scheme
-  gpu->pbAtomXYSP           = new GpuBuffer<PMEFloat2>(gpu->sim.stride);
-  gpu->pbAtomZSP            = new GpuBuffer<PMEFloat>(gpu->sim.stride);
+  gpu->pbAtomCoord_M32             = new GpuBuffer<int4>(gpu->sim.stride);
+  gpu->pbAtomCoordUnsorted_M32     = new GpuBuffer<int4>(gpu->sim.stride);  // MINT32 coords in original atom order (Array A)
+  gpu->pbAtomXYSP              = new GpuBuffer<PMEFloat2>(gpu->sim.stride);  // FP32 coords for NL build
+  gpu->pbAtomZSP               = new GpuBuffer<PMEFloat>(gpu->sim.stride);
+  gpu->pbOldAtomCoord_M32          = new GpuBuffer<int4>(gpu->sim.stride);
+  gpu->pbOldAtomCoordUnsorted_M32  = new GpuBuffer<int4>(gpu->sim.stride);  // Old MINT32 coords in original atom order (Array A)
+  gpu->pbOldAtomXYSP           = new GpuBuffer<PMEFloat2>(gpu->sim.stride);  // Old FP32 coords for NL
+  gpu->pbOldAtomZSP            = new GpuBuffer<PMEFloat>(gpu->sim.stride);
   gpu->pbAtomCharge         = new GpuBuffer<double>(gpu->sim.stride);
   gpu->pbAtomChargeSP       = new GpuBuffer<PMEFloat>(gpu->sim.stride);
   gpu->pbAtomSigEps         = new GpuBuffer<PMEFloat2>(gpu->sim.stride);
@@ -1034,63 +1301,16 @@ extern "C" void gpu_setup_system_(int *atoms, int *imin, double* tol, int* ntf,
     RTERROR(status, "cudaCreateTextureObject gpu->sim.texAtomX failed");
   }
   gpu->sim.pAtomY = gpu->pbAtom->_pDevData + gpu->sim.stride;
+  gpu->sim.pAtomCoord_M32 = gpu->pbAtomCoord_M32->_pDevData;
+  gpu->sim.pAtomCoordUnsorted_M32 = gpu->pbAtomCoordUnsorted_M32->_pDevData;
   gpu->sim.pAtomXYSP = gpu->pbAtomXYSP->_pDevData;
-#if !defined(use_DPFP)
-  {
-    cudaError_t status;
-    if (gpu->sim.texAtomXYSP != 0) {
-      status = cudaDestroyTextureObject(gpu->sim.texAtomXYSP);
-      RTERROR(status, "cudaDestroyTextureObject gpu->sim.texAtomXYSP failed");
-    }
-    cudaResourceDesc resDesc;
-    cudaTextureDesc texDesc;
-    memset(&resDesc, 0, sizeof(resDesc));
-    resDesc.resType = cudaResourceTypeLinear;
-    resDesc.res.linear.devPtr = gpu->sim.pAtomXYSP;
-    resDesc.res.linear.sizeInBytes = gpu->sim.stride * sizeof(float2);
-    resDesc.res.linear.desc.f = cudaChannelFormatKindFloat;
-    resDesc.res.linear.desc.x = 32;
-    resDesc.res.linear.desc.y = 32;
-    resDesc.res.linear.desc.z = 0;
-    resDesc.res.linear.desc.w = 0;
-    memset(&texDesc, 0, sizeof(texDesc));
-    texDesc.normalizedCoords = 0;
-    texDesc.filterMode = cudaFilterModePoint;
-    texDesc.addressMode[0] = cudaAddressModeClamp;
-    texDesc.readMode = cudaReadModeElementType;
-    status = cudaCreateTextureObject(&(gpu->sim.texAtomXYSP), &resDesc, &texDesc, NULL);
-    RTERROR(status, "cudaCreateTextureObject gpu->sim.texAtomXYSP failed");
-  }
-#endif
-  gpu->sim.pAtomZ = gpu->pbAtom->_pDevData + gpu->sim.stride2;
   gpu->sim.pAtomZSP = gpu->pbAtomZSP->_pDevData;
-#if !defined(use_DPFP)
-  {
-    cudaError_t status;
-    if (gpu->sim.texAtomZSP != 0) {
-      status = cudaDestroyTextureObject(gpu->sim.texAtomZSP);
-      RTERROR(status, "cudaDestroyTextureObject gpu->sim.texAtomZSP failed");
-    }
-    cudaResourceDesc resDesc;
-    cudaTextureDesc texDesc;
-    memset(&resDesc, 0, sizeof(resDesc));
-    resDesc.resType = cudaResourceTypeLinear;
-    resDesc.res.linear.devPtr = gpu->sim.pAtomZSP;
-    resDesc.res.linear.sizeInBytes = gpu->sim.stride * sizeof(float);
-    resDesc.res.linear.desc.f = cudaChannelFormatKindFloat;
-    resDesc.res.linear.desc.x = 32;
-    resDesc.res.linear.desc.y = 0;
-    resDesc.res.linear.desc.z = 0;
-    resDesc.res.linear.desc.w = 0;
-    memset(&texDesc, 0, sizeof(texDesc));
-    texDesc.normalizedCoords = 0;
-    texDesc.filterMode = cudaFilterModePoint;
-    texDesc.addressMode[0] = cudaAddressModeClamp;
-    texDesc.readMode = cudaReadModeElementType;
-    status = cudaCreateTextureObject(&(gpu->sim.texAtomZSP), &resDesc, &texDesc, NULL);
-    RTERROR(status, "cudaCreateTextureObject gpu->sim.texAtomZSP failed");
-  }
-#endif
+
+  gpu->sim.pOldAtomCoord_M32 = gpu->pbOldAtomCoord_M32->_pDevData;
+  gpu->sim.pOldAtomCoordUnsorted_M32 = gpu->pbOldAtomCoordUnsorted_M32->_pDevData;
+  gpu->sim.pOldAtomXYSP = gpu->pbOldAtomXYSP->_pDevData;
+  gpu->sim.pOldAtomZSP = gpu->pbOldAtomZSP->_pDevData;
+
   gpu->sim.pOldAtomX = gpu->pbAtom->_pDevData + gpu->sim.stride3;
   {
     cudaError_t status;
@@ -1821,8 +2041,11 @@ extern "C" void gpu_shuttle_post_data_(double atm_data[][3], int* flag)
 //---------------------------------------------------------------------------------------------
 extern "C" void gpu_upload_crd_(double atm_crd[][3])
 {
+  MINT32_LOG("ENTRY gpu_upload_crd");
   PRINTMETHOD("gpu_upload_crd");
   if (gpu->bNeighborList && (gpu->pbImageIndex != NULL)) {
+    MINT32_LOG("gpu_upload_crd: bNeighborList branch - populating pbAtomCoord");
+    static bool loggedNeighborUpload = false;
     if (gpu->bNewNeighborList) {
       gpu->pbImageIndex->Download();
       gpu->bNewNeighborList = false;
@@ -1835,32 +2058,128 @@ extern "C" void gpu_upload_crd_(double atm_crd[][3])
     }
     for (int i = 0; i < gpu->sim.atoms; i++) {
       int i1 = pImageAtomLookup[i];
-      pCrd[i1] = atm_crd[i][0];
-      pCrd[i1 + gpu->sim.stride] = atm_crd[i][1];
-      pCrd[i1 + gpu->sim.stride2] = atm_crd[i][2];
+      double xwrap = wrap_centered(atm_crd[i][0], gpu->sim.a);
+      double ywrap = wrap_centered(atm_crd[i][1], gpu->sim.b);
+      double zwrap = wrap_centered(atm_crd[i][2], gpu->sim.c);
+      // Store WRAPPED coordinates in pImageX/Y/Z for consistency with MINT32
+      pCrd[i1] = xwrap;
+      pCrd[i1 + gpu->sim.stride] = ywrap;
+      pCrd[i1 + gpu->sim.stride2] = zwrap;
+      // MINT32: Also populate pbAtomCoord for integer coordinate calculations
+      gpu->pbAtomCoord_M32->_pSysData[i1].x = mint32_from_double(xwrap, gpu->sim.mint32_scale.x);
+      gpu->pbAtomCoord_M32->_pSysData[i1].y = mint32_from_double(ywrap, gpu->sim.mint32_scale.y);
+      gpu->pbAtomCoord_M32->_pSysData[i1].z = mint32_from_double(zwrap, gpu->sim.mint32_scale.z);
+      gpu->pbAtomCoord_M32->_pSysData[i1].w = 0;
+      
+      // MINT32: Populate pbAtomCoordUnsorted (Array A) with original atom order
+      gpu->pbAtomCoordUnsorted_M32->_pSysData[i].x = mint32_from_double(xwrap, gpu->sim.mint32_scale.x);
+      gpu->pbAtomCoordUnsorted_M32->_pSysData[i].y = mint32_from_double(ywrap, gpu->sim.mint32_scale.y);
+      gpu->pbAtomCoordUnsorted_M32->_pSysData[i].z = mint32_from_double(zwrap, gpu->sim.mint32_scale.z);
+      gpu->pbAtomCoordUnsorted_M32->_pSysData[i].w = 0;
+
+      // MINT32: Populate pbOldAtomCoordUnsorted (Array A) with original atom order
+      gpu->pbOldAtomCoordUnsorted_M32->_pSysData[i] = gpu->pbAtomCoordUnsorted_M32->_pSysData[i];
+
+      if (!loggedNeighborUpload && i == 0) {
+        char buf[256];
+        snprintf(buf, sizeof(buf),
+                 "gpu_upload_crd neighbor mode first atom: i=%d pos=%d xyz=(%.6f,%.6f,%.6f) mint32=(%d,%d,%d) scale=(%.6f,%.6f,%.6f) dev=%p cSim=%p",
+                 i, i1, atm_crd[i][0], atm_crd[i][1], atm_crd[i][2],
+                 gpu->pbAtomCoord_M32->_pSysData[i1].x,
+                 gpu->pbAtomCoord_M32->_pSysData[i1].y,
+                 gpu->pbAtomCoord_M32->_pSysData[i1].z,
+                 gpu->sim.mint32_scale.x,
+                 gpu->sim.mint32_scale.y,
+                 gpu->sim.mint32_scale.z,
+                 (void*)gpu->pbAtomCoord_M32->_pDevData,
+                 (void*)gpu->sim.pAtomCoord);
+        MINT32_LOG(buf);
+        loggedNeighborUpload = true;
+      }
+    }
+    // MINT32: Populate padding atoms
+    for (int i = gpu->sim.atoms; i < gpu->sim.stride; i++) {
+      gpu->pbAtomCoord_M32->_pSysData[i].x = (int)(long long)round((9999990000.0 + i*2000.0) * gpu->sim.mint32_scale.x);
+      gpu->pbAtomCoord_M32->_pSysData[i].y = (int)(long long)round((9999990000.0 + i*2000.0) * gpu->sim.mint32_scale.y);
+      gpu->pbAtomCoord_M32->_pSysData[i].z = (int)(long long)round((9999990000.0 + i*2000.0) * gpu->sim.mint32_scale.z);
+      gpu->pbAtomCoord_M32->_pSysData[i].w = 0;
     }
     gpu->pbImage->Upload();
+    gpu->pbAtomCoord_M32->Upload();
+    gpu->pbAtomCoordUnsorted_M32->Upload();
+    gpu->pbOldAtomCoordUnsorted_M32->Upload();
+
+    // Initialize unsorted array (Array A) with original atom order - GPU to GPU copy
+    cudaError_t status = cudaMemcpy(gpu->pbAtomCoordUnsorted_M32->_pDevData,
+                                     gpu->pbAtomCoord_M32->_pDevData,
+                                     gpu->sim.stride * sizeof(int4),
+                                     cudaMemcpyDeviceToDevice);
+    RTERROR(status, "cudaMemcpy pAtomCoord to pAtomCoordUnsorted failed");
+
+    MINT32_LOG("gpu_upload_crd: Uploaded pbAtomCoord and copied to pbAtomCoordUnsorted");
   }
   else {
+    static int upload_count = 0;
+    upload_count++;
     for (int i = 0; i < gpu->sim.atoms; i++) {
-      gpu->pbAtom->_pSysData[i] = atm_crd[i][0];
-      gpu->pbAtom->_pSysData[i + gpu->sim.stride] = atm_crd[i][1];
-      gpu->pbAtom->_pSysData[i + gpu->sim.stride2] = atm_crd[i][2];
-      gpu->pbAtomXYSP->_pSysData[i].x = atm_crd[i][0];
-      gpu->pbAtomXYSP->_pSysData[i].y = atm_crd[i][1];
-      gpu->pbAtomZSP->_pSysData[i] = atm_crd[i][2];
+      double xwrap = wrap_centered(atm_crd[i][0], gpu->sim.a);
+      double ywrap = wrap_centered(atm_crd[i][1], gpu->sim.b);
+      double zwrap = wrap_centered(atm_crd[i][2], gpu->sim.c);
+      // Store WRAPPED coordinates in pbAtom for consistency with MINT32
+      gpu->pbAtom->_pSysData[i] = xwrap;
+      gpu->pbAtom->_pSysData[i + gpu->sim.stride] = ywrap;
+      gpu->pbAtom->_pSysData[i + gpu->sim.stride2] = zwrap;
+      gpu->pbAtomCoord_M32->_pSysData[i].x = mint32_from_double(xwrap, gpu->sim.mint32_scale.x);
+      gpu->pbAtomCoord_M32->_pSysData[i].y = mint32_from_double(ywrap, gpu->sim.mint32_scale.y);
+      gpu->pbAtomCoord_M32->_pSysData[i].z = mint32_from_double(zwrap, gpu->sim.mint32_scale.z);
+      gpu->pbAtomCoord_M32->_pSysData[i].w = 0;
     }
     for (int i = gpu->sim.atoms; i < gpu->sim.stride; i++) {
       gpu->pbAtom->_pSysData[i] = 9999990000.0 + i*2000.0;
       gpu->pbAtom->_pSysData[i + gpu->sim.stride] = 9999990000.0 + i*2000.0;
       gpu->pbAtom->_pSysData[i + gpu->sim.stride2] = 9999990000.0 + i*2000.0;
-      gpu->pbAtomXYSP->_pSysData[i].x = 9999990000.0f + i*2000.0;
-      gpu->pbAtomXYSP->_pSysData[i].y = 9999990000.0f + i*2000.0;
-      gpu->pbAtomZSP->_pSysData[i] = 9999990000.0f + i*2000.0;
+      gpu->pbAtomCoord_M32->_pSysData[i].x = (int)(long long)round((9999990000.0 + i*2000.0) * gpu->sim.mint32_scale.x);
+      gpu->pbAtomCoord_M32->_pSysData[i].y = (int)(long long)round((9999990000.0 + i*2000.0) * gpu->sim.mint32_scale.y);
+      gpu->pbAtomCoord_M32->_pSysData[i].z = (int)(long long)round((9999990000.0 + i*2000.0) * gpu->sim.mint32_scale.z);
+      gpu->pbAtomCoord_M32->_pSysData[i].w = 0;
+    }
+    if (upload_count <= 3) {
+      char buf[512];
+      snprintf(buf, sizeof(buf),
+               "gpu_upload_crd (no NL) call #%d: atom[0]=(%.6f,%.6f,%.6f) atom[1]=(%.6f,%.6f,%.6f) atoms=%d stride=%d",
+               upload_count,
+               gpu->pbAtom->_pSysData[0],
+               gpu->pbAtom->_pSysData[gpu->sim.stride],
+               gpu->pbAtom->_pSysData[gpu->sim.stride2],
+               gpu->pbAtom->_pSysData[1],
+               gpu->pbAtom->_pSysData[1 + gpu->sim.stride],
+               gpu->pbAtom->_pSysData[1 + gpu->sim.stride2],
+               gpu->sim.atoms, gpu->sim.stride);
+      MINT32_LOG(buf);
     }
     gpu->pbAtom->Upload();
-    gpu->pbAtomXYSP->Upload();
-    gpu->pbAtomZSP->Upload();
+    gpu->pbAtomCoord_M32->Upload();
+
+    // Initialize unsorted array (Array A) with original atom order - GPU to GPU copy
+    cudaError_t status = cudaMemcpy(gpu->pbAtomCoordUnsorted_M32->_pDevData,
+                                     gpu->pbAtomCoord_M32->_pDevData,
+                                     gpu->sim.stride * sizeof(int4),
+                                     cudaMemcpyDeviceToDevice);
+    RTERROR(status, "cudaMemcpy pAtomCoord to pAtomCoordUnsorted failed");
+
+    // Initialize pbOldAtomCoordUnsorted
+    cudaMemcpy(gpu->pbOldAtomCoordUnsorted_M32->_pDevData, gpu->pbAtomCoordUnsorted_M32->_pDevData,
+               gpu->sim.stride * sizeof(int4), cudaMemcpyDeviceToDevice);
+
+    // Initial seeding of pOldAtomCoord with current MINT32 coordinates
+    // This ensures SHAKE has valid old coordinates on first timestep
+    cudaMemcpy(gpu->pbOldAtomCoord_M32->_pDevData, gpu->pbAtomCoord_M32->_pDevData,
+               gpu->sim.stride * sizeof(int4), cudaMemcpyDeviceToDevice);
+  }
+  // Hybrid MINT32: REQUIRED for NL build (uses FP32), forces use MINT32
+  // Only call if pImageX is initialized (happens in gpu_neighbor_list_setup_)
+  if (gpu->sim.pImageX != nullptr) {
+    kMINT32ToFloat(gpu, 0);  // pImageX/Y/Z â†’ pAtomXYSP/ZSP
   }
 }
 
@@ -1877,24 +2196,33 @@ extern "C" void gpu_upload_crd_gb_cph_(double atm_crd[][3])
 {
   PRINTMETHOD("gpu_upload_crd_gb_cph");
   for (int i = 0; i < gpu->sim.atoms; i++) {
+    double xwrap = wrap_centered(atm_crd[i][0], gpu->sim.a);
+    double ywrap = wrap_centered(atm_crd[i][1], gpu->sim.b);
+    double zwrap = wrap_centered(atm_crd[i][2], gpu->sim.c);
     gpu->pbAtom->_pSysData[i] = atm_crd[i][0];
     gpu->pbAtom->_pSysData[i + gpu->sim.stride] = atm_crd[i][1];
     gpu->pbAtom->_pSysData[i + gpu->sim.stride2] = atm_crd[i][2];
-    gpu->pbAtomXYSP->_pSysData[i].x = atm_crd[i][0];
-    gpu->pbAtomXYSP->_pSysData[i].y = atm_crd[i][1];
-    gpu->pbAtomZSP->_pSysData[i] = atm_crd[i][2];
+    gpu->pbAtomCoord_M32->_pSysData[i].x = mint32_from_double(xwrap, gpu->sim.mint32_scale.x);
+    gpu->pbAtomCoord_M32->_pSysData[i].y = mint32_from_double(ywrap, gpu->sim.mint32_scale.y);
+    gpu->pbAtomCoord_M32->_pSysData[i].z = mint32_from_double(zwrap, gpu->sim.mint32_scale.z);
+    gpu->pbAtomCoord_M32->_pSysData[i].w = 0;
   }
   for (int i = gpu->sim.atoms; i < gpu->sim.stride; i++) {
     gpu->pbAtom->_pSysData[i] = 9999990000.0 + i * 2000.0;
     gpu->pbAtom->_pSysData[i + gpu->sim.stride] = 9999990000.0 + i * 2000.0;
     gpu->pbAtom->_pSysData[i + gpu->sim.stride2] = 9999990000.0 + i * 2000.0;
-    gpu->pbAtomXYSP->_pSysData[i].x = 9999990000.0f + i * 2000.0;
-    gpu->pbAtomXYSP->_pSysData[i].y = 9999990000.0f + i * 2000.0;
-    gpu->pbAtomZSP->_pSysData[i] = 9999990000.0f + i * 2000.0;
+    gpu->pbAtomCoord_M32->_pSysData[i].x = (int)(long long)round((9999990000.0 + i * 2000.0) * gpu->sim.mint32_scale.x);
+    gpu->pbAtomCoord_M32->_pSysData[i].y = (int)(long long)round((9999990000.0 + i * 2000.0) * gpu->sim.mint32_scale.y);
+    gpu->pbAtomCoord_M32->_pSysData[i].z = (int)(long long)round((9999990000.0 + i * 2000.0) * gpu->sim.mint32_scale.z);
+    gpu->pbAtomCoord_M32->_pSysData[i].w = 0;
   }
   gpu->pbAtom->Upload();
-  gpu->pbAtomXYSP->Upload();
-  gpu->pbAtomZSP->Upload();
+  gpu->pbAtomCoord_M32->Upload();
+  // Initialize unsorted array (Array A) with original atom order - GPU to GPU copy
+  cudaMemcpy(gpu->pbAtomCoordUnsorted_M32->_pDevData,
+             gpu->pbAtomCoord_M32->_pDevData,
+             gpu->sim.stride * sizeof(int4),
+             cudaMemcpyDeviceToDevice);
 }
 
 //---------------------------------------------------------------------------------------------
@@ -2402,19 +2730,33 @@ extern "C" void gpu_upload_charges_(double charge[])
     if (gpu->sim.pImageCharge != gpu->pbImageCharge->_pDevData) {
       pCharge = gpu->pbImageCharge->_pSysData + gpu->sim.stride;
     }
+    static int upload_count = 0;
+    FloatShift ljid;
     for (int i = 0; i < gpu->sim.atoms; i++) {
       int i1 = pImageAtomLookup[i];
       pCharge[i1] = charge[i];
       gpu->pbAtomChargeSP->_pSysData[i1] = charge[i];
       gpu->pbAtomChargeSPLJID->_pSysData[i1].x = charge[i];
+      ljid.ui = gpu->pbAtomLJID->_pSysData[i];
+      gpu->pbAtomChargeSPLJID->_pSysData[i1].y = ljid.f;
+      if (upload_count < 2 && i < 3) {
+        char buf[256];
+        snprintf(buf, sizeof(buf), "gpu_upload_charges: i=%d i1=%d charge=%.6f LJID_orig=%u LJID_float=%.6f",
+                 i, i1, charge[i], gpu->pbAtomLJID->_pSysData[i], ljid.f);
+        MINT32_LOG(buf);
+      }
     }
+    upload_count++;
     gpu->pbImageCharge->Upload();
   }
   else {
+    FloatShift ljid;
     for (int i = 0; i < gpu->sim.atoms; i++) {
       gpu->pbAtomCharge->_pSysData[i] = charge[i];
       gpu->pbAtomChargeSP->_pSysData[i] = charge[i];
       gpu->pbAtomChargeSPLJID->_pSysData[i].x = charge[i];
+      ljid.ui = gpu->pbAtomLJID->_pSysData[i];
+      gpu->pbAtomChargeSPLJID->_pSysData[i].y = ljid.f;
     }
     for (int i = gpu->sim.atoms; i < gpu->sim.paddedNumberOfAtoms; i++) {
       gpu->pbAtomCharge->_pSysData[i] = (PMEDouble)0.0;
@@ -2937,12 +3279,28 @@ extern "C" void gpu_download_frc_(double atm_frc[][3])
 extern "C" void gpu_upload_vel_(double atm_vel[][3])
 {
   PRINTMETHOD("gpu_upload_vel");
+  int nanInput = 0;
+  for (int i = 0; i < gpu->sim.atoms; i++) {
+    if (isnan(atm_vel[i][0]) || isnan(atm_vel[i][1]) || isnan(atm_vel[i][2])) {
+      printf("ERROR: gpu_upload_vel_ found NaN at atom %d: %f %f %f\n", i, atm_vel[i][0], atm_vel[i][1], atm_vel[i][2]);
+      nanInput++;
+    }
+  }
+
+  // Always populate unsorted velocities (original atom order)
+  for (int i = 0; i < gpu->sim.atoms; i++) {
+    gpu->pbVel->_pSysData[i] = atm_vel[i][0];
+    gpu->pbVel->_pSysData[i + gpu->sim.stride] = atm_vel[i][1];
+    gpu->pbVel->_pSysData[i + gpu->sim.stride2] = atm_vel[i][2];
+  }
+
+  gpu->pbVel->Upload();
+
   if (gpu->bNeighborList && (gpu->pbImageIndex != NULL)) {
     if (gpu->bNewNeighborList) {
       gpu->pbImageIndex->Download();
       gpu->bNewNeighborList = false;
     }
-    gpu->pbImageVel->Download();
     unsigned int* pImageAtomLookup = &(gpu->pbImageIndex->_pSysData[gpu->sim.imageStride * 2]);
     double *pVel = gpu->pbImageVel->_pSysData;
     if (gpu->sim.pImageVelX != gpu->pbImageVel->_pDevData) {
@@ -2956,14 +3314,6 @@ extern "C" void gpu_upload_vel_(double atm_vel[][3])
     }
     gpu->pbImageVel->Upload();
   }
-  else {
-    for (int i = 0; i < gpu->sim.atoms; i++) {
-      gpu->pbVel->_pSysData[i] = atm_vel[i][0];
-      gpu->pbVel->_pSysData[i + gpu->sim.stride] = atm_vel[i][1];
-      gpu->pbVel->_pSysData[i + gpu->sim.stride2] = atm_vel[i][2];
-    }
-    gpu->pbVel->Upload();
-  }
 }
 
 //---------------------------------------------------------------------------------------------
@@ -2975,31 +3325,12 @@ extern "C" void gpu_upload_vel_(double atm_vel[][3])
 extern "C" void gpu_download_vel_(double atm_vel[][3])
 {
   PRINTMETHOD("gpu_download_vel");
-  if (gpu->bNeighborList && (gpu->pbImageIndex != NULL)) {
-    if (gpu->bNewNeighborList) {
-      gpu->pbImageIndex->Download();
-      gpu->bNewNeighborList = false;
-    }
-    gpu->pbImageVel->Download();
-    unsigned int* pImageAtomLookup = &(gpu->pbImageIndex->_pSysData[gpu->sim.imageStride * 2]);
-    double *pVel = gpu->pbImageVel->_pSysData;
-    if (gpu->sim.pImageVelX != gpu->pbImageVel->_pDevData) {
-      pVel = gpu->pbImageVel->_pSysData + gpu->sim.stride3;
-    }
-    for (int i = 0; i < gpu->sim.atoms; i++) {
-      int i1 = pImageAtomLookup[i];
-      atm_vel[i][0] = pVel[i1];
-      atm_vel[i][1] = pVel[i1 + gpu->sim.stride];
-      atm_vel[i][2] = pVel[i1 + gpu->sim.stride2];
-    }
-  }
-  else {
-    gpu->pbVel->Download();
-    for (int i = 0; i < gpu->sim.atoms; i++) {
-      atm_vel[i][0] = gpu->pbVel->_pSysData[i];
-      atm_vel[i][1] = gpu->pbVel->_pSysData[i + gpu->sim.stride];
-      atm_vel[i][2] = gpu->pbVel->_pSysData[i + gpu->sim.stride2];
-    }
+  // Unsorted velocities are authoritative; download directly in original atom order
+  gpu->pbVel->Download();
+  for (int i = 0; i < gpu->sim.atoms; i++) {
+    atm_vel[i][0] = gpu->pbVel->_pSysData[i];
+    atm_vel[i][1] = gpu->pbVel->_pSysData[i + gpu->sim.stride];
+    atm_vel[i][2] = gpu->pbVel->_pSysData[i + gpu->sim.stride2];
   }
 }
 
@@ -3013,6 +3344,19 @@ extern "C" void gpu_download_vel_(double atm_vel[][3])
 extern "C" void gpu_upload_last_vel_(double atm_last_vel[][3])
 {
   PRINTMETHOD("gpu_upload_last_vel");
+  for (int i = 0; i < gpu->sim.atoms; i++) {
+    if (isnan(atm_last_vel[i][0]) || isnan(atm_last_vel[i][1]) || isnan(atm_last_vel[i][2])) {
+      printf("ERROR: gpu_upload_last_vel_ found NaN at atom %d: %f %f %f\n", i, atm_last_vel[i][0], atm_last_vel[i][1], atm_last_vel[i][2]);
+    }
+  }
+  // Always populate unsorted last velocities (original atom order)
+  for (int i = 0; i < gpu->sim.atoms; i++) {
+    gpu->pbLVel->_pSysData[i] = atm_last_vel[i][0];
+    gpu->pbLVel->_pSysData[i + gpu->sim.stride] = atm_last_vel[i][1];
+    gpu->pbLVel->_pSysData[i + gpu->sim.stride2]= atm_last_vel[i][2];
+  }
+  gpu->pbLVel->Upload();
+
   if (gpu->bNeighborList && (gpu->pbImageIndex != NULL)) {
     if (gpu->bNewNeighborList) {
       gpu->pbImageIndex->Download();
@@ -3032,14 +3376,6 @@ extern "C" void gpu_upload_last_vel_(double atm_last_vel[][3])
     }
     gpu->pbImageLVel->Upload();
   }
-  else {
-    for (int i = 0; i < gpu->sim.atoms; i++) {
-      gpu->pbLVel->_pSysData[i] = atm_last_vel[i][0];
-      gpu->pbLVel->_pSysData[i + gpu->sim.stride] = atm_last_vel[i][1];
-      gpu->pbLVel->_pSysData[i + gpu->sim.stride2]= atm_last_vel[i][2];
-    }
-    gpu->pbLVel->Upload();
-  }
 }
 
 //---------------------------------------------------------------------------------------------
@@ -3052,31 +3388,12 @@ extern "C" void gpu_upload_last_vel_(double atm_last_vel[][3])
 extern "C" void gpu_download_last_vel_(double atm_last_vel[][3])
 {
   PRINTMETHOD("gpu_download_last_vel");
-  if (gpu->bNeighborList && (gpu->pbImageIndex != NULL)) {
-    if (gpu->bNewNeighborList) {
-      gpu->pbImageIndex->Download();
-      gpu->bNewNeighborList = false;
-    }
-    gpu->pbImageLVel->Download();
-    unsigned int* pImageAtomLookup = &(gpu->pbImageIndex->_pSysData[gpu->sim.imageStride * 2]);
-    double *pLVel = gpu->pbImageLVel->_pSysData;
-    if (gpu->sim.pImageLVelX != gpu->pbImageLVel->_pDevData) {
-      pLVel = gpu->pbImageLVel->_pSysData + gpu->sim.stride3;
-    }
-    for (int i = 0; i < gpu->sim.atoms; i++) {
-      int i1 = pImageAtomLookup[i];
-      atm_last_vel[i][0] = pLVel[i1];
-      atm_last_vel[i][1] = pLVel[i1 + gpu->sim.stride];
-      atm_last_vel[i][2] = pLVel[i1 + gpu->sim.stride2];
-    }
-  }
-  else {
-    gpu->pbLVel->Download();
-    for (int i = 0; i < gpu->sim.atoms; i++) {
-      atm_last_vel[i][0] = gpu->pbLVel->_pSysData[i];
-      atm_last_vel[i][1] = gpu->pbLVel->_pSysData[i + gpu->sim.stride];
-      atm_last_vel[i][2] = gpu->pbLVel->_pSysData[i + gpu->sim.stride2];
-    }
+  // Unsorted last velocities are authoritative; download directly in original atom order
+  gpu->pbLVel->Download();
+  for (int i = 0; i < gpu->sim.atoms; i++) {
+    atm_last_vel[i][0] = gpu->pbLVel->_pSysData[i];
+    atm_last_vel[i][1] = gpu->pbLVel->_pSysData[i + gpu->sim.stride];
+    atm_last_vel[i][2] = gpu->pbLVel->_pSysData[i + gpu->sim.stride2];
   }
 }
 
@@ -4554,6 +4871,12 @@ extern "C" void gpu_nb14_setup_(int *nb14_cnt, int cit_nb14[][3], double gbl_one
                                 int cn7[], double cn8[], int *C4Pairwise, double cn114[], double cn214[])
 {
   PRINTMETHOD("gpu_nb14_setup");
+  {
+    char buf[256];
+    snprintf(buf, sizeof(buf), "gpu_nb14_setup ENTRY: atoms=%d ntypes=%d iac[0-4]=(%d,%d,%d,%d,%d)",
+             gpu->sim.atoms, *ntypes, iac[0], iac[1], iac[2], iac[3], iac[4]);
+    MINT32_LOG(buf);
+  }
   int i, j, k;
 
   // Allocate/reallocate GPU nb14 buffers
@@ -5319,6 +5642,7 @@ extern "C" void gpu_init_extra_pnts_nb14_(int* frame_cnt, ep_frame_rec* ep_frame
 extern "C" void gpu_constraints_setup_(int* natc, int* atm_jrc, double* atm_weight,
                                        double* atm_xc)
 {
+  MINT32_LOG("ENTRY gpu_constraints_setup");
   PRINTMETHOD("gpu_constraints_setup");
 
   // Delete old constraints
@@ -6219,6 +6543,7 @@ extern "C" void gpu_shake_ti_setup_(double atm_mass[], int ti_lst[][3], int ti_s
 extern "C" void gpu_final_case_inhibitors_(char errmsg[], int *errlen, int *abortsig,
                                            double atm_crd[][3])
 {
+  MINT32_LOG("ENTRY gpu_final_case_inhibitors");
   PRINTMETHOD("gpu_final_case_inhibitors");
   errmsg[0] = '\0';
 
@@ -6342,7 +6667,7 @@ extern "C" void gpu_final_case_inhibitors_(char errmsg[], int *errlen, int *abor
   }
 
   if (strlen(errmsg) != 0) {
-    printf("%s", errmsg);
+    *errlen = strlen(errmsg);
   }
   if (fatality) {
     *abortsig = 1;
@@ -6351,6 +6676,8 @@ extern "C" void gpu_final_case_inhibitors_(char errmsg[], int *errlen, int *abor
     *abortsig = 0;
   }
   *errlen = strlen(errmsg);
+  MINT32_LOG("EXIT gpu_final_case_inhibitors");
+  *errlen = strlen(errmsg);
 #else
   *errlen = 0;
   *abortsig = 0;
@@ -6584,6 +6911,14 @@ extern "C" void gpu_create_outputbuffers_()
 extern "C" void gpuCopyConstants()
 {
   PRINTMETHOD("gpuCopyConstants");
+  // Check frozen mode env var
+  const char* frozenEnv = getenv("MINT32_FROZEN_TEST");
+  if (frozenEnv && atoi(frozenEnv) > 0) {
+    gpu->sim.bFrozen = 1;
+  } else {
+    gpu->sim.bFrozen = 0;
+  }
+
 #ifdef GTI
   gpu->UpdateSimulationConst();
 #else
@@ -6636,6 +6971,14 @@ extern "C" void gpu_init_pbc_(double* a, double* b, double* c, double* alpha, do
                               double cut_factor[], double ucell[][3], double recip[][3])
 {
   PRINTMETHOD("gpu_init_pbc");
+  {
+    char buf[512];
+    snprintf(buf, sizeof(buf),
+             "gpu_init_pbc INPUT: *a=%.9f *b=%.9f *c=%.9f | pbc_box[0-2]=(%.9f,%.9f,%.9f) | ucell diag=(%.9f,%.9f,%.9f)",
+             *a, *b, *c, pbc_box[0], pbc_box[1], pbc_box[2],
+             ucell[0][0], ucell[1][1], ucell[2][2]);
+    MINT32_LOG(buf);
+  }
   gpu->bNeighborList = true;
   gpu->bNeedNewNeighborList = true;
   gpu->bNewNeighborList = false;
@@ -6672,6 +7015,25 @@ extern "C" void gpu_init_pbc_(double* a, double* b, double* c, double* alpha, do
   gpu->sim.uc_sphere = *uc_sphere;
   gpu->sim.pi_vol_inv = 1.0 / (PI * gpu->sim.uc_volume);
 
+  // MINT32: Initialize scaling factors at PBC setup time
+  // Full 32-bit range (wraparound is expected in MINT32)
+  const double int_range = 4294967296.0; // 2^32
+  gpu->sim.mint32_scale.x = int_range / gpu->sim.a;
+  gpu->sim.mint32_scale.y = int_range / gpu->sim.b;
+  gpu->sim.mint32_scale.z = int_range / gpu->sim.c;
+  gpu->sim.mint32_inv_scale.x = gpu->sim.a / int_range;
+  gpu->sim.mint32_inv_scale.y = gpu->sim.b / int_range;
+  gpu->sim.mint32_inv_scale.z = gpu->sim.c / int_range;
+  {
+    char buf[384];
+    snprintf(buf, sizeof(buf),
+             "gpu_init_pbc: a=%.9f b=%.9f c=%.9f | mint32_scale=(%.6f,%.6f,%.6f) inv=(%.12e,%.12e,%.12e)",
+             gpu->sim.a, gpu->sim.b, gpu->sim.c,
+             gpu->sim.mint32_scale.x, gpu->sim.mint32_scale.y, gpu->sim.mint32_scale.z,
+             gpu->sim.mint32_inv_scale.x, gpu->sim.mint32_inv_scale.y, gpu->sim.mint32_inv_scale.z);
+    MINT32_LOG(buf);
+  }
+
   gpuCopyConstants();
   return;
 }
@@ -6748,8 +7110,8 @@ extern "C" void gpu_neighbor_list_setup_(int numex[], int natex[], double *vdw_c
   // Delete any existing neighbor list data
   delete gpu->pbImageIndex;
   delete gpu->pbSubImageLookup;
-  delete gpu->pbAtomXYSaveSP;
-  delete gpu->pbAtomZSaveSP;
+  // Rename pbAtomCoordSave to pbAtomCoordSave_M32 for MINT32 consistency
+  delete gpu->pbAtomCoordSave_M32;
   delete gpu->pbImage;
   delete gpu->pbImageVel;
   delete gpu->pbImageLVel;
@@ -6828,6 +7190,19 @@ extern "C" void gpu_neighbor_list_setup_(int numex[], int natex[], double *vdw_c
   else {
     gpu->pbNLbSkinTestFail = new GpuBuffer<bool>(1);
   }
+  // Initialize skin test and completion flags to a known state
+  gpu->pbNLbSkinTestFail->_pSysData[0] = false;
+  if (!gpu->bCanMapHostMemory) {
+    gpu->pbNLbSkinTestFail->Upload();
+  }
+  gpu->pbKernelCompletionFlags->_pSysData[0] = 0u;
+  if (!gpu->bCanMapHostMemory) {
+    gpu->pbKernelCompletionFlags->Upload();
+  }
+  gpu->pbFinishedBlocksCounters->_pSysData[0] = 0u;
+  if (!gpu->bCanMapHostMemory) {
+    gpu->pbFinishedBlocksCounters->Upload();
+  }
 
   // Allocate cell hash
   gpu->pbNLCellHash = new GpuBuffer<unsigned int>(CELL_HASH_CELLS);
@@ -6841,6 +7216,9 @@ extern "C" void gpu_neighbor_list_setup_(int numex[], int natex[], double *vdw_c
   // Allocate new exclusion lists
   gpu->pbNLExclusionStartCount = new GpuBuffer<uint2>(gpu->sim.atoms);
 
+  // Rename pbAtomCoordSave to pbAtomCoordSave_M32 for MINT32 consistency
+  gpu->pbAtomCoordSave_M32   = new GpuBuffer<int4>(gpu->sim.stride);
+
   // Double count exclusions
   unsigned int* pExclusionCount = new unsigned int[gpu->sim.atoms];
   memset(pExclusionCount, 0, gpu->sim.atoms * sizeof(unsigned int));
@@ -7193,7 +7571,7 @@ extern "C" void gpu_neighbor_list_setup_(int numex[], int natex[], double *vdw_c
   unsigned int xDivisor     = 1;
   unsigned int xEntryWidth  = 128;
   unsigned int atomsPerCell = (gpu->sim.atoms + GRID) / gpu->sim.cells;
-  unsigned int atomsPerWarp = 16;
+  unsigned int atomsPerWarp = 32;
 
   // Adjust for crazy vacuum-filled solvent boxes, can waste memory like crazy if we hit this,
   // but we *shouldn't* be hitting this, period.
@@ -7270,14 +7648,11 @@ extern "C" void gpu_neighbor_list_setup_(int numex[], int natex[], double *vdw_c
 #else
   xEntryWidth          = 128;
 #endif
-  //yDivisor             = 8;
-  //xDivisor             = 1;
-  //atomsPerWarp         = 16;
-  // TBT TEMPORARY: Force 32 atoms per warp to avoid 16-atom MINT32 bug (doc 042)
-  yDivisor = 4;
-  xDivisor = 1;
-  atomsPerWarp = 32;
-  maxExclusionsPerWarp = maxExclusionsPerWarp16;
+  // TEMPORARY: Force 32 atoms per warp to avoid 16-atom MINT32 bug (doc 042)
+  yDivisor             = 4;
+  xDivisor             = 1;
+  atomsPerWarp         = 32;
+  maxExclusionsPerWarp = maxExclusionsPerWarp32;
   unsigned int NLMaxEntries = 4 * gpu->sim.cells *
                               ((atomsPerCell + atomsPerWarp) / atomsPerWarp) * NEIGHBOR_CELLS *
                               ((atomsPerCell + GRID) / GRID);
@@ -7370,7 +7745,12 @@ extern "C" void gpu_neighbor_list_setup_(int numex[], int natex[], double *vdw_c
   unsigned int offsetPerWarp  = GRID +                     // atom ID
                                 (gpu->sim.NLAtomsPerWarp * sizeof(PMEMask) /
                                  sizeof(unsigned int));    // masks stored as uints
-  unsigned int maxTotalOffset = 2 * workUnits * multiplier * warpsPerCell * warpsPerCell *
+  // The neighbor list can slightly exceed theoretical estimates for tightly packed cells,
+  // so add extra headroom to avoid overflow.
+  // Theoretical offset can underestimate tightly packed cells; add generous headroom to
+  // avoid buffer overflows corrupting nearby data. Empirically, factor 8 was still tight
+  // for MINT32 runs, so use 16Ã— to stay well clear.
+  unsigned int maxTotalOffset = 16 * workUnits * multiplier * warpsPerCell * warpsPerCell *
                                 offsetPerWarp;
   gpu->pbNLAtomList         = new GpuBuffer<unsigned int>(maxTotalOffset,
                                                           bShadowedOutputBuffers);
@@ -7427,8 +7807,8 @@ extern "C" void gpu_neighbor_list_setup_(int numex[], int natex[], double *vdw_c
   gpu->sim.imageStride  = ((gpu->sim.stride + 255) >> 8) << 8;
   gpu->pbImageIndex     = new GpuBuffer<unsigned int>(7 * gpu->sim.imageStride);
   gpu->pbSubImageLookup = new GpuBuffer<unsigned int>(gpu->sim.stride);
-  gpu->pbAtomXYSaveSP   = new GpuBuffer<PMEFloat2>(gpu->sim.stride);
-  gpu->pbAtomZSaveSP    = new GpuBuffer<PMEFloat>(gpu->sim.stride);
+  gpu->pbAtomCoordSave_M32   = new GpuBuffer<int4>(gpu->sim.stride);
+
   gpu->pbImage          = new GpuBuffer<double>(6 * gpu->sim.stride);
   gpu->pbImageVel       = new GpuBuffer<double>(6 * gpu->sim.stride);
   gpu->pbImageLVel      = new GpuBuffer<double>(6 * gpu->sim.stride);
@@ -7471,9 +7851,13 @@ extern "C" void gpu_neighbor_list_setup_(int numex[], int natex[], double *vdw_c
     gpu->pbImage->_pSysData[i]          = gpu->pbAtom->_pSysData[i];
     gpu->pbImage->_pSysData[istr]       = gpu->pbAtom->_pSysData[istr];
     gpu->pbImage->_pSysData[istr2]      = gpu->pbAtom->_pSysData[istr2];
-    gpu->pbAtomXYSaveSP->_pSysData[i].x = gpu->pbAtom->_pSysData[i];
-    gpu->pbAtomXYSaveSP->_pSysData[i].y = gpu->pbAtom->_pSysData[istr];
-    gpu->pbAtomZSaveSP->_pSysData[i]    = gpu->pbAtom->_pSysData[istr2];
+    double xwrap = wrap_centered(gpu->pbAtom->_pSysData[i], gpu->sim.a);
+    double ywrap = wrap_centered(gpu->pbAtom->_pSysData[i + gpu->sim.stride], gpu->sim.b);
+    double zwrap = wrap_centered(gpu->pbAtom->_pSysData[i + gpu->sim.stride2], gpu->sim.c);
+    gpu->pbAtomCoordSave_M32->_pSysData[i].x = mint32_from_double(xwrap, gpu->sim.mint32_scale.x);
+    gpu->pbAtomCoordSave_M32->_pSysData[i].y = mint32_from_double(ywrap, gpu->sim.mint32_scale.y);
+    gpu->pbAtomCoordSave_M32->_pSysData[i].z = mint32_from_double(zwrap, gpu->sim.mint32_scale.z);
+    gpu->pbAtomCoordSave_M32->_pSysData[i].w = 0;
     gpu->pbImageVel->_pSysData[i]       = gpu->pbVel->_pSysData[i];
     gpu->pbImageVel->_pSysData[istr]    = gpu->pbVel->_pSysData[istr];
     gpu->pbImageVel->_pSysData[istr2]   = gpu->pbVel->_pSysData[istr2];
@@ -7508,11 +7892,14 @@ extern "C" void gpu_neighbor_list_setup_(int numex[], int natex[], double *vdw_c
   gpu->pbImage->Upload();
   gpu->pbImageVel->Upload();
   gpu->pbImageLVel->Upload();
+  // Initialize neighbor-list reference coordinates on device
+  gpu->pbAtomCoordSave_M32->Upload();
   gpu->pbImageMass->Upload();
   gpu->pbImageCharge->Upload();
   gpu->pbImageSigEps->Upload();
   gpu->pbImageLJID->Upload();
   gpu->pbImageCellID->Upload();
+
   if (gpu->sim.ti_mode != 0) {
     gpu->pbImageTIRegion->Upload();
     if (gpu->sim.TIPaddedLinearAtmCnt > 0) {
@@ -7726,8 +8113,9 @@ extern "C" void gpu_neighbor_list_setup_(int numex[], int natex[], double *vdw_c
   gpu->sim.pImageIndex2           = gpu->pbImageIndex->_pDevData + gpu->sim.imageStride * 4;
   gpu->sim.pImageAtom2            = gpu->pbImageIndex->_pDevData + gpu->sim.imageStride * 5;
   gpu->sim.pImageHash2            = gpu->pbImageIndex->_pDevData + gpu->sim.imageStride * 6;
-  gpu->sim.pAtomXYSaveSP          = gpu->pbAtomXYSaveSP->_pDevData;
-  gpu->sim.pAtomZSaveSP           = gpu->pbAtomZSaveSP->_pDevData;
+  gpu->sim.pAtomCoordSave          = gpu->pbAtomCoordSave_M32->_pDevData;
+  gpu->sim.pAtomCoordUnsorted      = gpu->pbAtomCoordUnsorted_M32->_pDevData;  // MINT32 unsorted coords (Array A)
+
   gpu->sim.pImageX                = gpu->pbImage->_pDevData;
   gpu->sim.pImageY                = gpu->pbImage->_pDevData + gpu->sim.stride;
   gpu->sim.pImageZ                = gpu->pbImage->_pDevData + gpu->sim.stride2;
@@ -7890,10 +8278,33 @@ extern "C" void gpu_build_neighbor_list_()
   if (gpu->bNeedNewNeighborList) {
     gpu->bNeedNewNeighborList = false;
     gpu->bNewNeighborList = true;
+    // Prepare hash coordinates from current MINT32 positions to ensure hash input matches integration
+    kNLPrepareHashCoordinates(gpu);
+
     kNLGenerateSpatialHash(gpu);
+    
     kNLRadixSort(gpu);
+    
     kNLRemapImage(gpu);
+    
+    // CRITICAL: After remapping, pointer swaps happened on host side (gpu->sim).
+    // We MUST copy the updated pointers to device constant memory (cSim) so that
+    // subsequent kernels use the correct (newly sorted) arrays.
+    gpuCopyConstants();
+
+    // MINT32 FIX: Update pAtomCoordSave_M32 with newly sorted coordinates
+    // This ensures pAtomCoordSave always tracks the current remapped order
+    cudaError_t status = cudaMemcpy(gpu->pbAtomCoordSave_M32->_pDevData,
+                                    gpu->pbAtomCoord_M32->_pDevData,
+                                    gpu->sim.stride * sizeof(int4),
+                                    cudaMemcpyDeviceToDevice);
+    RTERROR(status, "cudaMemcpy pAtomCoord_M32 to pAtomCoordSave_M32 (post-remap) failed");
+
+    // Convert wrapped MINT32 to FP32 for kBNL (AFTER remap so coords are in new sorted order)
+    kMINT32ToFloat(gpu, 0);
+    
     kNLClearCellBoundaries(gpu);
+    
     kNLCalculateCellBoundaries(gpu);
 #ifdef MPI
     // Regenerate force send/receives
@@ -7946,15 +8357,75 @@ extern "C" void gpu_build_neighbor_list_()
     }
 #endif
     gpuCopyConstants();
+    
     kNLCalculateCellCoordinates(gpu);
+
     kNLBuildNeighborList(gpu);
+
+    // Optional neighbor-list stats to compare list size/shape vs. baseline
+    static int sNLStatsEnv = -1;
+    if (sNLStatsEnv == -1) {
+      const char* env = getenv("MINT32_NL_STATS");
+      sNLStatsEnv = (env && atoi(env) != 0) ? 1 : 0;
+    }
+    if (sNLStatsEnv == 1) {
+      gpu->pbNLEntries->Download();
+      gpu->pbNLTotalOffset->Download();
+      const unsigned int entries = gpu->pbNLEntries->_pSysData[0];
+      const unsigned int offset  = gpu->pbNLTotalOffset->_pSysData[0];
+      const double avg = (gpu->sim.atoms > 0) ?
+                         ((double)entries / (double)gpu->sim.atoms) : 0.0;
+      printf("NL_STATS: build entries=%u avg_per_atom=%.3f atoms=%d "
+             "atomsPerWarp=%u yDiv=%u NLRecords=%u offset_total=%u max_offset=%u "
+             "offset_per_warp=%u NLMaxEntries=%u\n",
+             entries,
+             avg,
+             gpu->sim.atoms,
+             gpu->sim.NLAtomsPerWarp,
+             gpu->sim.NLYDivisor,
+             gpu->sim.NLRecords,
+             offset,
+             gpu->sim.NLMaxTotalOffset,
+             gpu->sim.NLOffsetPerWarp,
+             gpu->sim.NLMaxEntries);
+      if (entries > 0) {
+        // Host-side xatom summary to gauge force-kernel workload
+        gpu->pbNLEntry->Download();
+        unsigned long long xatomSum = 0;
+        unsigned int xatomMax = 0;
+        const unsigned int count = std::min(entries, gpu->sim.NLMaxEntries);
+        for (unsigned int i = 0; i < count; i++) {
+          const unsigned int xa = gpu->pbNLEntry->_pSysData[i].NL.xatoms;
+          xatomSum += xa;
+          xatomMax = std::max(xatomMax, xa);
+        }
+        printf("NL_STATS_XATOM: sum=%llu max=%u avg_per_entry=%.2f\n",
+               (unsigned long long)xatomSum,
+               xatomMax,
+               (count > 0) ? ((double)xatomSum / (double)count) : 0.0);
+      }
+      if (entries > 0) {
+        NLEntry first = {};
+        cudaMemcpy(&first, gpu->sim.pNLEntry, sizeof(NLEntry), cudaMemcpyDeviceToHost);
+        printf("NL_STATS entry0: ypos=%u ymax=%u home=%u xatoms=%u offset=%u\n",
+               first.NL.ypos,
+               (first.NL.ymax >> NLENTRY_YMAX_SHIFT),
+               (unsigned)(first.NL.ymax & NLENTRY_HOME_CELL_MASK),
+               first.NL.xatoms,
+               first.NL.offset);
+      }
+    }
+
     kNLRemapLocalInteractions(gpu);
+    
     kNLRemapBondWorkUnits(gpu);
+
 #ifdef MPI
     if ((gpu->sim.ntp > 0) && (gpu->sim.barostat != 2)) {
       kClearNBForces(gpu);
     }
 #endif
+
   }
 }
 
@@ -9262,6 +9733,32 @@ extern "C" void gpu_fill_charge_grid_()
   PRINTMETHOD("gpu_fill_charge_grid");
   gpu_build_neighbor_list_();
   kPMEGetGridWeights(gpu);
+  // Optional PME input sampler (host-side) to cross-check fractional coords and charges.
+  if (getenv("MINT32_PME_SAMPLE")) {
+    gpu->pbFract->Download();
+    gpu->pbAtomChargeSP->Download();
+    const int sample = (gpu->sim.atoms < 8) ? gpu->sim.atoms : 8;
+    FILE* f = fopen("/tmp/mint32_pme_sample_host.log", "a");
+    if (f != nullptr) {
+      fprintf(f, "HOST_PME_INPUT nfft=(%d,%d,%d) atoms=%d stride=%d\n",
+              gpu->sim.nfft1, gpu->sim.nfft2, gpu->sim.nfft3, gpu->sim.atoms, gpu->sim.stride);
+      PMEFloat total_q = 0.0f;
+      for (int i = 0; i < gpu->sim.atoms; i++) total_q += gpu->pbAtomChargeSP->_pSysData[i];
+      fprintf(f, "total_charge=%.6f\n", (double)total_q);
+      for (int i = 0; i < sample; i++) {
+        PMEFloat fx = gpu->pbFract->_pSysData[i];
+        PMEFloat fy = gpu->pbFract->_pSysData[i + gpu->sim.stride];
+        PMEFloat fz = gpu->pbFract->_pSysData[i + gpu->sim.stride2];
+        PMEFloat q  = gpu->pbAtomChargeSP->_pSysData[i];
+        fprintf(f,
+                "atom %d: frac_raw=(%.6f,%.6f,%.6f) normed=(%.6f,%.6f,%.6f) charge=%.6f\n",
+                i, (double)fx, (double)fy, (double)fz,
+                (double)fx / gpu->sim.nfft1, (double)fy / gpu->sim.nfft2,
+                (double)fz / gpu->sim.nfft3, (double)q);
+      }
+      fclose(f);
+    }
+  }
   kPMEClearChargeGridBuffer(gpu);
   kPMEFillChargeGridBuffer(gpu);
   kPMEReduceChargeGridBuffer(gpu);
@@ -9511,11 +10008,13 @@ extern "C" void gpu_pme_ene_(double* ewaldcof, double* vol, pme_pot_ene_rec* pEn
                              int* ineb, int* nebfreq)
 {
   PRINTMETHOD("gpu_pme_ene");
+
   // Rebuild neighbor list
   gpu_build_neighbor_list_();
 
   // Clear forces
   kClearForces(gpu, gpu->sim.NLNonbondEnergyWarps);
+  
   if (gpu->ntf != 8) {
 
     // Download critical coordinates for force calculations to occur on the host side
@@ -9559,7 +10058,34 @@ extern "C" void gpu_pme_ene_(double* ewaldcof, double* vol, pme_pot_ene_rec* pEn
 #ifdef MPI
     if (gpu->bCalculateDirectSum) {
 #endif
+    int mint32SampleEnable = 0;
+    const char* sampleEnv = getenv("MINT32_SAMPLE_PAIRS");
+    {
+      char buf[128];
+      snprintf(buf, sizeof(buf), "MINT32_SAMPLE_PAIRS env=%s", sampleEnv ? sampleEnv : "unset");
+      MINT32_LOG(buf);
+    }
+    if (sampleEnv && atoi(sampleEnv) > 0) {
+      mint32SampleEnable = atoi(sampleEnv);
+      Mint32SampleInit(mint32SampleEnable);
+      MINT32_LOG("MINT32_SAMPLE_PAIRS enabled");
+    }
+    int mint32ChecksumEnable = Mint32ChecksumEnvValue();
+    {
+      char buf[160];
+      snprintf(buf, sizeof(buf), "MINT32_NL_CHECKSUM enable=%d", mint32ChecksumEnable);
+      MINT32_LOG(buf);
+    }
+    if (mint32ChecksumEnable > 0) {
+      Mint32NLChecksumInit(mint32ChecksumEnable);
+    }
     kCalculatePMENonbondEnergy(gpu);
+    if (mint32SampleEnable > 0) {
+      Mint32SampleDump();
+    }
+    if (mint32ChecksumEnable > 0) {
+      Mint32NLChecksumDump();
+    }
 
     // Run CPU (host)-side potential and force calculations while the non-bonded
     // force kernel is running.  This is the best opportunity to mask the cost.
@@ -9769,29 +10295,41 @@ extern "C" void gpu_pme_force_(double* ewaldcof, double* vol, double virial[3],
 #ifdef MPI
     }
     else {
-      kPMEClearChargeGridBuffer(gpu);
+    // kPMEClearChargeGridBuffer(gpu);
     }
 #endif
     // Non-bonded energy: every process will do either reciprocal space or direct sum
     // interactions and therefore must re-image coordinates.
-    kPMEGetGridWeights(gpu);
+    // kPMEGetGridWeights(gpu);
 #ifdef MPI
     if (gpu->bCalculateReciprocalSum) {
 #endif
     // Reciprocal space forces
-    kPMEFillChargeGridBuffer(gpu);
-    kPMEReduceChargeGridBuffer(gpu);
-    kPMEForwardFFT(gpu);
-    kPMEScalarSumRC(gpu, *vol);
-    kPMEBackwardFFT(gpu);
-    kPMEGradSum(gpu);
+    // kPMEFillChargeGridBuffer(gpu);
+    // kPMEReduceChargeGridBuffer(gpu);
+    // kPMEForwardFFT(gpu);
+    // kPMEScalarSumRC(gpu, *vol);
+    // kPMEBackwardFFT(gpu);
+    // kPMEGradSum(gpu);
 #ifdef MPI
     }
     if (gpu->bCalculateDirectSum) {
 #endif
 
     // Direct space forces
-    kCalculatePMENonbondForces(gpu);
+    int mint32ChecksumEnable = Mint32ChecksumEnvValue();
+    {
+      char buf[160];
+      snprintf(buf, sizeof(buf), "MINT32_NL_CHECKSUM enable=%d", mint32ChecksumEnable);
+      MINT32_LOG(buf);
+    }
+    if (mint32ChecksumEnable > 0) {
+      Mint32NLChecksumInit(mint32ChecksumEnable);
+    }
+    // kCalculatePMENonbondForces(gpu);
+    if (mint32ChecksumEnable > 0) {
+      Mint32NLChecksumDump();
+    }
 
     // Run CPU (host)-side potential and force calculations while the non-bonded
     // force kernel is running.  This is the best opportunity to mask the cost.
@@ -9951,7 +10489,34 @@ extern "C" void gpu_ti_pme_ene_(double* ewaldcof, double* vol, pme_pot_ene_rec*
 #ifdef MPI
   if (gpu->bCalculateDirectSum) {
 #endif
+  int mint32SampleEnable2 = 0;
+  const char* sampleEnv2 = getenv("MINT32_SAMPLE_PAIRS");
+  {
+    char buf[128];
+    snprintf(buf, sizeof(buf), "MINT32_SAMPLE_PAIRS env=%s", sampleEnv2 ? sampleEnv2 : "unset");
+    MINT32_LOG(buf);
+  }
+  if (sampleEnv2 && atoi(sampleEnv2) > 0) {
+    mint32SampleEnable2 = atoi(sampleEnv2);
+    Mint32SampleInit(mint32SampleEnable2);
+    MINT32_LOG("MINT32_SAMPLE_PAIRS enabled");
+  }
+  int mint32ChecksumEnable2 = Mint32ChecksumEnvValue();
+  {
+    char buf[160];
+    snprintf(buf, sizeof(buf), "MINT32_NL_CHECKSUM enable=%d", mint32ChecksumEnable2);
+    MINT32_LOG(buf);
+  }
+  if (mint32ChecksumEnable2 > 0) {
+    Mint32NLChecksumInit(mint32ChecksumEnable2);
+  }
   kCalculatePMENonbondEnergy(gpu);
+  if (mint32SampleEnable2 > 0) {
+    Mint32SampleDump();
+  }
+  if (mint32ChecksumEnable2 > 0) {
+    Mint32NLChecksumDump();
+  }
 #ifdef MPI
   }
 #endif
@@ -10232,7 +10797,19 @@ extern "C" void gpu_ti_pme_force_(double* ewaldcof, double* vol, double virial[3
 #ifdef MPI
     if (gpu->bCalculateDirectSum) {
 #endif
+    int mint32ChecksumEnable = Mint32ChecksumEnvValue();
+    {
+      char buf[160];
+      snprintf(buf, sizeof(buf), "MINT32_NL_CHECKSUM enable=%d", mint32ChecksumEnable);
+      MINT32_LOG(buf);
+    }
+    if (mint32ChecksumEnable > 0) {
+      Mint32NLChecksumInit(mint32ChecksumEnable);
+    }
     kCalculatePMENonbondForces(gpu);
+    if (mint32ChecksumEnable > 0) {
+      Mint32NLChecksumDump();
+    }
 #ifdef MPI
     }
 #endif
@@ -10603,6 +11180,24 @@ extern "C" void gpu_update_box_(double ucell[3][3], double recip[3][3], double*
       gpu->sim.cellOffset[i][2] = (PMEDouble)cellOffset[i][2] * gpu->sim.zcell;
     }
   }
+
+  // MINT32: Calculate scaling factors based on box dimensions
+  // Map box dimension to 2^32 (full integer range)
+  double box_x = ucell[0][0];
+  double box_y = ucell[1][1];
+  double box_z = ucell[2][2];
+  
+  // Use 4294967296.0 (2^32)
+  double int_range = 4294967296.0;
+
+  gpu->sim.mint32_scale.x = int_range / box_x;
+  gpu->sim.mint32_scale.y = int_range / box_y;
+  gpu->sim.mint32_scale.z = int_range / box_z;
+
+  gpu->sim.mint32_inv_scale.x = box_x / int_range;
+  gpu->sim.mint32_inv_scale.y = box_y / int_range;
+  gpu->sim.mint32_inv_scale.z = box_z / int_range;
+
   gpuCopyConstants();
 }
 
@@ -10721,6 +11316,16 @@ extern "C" void gpu_pressure_scale_(double ucell[3][3], double recip[3][3], doub
     }
     gpuCopyConstants();
   }
+  // MINT32: Update scaling factors
+  double int_range = 4294967296.0;
+  gpu->sim.mint32_scale.x = int_range / a;
+  gpu->sim.mint32_scale.y = int_range / b;
+  gpu->sim.mint32_scale.z = int_range / c;
+  gpu->sim.mint32_inv_scale.x = a / int_range;
+  gpu->sim.mint32_inv_scale.y = b / int_range;
+  gpu->sim.mint32_inv_scale.z = c / int_range;
+  gpuCopyConstants();
+
   kCalculateSoluteCOM(gpu);
   kPressureScaleCoordinates(gpu);
   if (gpu->sim.constraints > 0) {
@@ -10850,6 +11455,16 @@ extern "C" void gpu_ucell_set_(double ucell[3][3], double recip[3][3], double* u
       }
       gpuCopyConstants();
     }
+    // MINT32: Update scaling factors
+    double int_range = 4294967296.0;
+    gpu->sim.mint32_scale.x = int_range / a;
+    gpu->sim.mint32_scale.y = int_range / b;
+    gpu->sim.mint32_scale.z = int_range / c;
+    gpu->sim.mint32_inv_scale.x = a / int_range;
+    gpu->sim.mint32_inv_scale.y = b / int_range;
+    gpu->sim.mint32_inv_scale.z = c / int_range;
+    gpuCopyConstants();
+
     // Recalculate COM of everything to ensure correct virial.
     kCalculateCOM(gpu);
   }
@@ -11423,9 +12038,13 @@ extern "C" void gpu_update_phmd_(double* dtx)
 //   temp0:     target temperature
 //   gamma_ln:  Langevin collision frequency
 //---------------------------------------------------------------------------------------------
+static double saved_dt = 0.0;
+
 extern "C" void gpu_update_(double* dt, double* temp0, double* gamma_ln)
 {
+  saved_dt = *dt;
   PRINTMETHOD("gpu_update");
+  
   if (gpu->ischeme == 0) {
       kUpdate(gpu, *dt, *temp0, *gamma_ln);
   }
@@ -12674,7 +13293,19 @@ extern "C" void gpu_gb_forces_()
 extern "C" void gpu_get_nb_energy_()
 {
   PRINTMETHOD("gpu_get_nb_energy");
+  int mint32ChecksumEnable = Mint32ChecksumEnvValue();
+  {
+    char buf[160];
+    snprintf(buf, sizeof(buf), "MINT32_NL_CHECKSUM enable=%d", mint32ChecksumEnable);
+    MINT32_LOG(buf);
+  }
+  if (mint32ChecksumEnable > 0) {
+    Mint32NLChecksumInit(mint32ChecksumEnable);
+  }
   kCalculatePMENonbondForces(gpu);
+  if (mint32ChecksumEnable > 0) {
+    Mint32NLChecksumDump();
+  }
 }
 
 //---------------------------------------------------------------------------------------------
diff --git "a/C:\\amber\\amber_master\\src\\pmemd\\src/cuda/gpu.h" "b/C:\\amber\\amber_mint\\src\\pmemd\\src/cuda/gpu.h"
index 5c989c9..ae82605 100644
--- "a/C:\\amber\\amber_master\\src\\pmemd\\src/cuda/gpu.h"
+++ "b/C:\\amber\\amber_mint\\src\\pmemd\\src/cuda/gpu.h"
@@ -332,7 +332,9 @@ extern "C" void kPMEGradSum(gpuContext gpu);
 extern "C" void kPMEForwardFFT(gpuContext gpu);
 extern "C" void kPMEBackwardFFT(gpuContext gpu);
 extern "C" void kNLResetCounter();
+extern "C" void kNLPrepareHashCoordinates(gpuContext gpu);
 extern "C" void kNLGenerateSpatialHash(gpuContext gpu);
+extern "C" void kMINT32ToFloat(gpuContext gpu, int convertOld);  // Hybrid MINT32: convert MINT32 â†’ FP32
 extern "C" void kFilterImage(gpuContext gpu, bool map_ids);
 extern "C" void kExpandImage(gpuContext gpu, bool map_ids);
 extern "C" void kNLInitRadixSort(gpuContext gpu);
@@ -340,10 +342,13 @@ extern "C" void kNLDeleteRadixSort(gpuContext gpu);
 extern "C" void kNLRadixSort(gpuContext gpu);
 extern "C" void kNLRemapLocalInteractions(gpuContext gpu);
 extern "C" void kNLRemapBondWorkUnits(gpuContext gpu);
+extern "C" void kNLPrepareHashCoordinates(gpuContext gpu);
 extern "C" void kNLRemapImage(gpuContext gpu);
+extern "C" void kNLUnsortCoordinates(gpuContext gpu);
 extern "C" void kNLCalculateOffsets(gpuContext gpu);
 extern "C" void kNLCalculateCellCoordinates(gpuContext gpu);
 extern "C" void kNLBuildNeighborList(gpuContext gpu);
+extern "C" void kVelSnapshot(int tag);
 extern "C" void kNLClearCellBoundaries(gpuContext gpu);
 extern "C" void kNLCalculateCellBoundaries(gpuContext gpu);
 extern "C" void kCalculatePMENonbondEnergy(gpuContext gpu);
diff --git "a/C:\\amber\\amber_master\\src\\pmemd\\src/cuda/gpuHoneycomb.cpp" "b/C:\\amber\\amber_mint\\src\\pmemd\\src/cuda/gpuHoneycomb.cpp"
index fac8f89..62f2f70 100644
--- "a/C:\\amber\\amber_master\\src\\pmemd\\src/cuda/gpuHoneycomb.cpp"
+++ "b/C:\\amber\\amber_mint\\src\\pmemd\\src/cuda/gpuHoneycomb.cpp"
@@ -13,6 +13,9 @@
 #include "gpuContext.h"
 #include "gpuHoneycomb.h"
 
+// MINT32 debugging instrumentation
+#include "mint32_debug.h"
+
 using namespace std;
 
 //---------------------------------------------------------------------------------------------
@@ -71,19 +74,37 @@ int CountLJAtoms(gpuContext gpu)
 //---------------------------------------------------------------------------------------------
 int GetNTInstructionEst(gpuContext gpu)
 {
+  MINT32_LOG("ENTRY");
+  MINT32_LOG_DOUBLE(LowerCapBoundary, gpu->sim.LowerCapBoundary);
+  MINT32_LOG_DOUBLE(UpperCapBoundary, gpu->sim.UpperCapBoundary);
+
 #ifdef use_DPFP
   double divfac    = gpu->sim.npencils * 350;
 #else
   double divfac    = gpu->sim.npencils * 480;
 #endif
-  double cutfac    = (2.0 * gpu->sim.LowerCapBoundary) + 1.0;
+  // MINT32 fix: LowerCapBoundary and UpperCapBoundary are now boundary COORDINATES
+  // (e.g., -0.4 and 0.4) rather than padding widths. Compute the positive padding width
+  // directly from the upper boundary: capWidth = 0.5 - UpperCapBoundary
+  const double capWidth = 0.5 - gpu->sim.UpperCapBoundary;
+  MINT32_LOG_DOUBLE(capWidth, capWidth);
+
+  // Sanity check: capWidth must be positive
+  if (capWidth <= 0.0) {
+    MINT32_LOG("ERROR: Negative capWidth!");
+    exit(1);
+  }
+
+  double cutfac            = (2.0 * capWidth) + 1.0;
   double meanQQreg = (double)(gpu->sim.nQQatoms * 11) * cutfac / divfac;
   double meanLJreg = (double)(gpu->sim.nLJatoms * 11) * cutfac / divfac;
-  double qqlayers = gpu->sim.LowerCapBoundary / cutfac * meanQQreg;
-  double ljlayers = gpu->sim.LowerCapBoundary / cutfac * meanLJreg;
+  double qqlayers = capWidth / cutfac * meanQQreg;
+  double ljlayers = capWidth / cutfac * meanLJreg;
   int ntre = 2.25 * ((meanQQreg + 1.0)*qqlayers + (meanLJreg + 1.0)*ljlayers + 4.0);
   ntre *= gpu->sim.npencils;
 
+  MINT32_LOG_INT(ntre, ntre);
+  MINT32_LOG("EXIT");
   return ntre;
 }
 
@@ -724,8 +745,9 @@ static void GetSubImagePadding(gpuContext gpu, double atm_crd[][3], dmat *Uxs, d
 
   // Determine the padding factors for the X dimension (needed for pre-imaging atoms)
   double capbound = ComputeCapBoundary(lj_cutoff, nbskin, &invU);
-  gpu->sim.LowerCapBoundary = capbound;
-  gpu->sim.UpperCapBoundary = 1.0 - capbound;
+  // MINT32 requires coordinates in [-0.5, 0.5), not [0, 1)
+  gpu->sim.LowerCapBoundary = -0.5 + capbound;
+  gpu->sim.UpperCapBoundary = 0.5 - capbound;
 
   // Free allocated memory
   DestroyDmat(&U);
@@ -3322,7 +3344,24 @@ static void CheckGpuExpandedImage(gpuContext gpu, eimage *tIMG, int* limits,
 static void CheckNTStencils(gpuContext gpu, eimage *tIMG, int* limits, imat *ntrAssigned,
                             const char* subimg)
 {
+  MINT32_LOG("ENTRY");
   int i, j, k, m, n;
+  // MINT32 fix: LowerCapBoundary and UpperCapBoundary are now boundary COORDINATES
+  // (e.g., -0.4 and 0.4) rather than padding widths. Compute the positive padding width
+  // directly from the upper boundary: capWidth = 0.5 - UpperCapBoundary
+  const double capWidth   = 0.5 - gpu->sim.UpperCapBoundary;
+  MINT32_LOG_DOUBLE(capWidth, capWidth);
+
+  // Sanity check: capWidth must be positive
+  if (capWidth <= 0.0) {
+    MINT32_LOG("ERROR: Negative capWidth in CheckNTStencils!");
+    exit(1);
+  }
+
+  const double paddedSpan = 1.0 + (2.0 * capWidth);
+  MINT32_LOG_DOUBLE(paddedSpan, paddedSpan);
+  // MINT32 uses [-0.5, 0.5) coordinates, so fracShift = 0.5 to map back to [0, 1) for segment indexing
+  const PMEFloat fracShift = (PMEFloat)0.5;
 
   // Get the lists of pencil relationships.  Here, ntPencils will again be the list of 11
   // imported hash cells relative to some home cell, ntHandShakes will be the interactions
@@ -3395,11 +3434,32 @@ static void CheckNTStencils(gpuContext gpu, eimage *tIMG, int* limits, imat *ntr
     // Determine whether the atoms will all fit, or if multiple sections are needed.
     int nsegment = 1;
     if (ntAtomCount > NT_MAX_ATOMS) {
+      MINT32_LOG("NT segmentation loop starting");
+      MINT32_LOG_INT(ntAtomCount, ntAtomCount);
+      MINT32_LOG_INT(NT_MAX_ATOMS, NT_MAX_ATOMS);
       nsegment = (ntAtomCount + NT_MAX_ATOMS - 1) / NT_MAX_ATOMS;
+      MINT32_LOG_INT(nsegment, nsegment);
       bool success = false;
+      int loop_count = 0;
       while (!success) {
+        if (++loop_count > 1000) {
+          MINT32_LOG("ERROR: Infinite loop detected!");
+          fprintf(stderr, "ERROR: Infinite loop detected in NT segmentation! loop_count=%d, nsegment=%d\n", loop_count, nsegment);
+          exit(1);
+        }
+        if (loop_count % 100 == 0) {
+          MINT32_LOG_INT(loop_iteration, loop_count);
+        }
         success = true;
-        PMEFloat invTWidth = (PMEFloat)nsegment / (1.0 + 2.0 * gpu->sim.LowerCapBoundary);
+        PMEFloat invTWidth = (PMEFloat)nsegment / (PMEFloat)paddedSpan;
+
+        // Sanity check: invTWidth must be positive
+        if (invTWidth <= 0.0) {
+          fprintf(stderr, "ERROR: Negative or zero invTWidth (%f) detected. nsegment = %d, paddedSpan = %f\n",
+                  invTWidth, nsegment, paddedSpan);
+          exit(1);
+        }
+
         SetIVec(segpop.data, segpop.row * segpop.col, 0);
         for (j = 0; j < 11; j++) {
           int pcell = ntCollection.map[j][2];
@@ -3409,7 +3469,8 @@ static void CheckNTStencils(gpuContext gpu, eimage *tIMG, int* limits, imat *ntr
             PMEFloat atmy = packet.y;
             PMEFloat atmz = packet.z;
             PMEFloat rbx = U.data[0]*atmx + U.data[3]*atmy + U.data[6]*atmz;
-            int irbx = (rbx + gpu->sim.LowerCapBoundary) * invTWidth;
+            PMEFloat rbxNorm = rbx + fracShift;
+            int irbx = (rbxNorm + (PMEFloat)capWidth) * invTWidth;
             segloc.map[j][k - tIMG->cells[pcell].lcapStart] = irbx;
           }
           for (k = 0; k < nsegment; k++) {
@@ -3447,8 +3508,15 @@ static void CheckNTStencils(gpuContext gpu, eimage *tIMG, int* limits, imat *ntr
       nspan = 0;
     }
     else {
-      nspan = ceil(gpu->sim.LowerCapBoundary * (double)nsegment /
-                   (1.0 + (2.0 * gpu->sim.LowerCapBoundary)));
+      nspan = ceil(capWidth * (double)nsegment /
+                   (1.0 + (2.0 * capWidth)));
+
+      // Sanity check: nspan must be non-negative
+      if (nspan < 0) {
+        fprintf(stderr, "ERROR: Negative nspan (%d) detected. capWidth = %f, nsegment = %d\n",
+                nspan, capWidth, nsegment);
+        exit(1);
+      }
     }
 
     // Determine the compartmentalization of all hash cells.  Where does each
diff --git "a/C:\\amber\\amber_master\\src\\pmemd\\src/cuda/gputypes.h" "b/C:\\amber\\amber_mint\\src\\pmemd\\src/cuda/gputypes.h"
index 2891838..a6008fc 100644
--- "a/C:\\amber\\amber_master\\src\\pmemd\\src/cuda/gputypes.h"
+++ "b/C:\\amber\\amber_mint\\src\\pmemd\\src/cuda/gputypes.h"
@@ -455,8 +455,8 @@ static const int IPSNONBONDFORCES_THREADS_PER_BLOCK       = 1024;
 static const int IPSNONBONDENERGY_BLOCKS_MULTIPLIER       = 1;
 static const int TRANSPOSE_QMESH_THREADS_PER_BLOCK        = 64;
 #else
-static const int PMENONBONDFORCES_THREADS_PER_BLOCK       = 128;
-static const int PMENONBONDENERGY_THREADS_PER_BLOCK       = 128;
+static const int PMENONBONDFORCES_THREADS_PER_BLOCK       = 64;
+static const int PMENONBONDENERGY_THREADS_PER_BLOCK       = 64;
 static const int GBBORNRADII_THREADS_PER_BLOCK            = 64;
 static const int GBNONBONDENERGY1_THREADS_PER_BLOCK       = 64;
 static const int GBNONBONDENERGY2_THREADS_PER_BLOCK       = 64;
@@ -734,6 +734,8 @@ GL_CONST PMEFloat VIRIALSCALEF = (PMEFloat)VSCALE;
 GL_CONST PMEDouble ONEOVERVIRIALSCALE = (PMEDouble)1.0 / (PMEDouble)VSCALE;
 GL_CONST PMEFloat  ONEOVERVIRIALSCALEF = (PMEDouble)1.0 / (PMEDouble)VSCALE;
 
+// MINT32 now uses dynamic per-dimension scaling stored in cSim.mint32_scale
+// and cSim.mint32_inv_scale (calculated as 2^32 / box_dimension)
 GL_CONST unsigned int REAL32_NONZERO_MASK           = ((1u << 31) - 1);
 GL_CONST unsigned long long int REAL64_NONZERO_MASK = ((1ull << 63) - 1);
 
diff --git "a/C:\\amber\\amber_master\\src\\pmemd\\src/cuda/gti_f95.cpp" "b/C:\\amber\\amber_mint\\src\\pmemd\\src/cuda/gti_f95.cpp"
index 14fa59e..c577c6a 100644
--- "a/C:\\amber\\amber_master\\src\\pmemd\\src/cuda/gti_f95.cpp"
+++ "b/C:\\amber\\amber_mint\\src\\pmemd\\src/cuda/gti_f95.cpp"
@@ -3,12 +3,34 @@
 #include <vector>
 #include <algorithm>
 #include <stdexcept> 
+#include <cstdio>
+#include <cstdlib>
 #include "gpu.h"
 #include "gti_gpu.h"
 #include "gti_cuda.cuh"
 #include "gti_schedule_functions.h"
 #include "gti_f95.h"
 
+// Optional debug dump of raw energy terms downloaded from the GPU.
+// Controlled via env var MINT32_DUMP_ENERGY (nonzero => dump to /tmp/mint32_energy.log).
+static void Mint32DumpEnergy(int step, gpuContext gpu, const double* energy, int count, bool masterNode)
+{
+  const char* env = getenv("MINT32_DUMP_ENERGY");
+  if (env == nullptr || atoi(env) == 0) return;
+
+  FILE* f = fopen("/tmp/mint32_energy.log", "a");
+  if (f == nullptr) return;
+  fprintf(f, "step %d gpu %d master %d vdw_recip=%g ee_plasma=%g\n",
+          step, gpu->gpuID, masterNode ? 1 : 0, gpu->vdw_recip, gpu->ee_plasma);
+  int perLine = 6;
+  for (int i = 0; i < count; i++) {
+    fprintf(f, "e[%02d]=%.10g%s", i, energy[i], ((i % perLine) == perLine - 1) ? "\n" : " ");
+  }
+  if ((count % perLine) != 0) fprintf(f, "\n");
+  fflush(f);
+  fclose(f);
+}
+
 //---------------------------------------------------------------------------------------------
 // gti_sync_vector_:
 //
@@ -1791,6 +1813,12 @@ extern "C" void gti_bonded_(bool* needPotEnergy, bool* need_virials, int* ti_mod
 
   gpuContext gpu = theGPUContext::GetPointer();
 
+  // REMOVED: The original comment was incorrect. kPMEGetGridWeights does NOT overwrite
+  // pAtomCoord_M32 - it only writes to pAtomXYSP/pAtomZSP (cell-relative for kBNL).
+  // Restoring from pAtomCoordSave_M32 would clobber POST-SHAKE coordinates with
+  // PRE-SHAKE coordinates, causing velocity recalculation errors.
+  // See: The save buffer is updated in kU.h BEFORE SHAKE, not after.
+
   if (gpu->iphmd > 0) {
     kExecuteBondWorkUnits(gpu, 1);
   }
@@ -1891,6 +1919,7 @@ extern "C" void gti_update_md_ene_(pme_pot_ene_rec* pEnergy, double enmr[3], dou
   bool masterNode= (!useMPI || gpu->gpuID == 0);
 
   icc_GetEnergyFromGPU(gpu, energy);
+  Mint32DumpEnergy(*nstep, gpu, energy, EXTENDED_ENERGY_TERMS, masterNode);
   pEnergy->vdw_dir = energy[1];
   pEnergy->vdw_recip = gpu->vdw_recip;
   pEnergy->vdw_tot = pEnergy->vdw_dir;
diff --git "a/C:\\amber\\amber_master\\src\\pmemd\\src/cuda/kBNL.h" "b/C:\\amber\\amber_mint\\src\\pmemd\\src/cuda/kBNL.h"
index 7857ba1..d3c3093 100644
--- "a/C:\\amber\\amber_master\\src\\pmemd\\src/cuda/kBNL.h"
+++ "b/C:\\amber\\amber_mint\\src\\pmemd\\src/cuda/kBNL.h"
@@ -47,6 +47,7 @@
   const int THREADS_PER_BLOCK = NLBUILD_NEIGHBORLIST8_THREADS_PER_BLOCK;
 #endif
   __shared__ VOLATILE BNLWarp sNLWarp[THREADS_PER_BLOCK / GRID];
+  __shared__ float sBBox[THREADS_PER_BLOCK / GRID * 6];  // Bbox fix: shared memory for Y-atom bbox
 #ifdef PME_VIRIAL
   __shared__ float sCutPlusSkin2;
   __shared__ float sUcellf[9];
@@ -97,18 +98,31 @@
   // The union shNlRecord is used for various pieces of information, both floats
   // and unsigned integers, and in only one critical case is it ever used to
   // interpret one as the other.
-  while (__SHFL(WARP_MASK, shNlRecord.u, NEIGHBOR_CELLS + 2) < cSim.NLRecords) {
+  // MINT32 FIX: Calculate activemask before main loop
+  unsigned int activeMask = __activemask();
+  
+  // DEBUG: Print from first thread to confirm kernel launch
+  if (threadIdx.x == 0 && blockIdx.x == 0) {
+  }
+  
+  while (__SHFL(activeMask, shNlRecord.u, NEIGHBOR_CELLS + 2) < cSim.NLRecords) {
+    // DEBUG: Print from block 0 every iteration
+    if (blockIdx.x == 0 && threadIdx.x == (NEIGHBOR_CELLS + 2)) {
+    }
+    
+    // MINT32 FIX (doc 026/044): Refresh activemask to track divergence
+    activeMask = __activemask();
     unsigned int tgx = threadIdx.x & GRID_BITS_MASK;
 
     // Read NLRecord information
-    unsigned int pos1 = __SHFL(WARP_MASK, shNlRecord.u, NEIGHBOR_CELLS + 2);
+    unsigned int pos1 = __SHFL(activeMask, shNlRecord.u, NEIGHBOR_CELLS + 2);
     if (tgx < NEIGHBOR_CELLS + 2) {
       shNlRecord.u = cSim.pNLRecord[pos1].array[tgx];
     }
 
     // Calculate Exclusion/neighbor list space required
-    int atomOffset = __SHFL(WARP_MASK, shNlRecord.u, NEIGHBOR_CELLS + 1) >> NLRECORD_YOFFSET_SHIFT;
-    uint2 homeCell = cSim.pNLNonbondCellStartEnd[__SHFL(WARP_MASK, shNlRecord.u, NEIGHBOR_CELLS)];
+    int atomOffset = __SHFL(activeMask, shNlRecord.u, NEIGHBOR_CELLS + 1) >> NLRECORD_YOFFSET_SHIFT;
+    uint2 homeCell = cSim.pNLNonbondCellStartEnd[__SHFL(activeMask, shNlRecord.u, NEIGHBOR_CELLS)];
 
     if (tgx == (NEIGHBOR_CELLS + 4)) {
       shNlRecord.u = homeCell.x;
@@ -116,8 +130,8 @@
     if (tgx == (NEIGHBOR_CELLS + 5)) {
       shNlRecord.u = homeCell.y;
     }
-    int ysize = max(0, (int)(__SHFL(WARP_MASK, shNlRecord.u, NEIGHBOR_CELLS + 5) -
-                             __SHFL(WARP_MASK, shNlRecord.u, NEIGHBOR_CELLS + 4) -
+    int ysize = max(0, (int)(__SHFL(activeMask, shNlRecord.u, NEIGHBOR_CELLS + 5) -
+                             __SHFL(activeMask, shNlRecord.u, NEIGHBOR_CELLS + 4) -
                              atomOffset * cSim.NLAtomsPerWarp));
     if (ysize > 0) {
       ysize = 1 + max(0, ysize - 1) / (cSim.NLAtomsPerWarp * cSim.NLYDivisor);
@@ -127,8 +141,8 @@
     // cSim.pNLRecord[pos1].array[tgx] for tgx < 16, homeCell.x for tgx == 18, and homeCell.y
     // for tgx == 19.  The value broadcast for setting cells on all threads is going to be
     // for tgx == 15, which is the final cell with which .
-    unsigned int cells = __SHFL(WARP_MASK, shNlRecord.u, NEIGHBOR_CELLS + 1) & NLRECORD_CELL_COUNT_MASK;
-    PMEMask mask1 = __BALLOT(WARP_MASK, (tgx < cells) );
+    unsigned int cells = __SHFL(activeMask, shNlRecord.u, NEIGHBOR_CELLS + 1) & NLRECORD_CELL_COUNT_MASK;
+    PMEMask mask1 = __BALLOT(activeMask, (tgx < cells) );
     if (tgx < cells) {
       uint2 cell = cSim.pNLNonbondCellStartEnd[__SHFL(mask1, shNlRecord.u, tgx) >>
                                                NLRECORD_CELL_SHIFT];
@@ -139,32 +153,51 @@
     else {
       psWarp->atomList[tgx] = 0;
     }
-    __SYNCWARP(WARP_MASK);
+    __SYNCWARP(activeMask);
     // Reduce xsize down to thread 0 of each warp
     uint temp;
     temp = psWarp->atomList[tgx];
 #if (NEIGHBOR_CELLS > 16)
-    temp += __SHFL_DOWN(WARP_MASK, temp, 16);
+    temp += __SHFL_DOWN(activeMask, temp, 16);
 #endif
     for (int offset = 8; offset > 0; offset /= 2)
-      temp += __SHFL_DOWN(WARP_MASK, temp, offset);
+      temp += __SHFL_DOWN(activeMask, temp, offset);
     psWarp->atomList[tgx]=temp;
-    __SYNCWARP(WARP_MASK);
+    __SYNCWARP(activeMask);
 
     if (tgx == (NEIGHBOR_CELLS + 2)) {
       uint totalXSize = ((psWarp->atomList[0] + GRID - 1) >> GRID_BITS);
-      uint offset = atomicAdd(cSim.pNLTotalOffset,
-                              totalXSize*ysize*cSim.NLOffsetPerWarp + cSim.NLAtomsPerWarp);
-      psWarp->offset = offset;
+
+      // DEFENSIVE: Check for suspiciously large totalXSize indicating corrupted cell boundary data
+      if (totalXSize > (uint)cSim.atoms * 2) {
+        printf("ERROR kBNL Block %d: totalXSize=%u suspiciously large (atoms=%d, atomList[0]=%u, ysize=%d)\n",
+               blockIdx.x, totalXSize, cSim.atoms, psWarp->atomList[0], ysize);
+        psWarp->offset = 0;  // Use safe default to prevent crash
+      }
+      else {
+        uint requestedSize = totalXSize*ysize*cSim.NLOffsetPerWarp + cSim.NLAtomsPerWarp;
+        uint offset = atomicAdd(cSim.pNLTotalOffset, requestedSize);
+
+        // Check if we exceeded neighbor list buffer
+        if (offset + requestedSize > (uint)cSim.NLMaxTotalOffset) {
+          printf("ERROR kBNL Block %d: NL buffer overflow! offset=%u + requested=%u > max=%d\n",
+                 blockIdx.x, offset, requestedSize, cSim.NLMaxTotalOffset);
+          // Signal fatal overflow - set to max value to stop other blocks
+          atomicMax(cSim.pNLTotalOffset, 0x7FFFFFFFu);
+        }
+        psWarp->offset = offset;
+      }
     }
-    __SYNCWARP(WARP_MASK);
+    __SYNCWARP(activeMask);
 
     // Generate actual neighbor list entry
-    uint ypos = __SHFL(WARP_MASK, shNlRecord.u, NEIGHBOR_CELLS + 4) +
-                ((__SHFL(WARP_MASK, shNlRecord.u,
+    uint ypos = __SHFL(activeMask, shNlRecord.u, NEIGHBOR_CELLS + 4) +
+                ((__SHFL(activeMask, shNlRecord.u,
                          NEIGHBOR_CELLS + 1) >> NLRECORD_YOFFSET_SHIFT) * cSim.NLAtomsPerWarp);
-    uint homecellY = __SHFL(WARP_MASK, shNlRecord.u, NEIGHBOR_CELLS + 5);
+    uint homecellY = __SHFL(activeMask, shNlRecord.u, NEIGHBOR_CELLS + 5);
     while ( ypos < homecellY ) {
+      // MINT32 FIX: Refresh activemask for each y-swath iteration
+      activeMask = __activemask();
 
       // Calculate y bounds and set to calculate homecell interaction
       uint ymax = min(ypos + cSim.NLAtomsPerWarp, homecellY);
@@ -174,7 +207,7 @@
         psWarp->nlEntry.NL.xatoms = 0;
         psWarp->nlEntry.NL.offset = psWarp->offset;
       }
-      __SYNCWARP(WARP_MASK);
+      __SYNCWARP(activeMask);
 
       // Read y atoms
       float xi;
@@ -209,24 +242,25 @@
       // Calculate bounding box on SM 2.0 and up
       float bmin = (index < ymax) ? 0.5f * xi :  999999.0f;
       float bmax = (index < ymax) ? 0.5f * xi : -999999.0f;
-      bmin = min(__SHFL(WARP_MASK, bmin, tgx ^ 1), bmin);
-      bmin = min(__SHFL(WARP_MASK, bmin, tgx ^ 2), bmin);
-      bmin = min(__SHFL(WARP_MASK, bmin, tgx ^ 4), bmin);
+      __SYNCWARP(activeMask); // MINT32 FIX: Sync before reduction
+      bmin = min(__SHFL(activeMask, bmin, tgx ^ 1), bmin);
+      bmin = min(__SHFL(activeMask, bmin, tgx ^ 2), bmin);
+      bmin = min(__SHFL(activeMask, bmin, tgx ^ 4), bmin);
 #if (PME_ATOMS_PER_WARP >= 16)
-      bmin = min(__SHFL(WARP_MASK, bmin, tgx ^ 8), bmin);
+      bmin = min(__SHFL(activeMask, bmin, tgx ^ 8), bmin);
 #endif
 #if (PME_ATOMS_PER_WARP == 32)
-      bmin = min(__SHFL(WARP_MASK, bmin, tgx ^ 16), bmin);
+      bmin = min(__SHFL(activeMask, bmin, tgx ^ 16), bmin);
 #endif
-      bmax = max(__SHFL(WARP_MASK, bmax, tgx ^ 1), bmax);
-      bmax = max(__SHFL(WARP_MASK, bmax, tgx ^ 2), bmax);
-      bmax = max(__SHFL(WARP_MASK, bmax, tgx ^ 4), bmax);
+      bmax = max(__SHFL(activeMask, bmax, tgx ^ 1), bmax);
+      bmax = max(__SHFL(activeMask, bmax, tgx ^ 2), bmax);
+      bmax = max(__SHFL(activeMask, bmax, tgx ^ 4), bmax);
 
 #if (PME_ATOMS_PER_WARP >= 16)
-      bmax = max(__SHFL(WARP_MASK, bmax, tgx ^ 8), bmax);
+      bmax = max(__SHFL(activeMask, bmax, tgx ^ 8), bmax);
 #endif
 #if (PME_ATOMS_PER_WARP == 32)
-      bmax = max(__SHFL(WARP_MASK, bmax, tgx ^ 16), bmax);
+      bmax = max(__SHFL(activeMask, bmax, tgx ^ 16), bmax);
 #endif
       if (tgx == NEIGHBOR_CELLS + 6) {
         shNlRecord.f = bmax + bmin;
@@ -236,23 +270,23 @@
       }
       bmin = (index < ymax) ? 0.5f * yi :  999999.0f;
       bmax = (index < ymax) ? 0.5f * yi : -999999.0f;
-      bmin = min(__SHFL(WARP_MASK, bmin, tgx ^ 1), bmin);
-      bmin = min(__SHFL(WARP_MASK, bmin, tgx ^ 2), bmin);
-      bmin = min(__SHFL(WARP_MASK, bmin, tgx ^ 4), bmin);
+      bmin = min(__SHFL(activeMask, bmin, tgx ^ 1), bmin);
+      bmin = min(__SHFL(activeMask, bmin, tgx ^ 2), bmin);
+      bmin = min(__SHFL(activeMask, bmin, tgx ^ 4), bmin);
 #if (PME_ATOMS_PER_WARP >= 16)
-      bmin = min(__SHFL(WARP_MASK, bmin, tgx ^ 8), bmin);
+      bmin = min(__SHFL(activeMask, bmin, tgx ^ 8), bmin);
 #endif
 #if (PME_ATOMS_PER_WARP == 32)
-      bmin = min(__SHFL(WARP_MASK, bmin, tgx ^ 16), bmin);
+      bmin = min(__SHFL(activeMask, bmin, tgx ^ 16), bmin);
 #endif
-      bmax = max(__SHFL(WARP_MASK, bmax, tgx ^ 1), bmax);
-      bmax = max(__SHFL(WARP_MASK, bmax, tgx ^ 2), bmax);
-      bmax = max(__SHFL(WARP_MASK, bmax, tgx ^ 4), bmax);
+      bmax = max(__SHFL(activeMask, bmax, tgx ^ 1), bmax);
+      bmax = max(__SHFL(activeMask, bmax, tgx ^ 2), bmax);
+      bmax = max(__SHFL(activeMask, bmax, tgx ^ 4), bmax);
 #if (PME_ATOMS_PER_WARP >= 16)
-      bmax = max(__SHFL(WARP_MASK, bmax, tgx ^ 8), bmax);
+      bmax = max(__SHFL(activeMask, bmax, tgx ^ 8), bmax);
 #endif
 #if (PME_ATOMS_PER_WARP == 32)
-      bmax = max(__SHFL(WARP_MASK, bmax, tgx ^ 16), bmax);
+      bmax = max(__SHFL(activeMask, bmax, tgx ^ 16), bmax);
 #endif
       if (tgx == NEIGHBOR_CELLS + 8) {
         shNlRecord.f = bmax + bmin;
@@ -262,23 +296,23 @@
       }
       bmin = (index < ymax) ? 0.5f * zi :  999999.0f;
       bmax = (index < ymax) ? 0.5f * zi : -999999.0f;
-      bmin = min(__SHFL(WARP_MASK, bmin, tgx ^ 1), bmin);
-      bmin = min(__SHFL(WARP_MASK, bmin, tgx ^ 2), bmin);
-      bmin = min(__SHFL(WARP_MASK, bmin, tgx ^ 4), bmin);
+      bmin = min(__SHFL(activeMask, bmin, tgx ^ 1), bmin);
+      bmin = min(__SHFL(activeMask, bmin, tgx ^ 2), bmin);
+      bmin = min(__SHFL(activeMask, bmin, tgx ^ 4), bmin);
 #if (PME_ATOMS_PER_WARP >= 16)
-      bmin = min(__SHFL(WARP_MASK, bmin, tgx ^ 8), bmin);
+      bmin = min(__SHFL(activeMask, bmin, tgx ^ 8), bmin);
 #endif
 #if (PME_ATOMS_PER_WARP == 32)
-      bmin = min(__SHFL(WARP_MASK, bmin, tgx ^ 16), bmin);
+      bmin = min(__SHFL(activeMask, bmin, tgx ^ 16), bmin);
 #endif
-      bmax = max(__SHFL(WARP_MASK, bmax, tgx ^ 1), bmax);
-      bmax = max(__SHFL(WARP_MASK, bmax, tgx ^ 2), bmax);
-      bmax = max(__SHFL(WARP_MASK, bmax, tgx ^ 4), bmax);
+      bmax = max(__SHFL(activeMask, bmax, tgx ^ 1), bmax);
+      bmax = max(__SHFL(activeMask, bmax, tgx ^ 2), bmax);
+      bmax = max(__SHFL(activeMask, bmax, tgx ^ 4), bmax);
 #if (PME_ATOMS_PER_WARP >= 16)
-      bmax = max(__SHFL(WARP_MASK, bmax, tgx ^ 8), bmax);
+      bmax = max(__SHFL(activeMask, bmax, tgx ^ 8), bmax);
 #endif
 #if (PME_ATOMS_PER_WARP == 32)
-      bmax = max(__SHFL(WARP_MASK, bmax, tgx ^ 16), bmax);
+      bmax = max(__SHFL(activeMask, bmax, tgx ^ 16), bmax);
 #endif
       if (tgx == NEIGHBOR_CELLS + 10) {
         shNlRecord.f = bmax + bmin;
@@ -287,6 +321,32 @@
         shNlRecord.f = bmax - bmin;
       }
 
+      // Bbox fix: Write bbox values to shared memory instead of relying on shuffle from union
+      unsigned int warpId = threadIdx.x / GRID;
+      unsigned int bboxBase = warpId * 6;
+
+      // MINT32 FIX: Move SHFL out of divergent branches to prevent deadlock
+      float bxc = __SHFL(activeMask, shNlRecord.f, NEIGHBOR_CELLS + 6);
+      float bxr = __SHFL(activeMask, shNlRecord.f, NEIGHBOR_CELLS + 7);
+      float byc = __SHFL(activeMask, shNlRecord.f, NEIGHBOR_CELLS + 8);
+      float byr = __SHFL(activeMask, shNlRecord.f, NEIGHBOR_CELLS + 9);
+      float bzc = __SHFL(activeMask, shNlRecord.f, NEIGHBOR_CELLS + 10);
+      float bzr = __SHFL(activeMask, shNlRecord.f, NEIGHBOR_CELLS + 11);
+
+      if (tgx == 0) {
+        sBBox[bboxBase + 0] = bxc;
+        sBBox[bboxBase + 1] = bxr;
+      }
+      if (tgx == 1) {
+        sBBox[bboxBase + 2] = byc;
+        sBBox[bboxBase + 3] = byr;
+      }
+      if (tgx == 2) {
+        sBBox[bboxBase + 4] = bzc;
+        sBBox[bboxBase + 5] = bzr;
+      }
+      __SYNCWARP(activeMask);
+
       // Read exclusions into L1 or shared memory
       if (tgx < ymax - ypos) {
         uint atom = cSim.pImageAtom[index];
@@ -298,7 +358,7 @@
         psWarp->atomList[tgx] = 0;
         psWarp->exclusionMask[tgx] = 0;
       }
-      __SYNCWARP(WARP_MASK);
+      __SYNCWARP(activeMask);
 
       uint totalExclusions = 0;
       unsigned int minExclusion = cSim.atoms;
@@ -316,19 +376,19 @@
         }
         totalExclusions += count;
       }
-      minExclusion = min(__SHFL(WARP_MASK, minExclusion, tgx ^ 1), minExclusion);
-      minExclusion = min(__SHFL(WARP_MASK, minExclusion, tgx ^ 2), minExclusion);
-      minExclusion = min(__SHFL(WARP_MASK, minExclusion, tgx ^ 4), minExclusion);
-      minExclusion = min(__SHFL(WARP_MASK, minExclusion, tgx ^ 8), minExclusion);
-      minExclusion = min(__SHFL(WARP_MASK, minExclusion, tgx ^ 16), minExclusion);
-      maxExclusion = max(__SHFL(WARP_MASK, maxExclusion, tgx ^ 1), maxExclusion);
-      maxExclusion = max(__SHFL(WARP_MASK, maxExclusion, tgx ^ 2), maxExclusion);
-      maxExclusion = max(__SHFL(WARP_MASK, maxExclusion, tgx ^ 4), maxExclusion);
-      maxExclusion = max(__SHFL(WARP_MASK, maxExclusion, tgx ^ 8), maxExclusion);
-      maxExclusion = max(__SHFL(WARP_MASK, maxExclusion, tgx ^ 16), maxExclusion);
+      minExclusion = min(__SHFL(activeMask, minExclusion, tgx ^ 1), minExclusion);
+      minExclusion = min(__SHFL(activeMask, minExclusion, tgx ^ 2), minExclusion);
+      minExclusion = min(__SHFL(activeMask, minExclusion, tgx ^ 4), minExclusion);
+      minExclusion = min(__SHFL(activeMask, minExclusion, tgx ^ 8), minExclusion);
+      minExclusion = min(__SHFL(activeMask, minExclusion, tgx ^ 16), minExclusion);
+      maxExclusion = max(__SHFL(activeMask, maxExclusion, tgx ^ 1), maxExclusion);
+      maxExclusion = max(__SHFL(activeMask, maxExclusion, tgx ^ 2), maxExclusion);
+      maxExclusion = max(__SHFL(activeMask, maxExclusion, tgx ^ 4), maxExclusion);
+      maxExclusion = max(__SHFL(activeMask, maxExclusion, tgx ^ 8), maxExclusion);
+      maxExclusion = max(__SHFL(activeMask, maxExclusion, tgx ^ 16), maxExclusion);
 #ifdef AMBER_PLATFORM_AMD_WARP64
-      minExclusion = min(__SHFL(WARP_MASK, minExclusion, tgx ^ 32), minExclusion);
-      maxExclusion = max(__SHFL(WARP_MASK, maxExclusion, tgx ^ 32), maxExclusion);
+      minExclusion = min(__SHFL(activeMask, minExclusion, tgx ^ 32), minExclusion);
+      maxExclusion = max(__SHFL(activeMask, maxExclusion, tgx ^ 32), maxExclusion);
 #endif
       if (tgx == NEIGHBOR_CELLS + 12) {
         shNlRecord.u = minExclusion;
@@ -338,9 +398,9 @@
       }
 
       // Initialize Neighbor List variables for current line of entry
-      __SYNCWARP(0xFFFFFFFF);
+      __SYNCWARP(activeMask);  // MINT32 FIX: Use activemask instead of 0xFFFFFFFF
       psWarp->exclusionMask[tgx] = 0;
-      __SYNCWARP(WARP_MASK);
+      __SYNCWARP(activeMask);
       unsigned int cpos = 0;
       unsigned int atoms = 0;
       uint minAtom = cSim.atoms;
@@ -348,16 +408,18 @@
       PMEMask threadmask = (PMEMask)1 << tgx;
 
       while (cpos < cells) {
+        // MINT32 FIX: Refresh activemask for each cell iteration
+        activeMask = __activemask();
 
         // Check for home cell
-        shCellID = __SHFL(WARP_MASK, shNlRecord.u, cpos) & NLRECORD_CELL_TYPE_MASK;
+        shCellID = __SHFL(activeMask, shNlRecord.u, cpos) & NLRECORD_CELL_TYPE_MASK;
         uint xpos;
 
         // Cell 0 always starts along force matrix diagonal
         if ((cpos == 0) && (psWarp->nlEntry.NL.ymax & NLENTRY_HOME_CELL_MASK)) {
           // Calculate exclusions assuming all atoms are in range of each other
           psWarp->exclusionMask[tgx] = 0;
-          __SYNCWARP(WARP_MASK);
+          __SYNCWARP(activeMask);
           for (int i = tgx; i < totalExclusions; i += GRID) {
             uint atom = psExclusion[i] >> NLEXCLUSION_SHIFT;
             if ((atom >= ypos) && (atom < ymax)) {
@@ -366,28 +428,36 @@
                        ((PMEMask)1 << (psExclusion[i] & NLEXCLUSION_ATOM_MASK)));
             }
           }
-          __SYNCWARP(WARP_MASK);
+          __SYNCWARP(activeMask);
 
           // Output exclusion masks
           if (tgx < cSim.NLAtomsPerWarp) {
             PMEMask mask = psWarp->exclusionMask[tgx];
             mask = ((mask >> (1 + tgx)) | (mask << (cSim.NLAtomsPerWarp - tgx - 1))) &
                    cSim.NLAtomsPerWarpMask;
-            cSim.pNLAtomList[psWarp->offset + tgx] = mask;
+            // DEFENSIVE: Bounds check before write
+            if (psWarp->offset + tgx < (uint)cSim.NLMaxTotalOffset) {
+              cSim.pNLAtomList[psWarp->offset + tgx] = mask;
+            } else {
+              printf("ERROR kBNL Block %d: Write out of bounds at offset=%u + %u\n",
+                     blockIdx.x, psWarp->offset, tgx);
+            }
           }
-          __SYNCWARP(WARP_MASK);
+          __SYNCWARP(activeMask);
           if (tgx == 0)
             psWarp->offset += cSim.NLAtomsPerWarp;
-          __SYNCWARP(WARP_MASK);
+          __SYNCWARP(activeMask);
           xpos = ypos + cSim.NLAtomsPerWarp;
         }
         else {
-          xpos = __SHFL(WARP_MASK, shCell.x, cpos);
+          xpos = __SHFL(activeMask, shCell.x, cpos);
         }
 
         // Read x atoms
-        unsigned int CELLY_cpos = __SHFL(WARP_MASK, shCell.y, cpos);
+        unsigned int CELLY_cpos = __SHFL(activeMask, shCell.y, cpos);
         while (xpos < CELLY_cpos) {
+          // MINT32 FIX: Refresh activemask for each x-swath iteration
+          activeMask = __activemask();
 
           // Calculate number of atoms in this iteration
           uint xmax = min(xpos + GRID, CELLY_cpos) - xpos;
@@ -433,13 +503,16 @@
 #  endif
 #endif
           // Bounding box test on SM 2.0+
+          // Bbox fix: Read from shared memory instead of shuffle from union
           float trivialCut2 = 0.5625f * cutPlusSkin2;
-          float bxc = __SHFL(WARP_MASK, shNlRecord.f, NEIGHBOR_CELLS + 6);
-          float bxr = __SHFL(WARP_MASK, shNlRecord.f, NEIGHBOR_CELLS + 7);
-          float byc = __SHFL(WARP_MASK, shNlRecord.f, NEIGHBOR_CELLS + 8);
-          float byr = __SHFL(WARP_MASK, shNlRecord.f, NEIGHBOR_CELLS + 9);
-          float bzc = __SHFL(WARP_MASK, shNlRecord.f, NEIGHBOR_CELLS + 10);
-          float bzr = __SHFL(WARP_MASK, shNlRecord.f, NEIGHBOR_CELLS + 11);
+          unsigned int warpId = threadIdx.x / GRID;
+          unsigned int bboxBase = warpId * 6;
+          float bxc = sBBox[bboxBase + 0];
+          float bxr = sBBox[bboxBase + 1];
+          float byc = sBBox[bboxBase + 2];
+          float byr = sBBox[bboxBase + 3];
+          float bzc = sBBox[bboxBase + 4];
+          float bzr = sBBox[bboxBase + 5];
           float tx = fabs(sAtomx - bxc);
           float ty = fabs(sAtomy - byc);
           float tz = fabs(sAtomz - bzc);
@@ -447,8 +520,8 @@
           ty = ty - min(ty, byr);
           tz = tz - min(tz, bzr);
           float tr2 = tx*tx + ty*ty + tz*tz;
-          PMEMask bpred = __BALLOT(WARP_MASK, (tgx < xmax) && (tr2 < cutPlusSkin2));
-          PMEMask apred = __BALLOT(WARP_MASK, (tgx < xmax) && (tr2 < trivialCut2));
+          PMEMask bpred = __BALLOT(activeMask, (tgx < xmax) && (tr2 < cutPlusSkin2));
+          PMEMask apred = __BALLOT(activeMask, (tgx < xmax) && (tr2 < trivialCut2));
 
           // Bitwise exclusive OR plus assignment compound operator ^=
           // This is bpred = bpred ^ apred
@@ -458,6 +531,8 @@
           PMEMask mask = (PMEMask)cSim.NLAtomsPerWarpMask <<
                          (PME_ATOMS_PER_WARP * (tgx >> cSim.NLAtomsPerWarpBits));
           while (bpred) {
+            // MINT32 FIX: Shadow activeMask inside divergent loop
+            unsigned int activeMask = __activemask();
             int pos = maskFfs(bpred) - 1;
             bpred &= ~(PMEMask)1 << pos;
             // HIP-TODO: Support PME_ATOMS_PER_WARP = 32 and 8
@@ -509,9 +584,9 @@
             }
 #endif
 
-            float ax = __SHFL(WARP_MASK, sAtomx, pos);
-            float ay = __SHFL(WARP_MASK, sAtomy, pos);
-            float az = __SHFL(WARP_MASK, sAtomz, pos);
+            float ax = __SHFL(activeMask, sAtomx, pos);
+            float ay = __SHFL(activeMask, sAtomy, pos);
+            float az = __SHFL(activeMask, sAtomz, pos);
             int pred = 0;
             if (pos >= 0) {
               float dx = xi - ax;
@@ -522,27 +597,27 @@
             }
 
             // Signal acceptance or rejection of atoms
-            if (__BALLOT(WARP_MASK, pred) & mask) {
+            if (__BALLOT(activeMask, pred) & mask) {
               apred |= (PMEMask)1 << pos;
             }
           }
-          __SYNCWARP(WARP_MASK);
+          __SYNCWARP(activeMask);
           // HIP-TODO: Support PME_ATOMS_PER_WARP = 32 and 8
 #if (PME_ATOMS_PER_WARP < 32)
           psWarp->exclusionMask[tgx] = apred;
-          __SYNCWARP(WARP_MASK);
+          __SYNCWARP(activeMask);
 #if (PME_ATOMS_PER_WARP == 8)
           psWarp->exclusionMask[tgx] |= psWarp->exclusionMask[tgx ^ 8];
-          __SYNCWARP(WARP_MASK);
+          __SYNCWARP(activeMask);
           psWarp->exclusionMask[tgx] |= psWarp->exclusionMask[tgx ^ 16];
-          __SYNCWARP(WARP_MASK);
+          __SYNCWARP(activeMask);
           apred = psWarp->exclusionMask[tgx];
 #else
 #ifdef AMBER_PLATFORM_AMD_WARP64
           psWarp->exclusionMask[tgx] |= psWarp->exclusionMask[tgx ^ 16];
-          __SYNCWARP(WARP_MASK);
+          __SYNCWARP(activeMask);
           psWarp->exclusionMask[tgx] |= psWarp->exclusionMask[tgx ^ 32];
-          __SYNCWARP(WARP_MASK);
+          __SYNCWARP(activeMask);
           apred = psWarp->exclusionMask[tgx];
 #else
           apred |= psWarp->exclusionMask[tgx ^ 16];
@@ -553,6 +628,8 @@
 
           // Add all accepted atoms to atom list
           while (bpred) {
+            // MINT32 FIX: Shadow activeMask inside divergent loop
+            unsigned int activeMask = __activemask();
             int maxAccepts = min(GRID - atoms, maskPopc(bpred));
             int pos = -1;
 
@@ -575,55 +652,61 @@
               // Bitwise exclusive OR plus assignment compound operator
               bpred ^= threadmask;
             }
-            __SYNCWARP(WARP_MASK);
+            __SYNCWARP(activeMask);
             atoms += maxAccepts;
 
             // Output GRID atoms if ready
             if (atoms == GRID) {
 
               // Write swath of atoms to global memory
-              cSim.pNLAtomList[psWarp->offset + tgx] = psWarp->atomList[tgx];
-              __SYNCWARP(WARP_MASK);
+              // DEFENSIVE: Bounds check before write
+              if (psWarp->offset + tgx < (uint)cSim.NLMaxTotalOffset) {
+                cSim.pNLAtomList[psWarp->offset + tgx] = psWarp->atomList[tgx];
+              } else {
+                printf("ERROR kBNL Block %d: Write out of bounds (GRID) at offset=%u + %u\n",
+                       blockIdx.x, psWarp->offset, tgx);
+              }
+              __SYNCWARP(activeMask);
               if (tgx == 0) {
                 psWarp->offset += GRID;
                 psWarp->nlEntry.NL.xatoms += GRID;
               }
-              __SYNCWARP(WARP_MASK);
+              __SYNCWARP(activeMask);
 
               // Clear used bits from bpred
-              bpred &= __SHFL(WARP_MASK, bpred, tgx ^ 1);
-              bpred &= __SHFL(WARP_MASK, bpred, tgx ^ 2);
-              bpred &= __SHFL(WARP_MASK, bpred, tgx ^ 4);
-              bpred &= __SHFL(WARP_MASK, bpred, tgx ^ 8);
-              bpred &= __SHFL(WARP_MASK, bpred, tgx ^ 16);
+              bpred &= __SHFL(activeMask, bpred, tgx ^ 1);
+              bpred &= __SHFL(activeMask, bpred, tgx ^ 2);
+              bpred &= __SHFL(activeMask, bpred, tgx ^ 4);
+              bpred &= __SHFL(activeMask, bpred, tgx ^ 8);
+              bpred &= __SHFL(activeMask, bpred, tgx ^ 16);
 #ifdef AMBER_PLATFORM_AMD_WARP64
-              bpred &= __SHFL(WARP_MASK, bpred, tgx ^ 32);
+              bpred &= __SHFL(activeMask, bpred, tgx ^ 32);
 #endif
 
               // Reduce minatom and maxatom
-              minAtom = min(minAtom, __SHFL(WARP_MASK, minAtom, tgx ^ 1));
-              minAtom = min(minAtom, __SHFL(WARP_MASK, minAtom, tgx ^ 2));
-              minAtom = min(minAtom, __SHFL(WARP_MASK, minAtom, tgx ^ 4));
-              minAtom = min(minAtom, __SHFL(WARP_MASK, minAtom, tgx ^ 8));
-              minAtom = min(minAtom, __SHFL(WARP_MASK, minAtom, tgx ^ 16));
-              maxAtom = max(maxAtom, __SHFL(WARP_MASK, maxAtom, tgx ^ 1));
-              maxAtom = max(maxAtom, __SHFL(WARP_MASK, maxAtom, tgx ^ 2));
-              maxAtom = max(maxAtom, __SHFL(WARP_MASK, maxAtom, tgx ^ 4));
-              maxAtom = max(maxAtom, __SHFL(WARP_MASK, maxAtom, tgx ^ 8));
-              maxAtom = max(maxAtom, __SHFL(WARP_MASK, maxAtom, tgx ^ 16));
+              minAtom = min(minAtom, __SHFL(activeMask, minAtom, tgx ^ 1));
+              minAtom = min(minAtom, __SHFL(activeMask, minAtom, tgx ^ 2));
+              minAtom = min(minAtom, __SHFL(activeMask, minAtom, tgx ^ 4));
+              minAtom = min(minAtom, __SHFL(activeMask, minAtom, tgx ^ 8));
+              minAtom = min(minAtom, __SHFL(activeMask, minAtom, tgx ^ 16));
+              maxAtom = max(maxAtom, __SHFL(activeMask, maxAtom, tgx ^ 1));
+              maxAtom = max(maxAtom, __SHFL(activeMask, maxAtom, tgx ^ 2));
+              maxAtom = max(maxAtom, __SHFL(activeMask, maxAtom, tgx ^ 4));
+              maxAtom = max(maxAtom, __SHFL(activeMask, maxAtom, tgx ^ 8));
+              maxAtom = max(maxAtom, __SHFL(activeMask, maxAtom, tgx ^ 16));
 #ifdef AMBER_PLATFORM_AMD_WARP64
-              minAtom = min(minAtom, __SHFL(WARP_MASK, minAtom, tgx ^ 32));
-              maxAtom = max(maxAtom, __SHFL(WARP_MASK, maxAtom, tgx ^ 32));
+              minAtom = min(minAtom, __SHFL(activeMask, minAtom, tgx ^ 32));
+              maxAtom = max(maxAtom, __SHFL(activeMask, maxAtom, tgx ^ 32));
 #endif
 
               // Search for y atom exclusions matching any x atom all at once (this should
               // reduce exclusion tests by a factor of approximately 100 overall).
               // But first, rule out skipping exclusion test.
-              uint minExclusion = __SHFL(WARP_MASK, shNlRecord.u, NEIGHBOR_CELLS + 12);
-              uint maxExclusion = __SHFL(WARP_MASK, shNlRecord.u, NEIGHBOR_CELLS + 13);
+              uint minExclusion = __SHFL(activeMask, shNlRecord.u, NEIGHBOR_CELLS + 12);
+              uint maxExclusion = __SHFL(activeMask, shNlRecord.u, NEIGHBOR_CELLS + 13);
 
               psWarp->exclusionMask[tgx] = 0;
-              __SYNCWARP(WARP_MASK);
+              __SYNCWARP(activeMask);
               if ((minAtom <= maxExclusion) && (maxAtom >= minExclusion)) {
                 uint atom = (psWarp->atomList[tgx] >> NLATOM_CELL_SHIFT);
                 for (int j = 0; j < totalExclusions; j += GRID) {
@@ -632,7 +715,7 @@
                   if (offset < totalExclusions) {
                     eatom = psExclusion[offset] >> NLEXCLUSION_SHIFT;
                   }
-                  PMEMask vote = __BALLOT(WARP_MASK, (eatom >= minAtom) && (eatom <= maxAtom));
+                  PMEMask vote = __BALLOT(activeMask, (eatom >= minAtom) && (eatom <= maxAtom));
 
                   while (vote) {
                     unsigned int k = maskFfs(vote) - 1;
@@ -642,28 +725,35 @@
                       psWarp->exclusionMask[psExclusion[offset] & NLEXCLUSION_ATOM_MASK] |=
                         threadmask;
                     }
-                    __SYNCWARP(WARP_MASK);
+                    __SYNCWARP(activeMask);
                     vote ^= (PMEMask)1 << k;
                   }
                 }
               }
-              __SYNCWARP(WARP_MASK);
+              __SYNCWARP(activeMask);
 
               // Output exclusion masks
               if (tgx < cSim.NLAtomsPerWarp) {
                 PMEMask emask = psWarp->exclusionMask[tgx];
-                ((PMEMask*)&cSim.pNLAtomList[psWarp->offset])[tgx] =
-                  ((emask >> tgx) & exclusionMask) |
-                  ((emask << (cSim.NLAtomsPerWarp - tgx)) & ~exclusionMask);
+                // DEFENSIVE: Bounds check before write (accounting for PMEMask size)
+                uint maskOffset = psWarp->offset + tgx * sizeof(PMEMask) / sizeof(unsigned int);
+                if (maskOffset < (uint)cSim.NLMaxTotalOffset) {
+                  ((PMEMask*)&cSim.pNLAtomList[psWarp->offset])[tgx] =
+                    ((emask >> tgx) & exclusionMask) |
+                    ((emask << (cSim.NLAtomsPerWarp - tgx)) & ~exclusionMask);
+                } else {
+                  printf("ERROR kBNL Block %d: Mask write out of bounds at offset=%u (mask element %u)\n",
+                         blockIdx.x, maskOffset, tgx);
+                }
               }
-              __SYNCWARP(WARP_MASK);
+              __SYNCWARP(activeMask);
               if (tgx == 0)
                 psWarp->offset += cSim.NLAtomsPerWarp * sizeof(PMEMask) / sizeof(unsigned int);
               atoms = 0;
               psWarp->atomList[tgx] = 0;
               minAtom = cSim.atoms;
               maxAtom = 0;
-              __SYNCWARP(WARP_MASK);
+              __SYNCWARP(activeMask);
 
               // Output neighbor list entry if width is sufficient
               if (psWarp->nlEntry.NL.xatoms >= cSim.NLXEntryWidth) {
@@ -671,11 +761,11 @@
                 if (tgx == 0) {
                   nlpos = atomicAdd(cSim.pNLEntries, 1);
                 }
-                nlpos = __SHFL(WARP_MASK, nlpos, 0);
+                nlpos = __SHFL(activeMask, nlpos, 0);
                 if (tgx < 4) {
                   cSim.pNLEntry[nlpos].array[tgx] = psWarp->nlEntry.array[tgx];
                 }
-                __SYNCWARP(0xFFFFFFFF);
+                __SYNCWARP(activeMask);  // MINT32 FIX: Use activemask instead of 0xFFFFFFFF
                 if (tgx == 0) {
                   psWarp->nlEntry.NL.xatoms  = 0;
                   psWarp->nlEntry.NL.ymax   &= ~NLENTRY_HOME_CELL_MASK;
@@ -697,40 +787,48 @@
         // Move to next cell
         cpos++;
       }
+      // MINT32 FIX: Refresh activemask after inner loop to restore full mask
+      activeMask = __activemask();
 
       // Output last batch of atoms for this swath
       if (atoms > 0) {
         // Reduce minatom and maxatom
-        minAtom = min(minAtom, __SHFL(WARP_MASK, minAtom, tgx ^ 1));
-        minAtom = min(minAtom, __SHFL(WARP_MASK, minAtom, tgx ^ 2));
-        minAtom = min(minAtom, __SHFL(WARP_MASK, minAtom, tgx ^ 4));
-        minAtom = min(minAtom, __SHFL(WARP_MASK, minAtom, tgx ^ 8));
-        minAtom = min(minAtom, __SHFL(WARP_MASK, minAtom, tgx ^ 16));
-        maxAtom = max(maxAtom, __SHFL(WARP_MASK, maxAtom, tgx ^ 1));
-        maxAtom = max(maxAtom, __SHFL(WARP_MASK, maxAtom, tgx ^ 2));
-        maxAtom = max(maxAtom, __SHFL(WARP_MASK, maxAtom, tgx ^ 4));
-        maxAtom = max(maxAtom, __SHFL(WARP_MASK, maxAtom, tgx ^ 8));
-        maxAtom = max(maxAtom, __SHFL(WARP_MASK, maxAtom, tgx ^ 16));
+        minAtom = min(minAtom, __SHFL(activeMask, minAtom, tgx ^ 1));
+        minAtom = min(minAtom, __SHFL(activeMask, minAtom, tgx ^ 2));
+        minAtom = min(minAtom, __SHFL(activeMask, minAtom, tgx ^ 4));
+        minAtom = min(minAtom, __SHFL(activeMask, minAtom, tgx ^ 8));
+        minAtom = min(minAtom, __SHFL(activeMask, minAtom, tgx ^ 16));
+        maxAtom = max(maxAtom, __SHFL(activeMask, maxAtom, tgx ^ 1));
+        maxAtom = max(maxAtom, __SHFL(activeMask, maxAtom, tgx ^ 2));
+        maxAtom = max(maxAtom, __SHFL(activeMask, maxAtom, tgx ^ 4));
+        maxAtom = max(maxAtom, __SHFL(activeMask, maxAtom, tgx ^ 8));
+        maxAtom = max(maxAtom, __SHFL(activeMask, maxAtom, tgx ^ 16));
 #ifdef AMBER_PLATFORM_AMD_WARP64
-        minAtom = min(minAtom, __SHFL(WARP_MASK, minAtom, tgx ^ 32));
-        maxAtom = max(maxAtom, __SHFL(WARP_MASK, maxAtom, tgx ^ 32));
+        minAtom = min(minAtom, __SHFL(activeMask, minAtom, tgx ^ 32));
+        maxAtom = max(maxAtom, __SHFL(activeMask, maxAtom, tgx ^ 32));
 #endif
-        if (tgx < atoms) {
-          cSim.pNLAtomList[psWarp->offset + tgx] = psWarp->atomList[tgx];
-        }
-        else {
-          cSim.pNLAtomList[psWarp->offset + tgx] = 0;
+        // DEFENSIVE: Bounds check before write
+        if (psWarp->offset + tgx < (uint)cSim.NLMaxTotalOffset) {
+          if (tgx < atoms) {
+            cSim.pNLAtomList[psWarp->offset + tgx] = psWarp->atomList[tgx];
+          }
+          else {
+            cSim.pNLAtomList[psWarp->offset + tgx] = 0;
+          }
+        } else {
+          printf("ERROR kBNL Block %d: Final write out of bounds at offset=%u + %u\n",
+                 blockIdx.x, psWarp->offset, tgx);
         }
-        __SYNCWARP(WARP_MASK);
+        __SYNCWARP(activeMask);
         if (tgx == 0)
           psWarp->offset += GRID;
 
         // Search for y atom exclusions matching any x atom all at once (this should
         // reduce exclusion tests by a factor of approximately 100 overall)
-        uint minExclusion = __SHFL(WARP_MASK, shNlRecord.u, NEIGHBOR_CELLS + 12);
-        uint maxExclusion = __SHFL(WARP_MASK, shNlRecord.u, NEIGHBOR_CELLS + 13);
+        uint minExclusion = __SHFL(activeMask, shNlRecord.u, NEIGHBOR_CELLS + 12);
+        uint maxExclusion = __SHFL(activeMask, shNlRecord.u, NEIGHBOR_CELLS + 13);
         psWarp->exclusionMask[tgx] = 0;
-        __SYNCWARP(WARP_MASK);
+        __SYNCWARP(activeMask);
         if ((tgx < atoms) && ((minAtom <= maxExclusion) && (maxAtom >= minExclusion))) {
           unsigned int atom = (psWarp->atomList[tgx] >> NLATOM_CELL_SHIFT);
           for (int j = 0; j < totalExclusions; j++) {
@@ -740,16 +838,23 @@
             }
           }
         }
-        __SYNCWARP(WARP_MASK);
+        __SYNCWARP(activeMask);
 
         // Output exclusion masks
         if (tgx < cSim.NLAtomsPerWarp) {
           PMEMask emask = psWarp->exclusionMask[tgx];
-          ((PMEMask*)&cSim.pNLAtomList[psWarp->offset])[tgx] =
-            ((emask >> tgx) & exclusionMask) |
-            ((emask << (cSim.NLAtomsPerWarp - tgx)) & ~exclusionMask);
+          // DEFENSIVE: Bounds check before write (accounting for PMEMask size)
+          uint maskOffset = psWarp->offset + tgx * sizeof(PMEMask) / sizeof(unsigned int);
+          if (maskOffset < (uint)cSim.NLMaxTotalOffset) {
+            ((PMEMask*)&cSim.pNLAtomList[psWarp->offset])[tgx] =
+              ((emask >> tgx) & exclusionMask) |
+              ((emask << (cSim.NLAtomsPerWarp - tgx)) & ~exclusionMask);
+          } else {
+            printf("ERROR kBNL Block %d: Final mask write out of bounds at offset=%u (mask element %u)\n",
+                   blockIdx.x, maskOffset, tgx);
+          }
         }
-        __SYNCWARP(WARP_MASK);
+        __SYNCWARP(activeMask);
         if (tgx == 0) {
           psWarp->nlEntry.NL.xatoms += atoms;
           psWarp->offset += cSim.NLAtomsPerWarp * sizeof(PMEMask) / sizeof(unsigned int);
@@ -757,7 +862,7 @@
         atoms  = 0;
         psWarp->atomList[tgx] = 0;
       }
-      __SYNCWARP(0xFFFFFFFF);
+      __SYNCWARP(activeMask);  // MINT32 FIX: Use activemask instead of 0xFFFFFFFF
       // End contingency for committing the final batch of atoms
 
       // Output final neighbor list entry if xatoms > 0
@@ -766,21 +871,25 @@
         if (tgx == 0) {
           nlpos = atomicAdd(cSim.pNLEntries, 1);
         }
-        nlpos = __SHFL(WARP_MASK, nlpos, 0);
+        nlpos = __SHFL(activeMask, nlpos, 0);
         if (tgx < 4) {
           cSim.pNLEntry[nlpos].array[tgx] = psWarp->nlEntry.array[tgx];
         }
       }
-      __SYNCWARP(WARP_MASK);
+      __SYNCWARP(activeMask);
 
       // Advance to next swath of atoms
       ypos += cSim.NLYDivisor * cSim.NLAtomsPerWarp;
     }
 
     // Advance to next NLRecord entry
+    if (blockIdx.x == 0 && threadIdx.x == (NEIGHBOR_CELLS + 2)) {
+    }
     if (tgx == (NEIGHBOR_CELLS + 2)) {
       shNlRecord.u = atomicAdd(&cSim.pFrcBlkCounters[0], 1);
     }
+    // MINT32 FIX: Refresh activemask for next loop iteration check
+    activeMask = __activemask();
   }
 #undef VOLATILE
 }
diff --git "a/C:\\amber\\amber_mint\\src\\pmemd\\src/cuda/kBNL.h.backup" "b/C:\\amber\\amber_mint\\src\\pmemd\\src/cuda/kBNL.h.backup"
new file mode 100644
index 0000000..93b549f
--- /dev/null
+++ "b/C:\\amber\\amber_mint\\src\\pmemd\\src/cuda/kBNL.h.backup"
@@ -0,0 +1,847 @@
+#include "copyright.i"
+
+//---------------------------------------------------------------------------------------------
+// AMBER NVIDIA CUDA GPU IMPLEMENTATION: PMEMD VERSION
+//
+// July 2017, by Scott Le Grand, David S. Cerutti, Daniel J. Mermelstein, Charles Lin, and
+//               Ross C. Walker
+//---------------------------------------------------------------------------------------------
+
+//---------------------------------------------------------------------------------------------
+// This file is included by kNeighborList.cu multiple times with different pre-processor
+// definitions (PME_VIRIAL, PME_IS_ORTHOGONAL, PME_ATOMS_PER_WARP) to generate multiple
+// implementations of kNLBuildNeighborList_????_kernel().
+//---------------------------------------------------------------------------------------------
+{
+#if !defined(AMBER_PLATFORM_AMD)
+#  define VOLATILE volatile
+#else
+#  define VOLATILE
+#endif
+  struct BNLAtom {
+    float x;     //
+    float y;     // Coordinates of the atom
+    float z;     //
+  };
+
+  union uintfloat {
+    uint u;      // Makes it possible to read a floating point number
+    float f;     //   as an unsigned integer
+  };
+
+  struct BNLWarp {
+    uint offset;
+    uint atomList[GRID];
+    PMEMask exclusionMask[GRID];
+    NLEntry nlEntry;
+  };
+
+  uint2 shCell;
+  uint shCellID;
+  VOLATILE uintfloat shNlRecord;
+#if (PME_ATOMS_PER_WARP == 32)
+  const int THREADS_PER_BLOCK = NLBUILD_NEIGHBORLIST32_THREADS_PER_BLOCK;
+#elif (PME_ATOMS_PER_WARP == 16)
+  const int THREADS_PER_BLOCK = NLBUILD_NEIGHBORLIST16_THREADS_PER_BLOCK;
+#else
+  const int THREADS_PER_BLOCK = NLBUILD_NEIGHBORLIST8_THREADS_PER_BLOCK;
+#endif
+  __shared__ VOLATILE BNLWarp sNLWarp[THREADS_PER_BLOCK / GRID];
+  __shared__ float sBBox[THREADS_PER_BLOCK / GRID * 6];  // 6 bbox values per warp
+#ifdef PME_VIRIAL
+  __shared__ float sCutPlusSkin2;
+  __shared__ float sUcellf[9];
+#  ifdef PME_IS_ORTHOGONAL
+  __shared__ float3 sCellOffset[NEIGHBOR_CELLS];
+#  endif
+#endif
+#ifdef PME_VIRIAL
+  if (threadIdx.x < 9) {
+    sUcellf[threadIdx.x] = cSim.pNTPData->ucellf[threadIdx.x];
+  }
+  if (threadIdx.x == 32) {
+    sCutPlusSkin2 = cSim.pNTPData->cutPlusSkin2;
+  }
+#ifdef PME_IS_ORTHOGONAL
+  __syncthreads();
+  if (threadIdx.x < NEIGHBOR_CELLS) {
+    sCellOffset[threadIdx.x].x = sUcellf[0] * cSim.cellOffset[threadIdx.x][0];
+    sCellOffset[threadIdx.x].y = sUcellf[4] * cSim.cellOffset[threadIdx.x][1];
+    sCellOffset[threadIdx.x].z = sUcellf[8] * cSim.cellOffset[threadIdx.x][2];
+  }
+#endif
+  __syncthreads();
+  float cutPlusSkin2 = sCutPlusSkin2;
+#else
+  float cutPlusSkin2 = cSim.cutPlusSkin2;
+#endif
+  unsigned int warp = threadIdx.x >> GRID_BITS;
+  VOLATILE BNLWarp* psWarp = &sNLWarp[warp];
+  unsigned int globalWarp = warp + ((blockIdx.x * blockDim.x) >> GRID_BITS);
+  unsigned int* psExclusion = &cSim.pBNLExclusionBuffer[globalWarp * cSim.NLExclusionBufferSize];
+  if ((threadIdx.x & GRID_BITS_MASK) == (NEIGHBOR_CELLS + 2)) {
+    shNlRecord.u = (blockIdx.x * blockDim.x + threadIdx.x) >> GRID_BITS;
+  }
+  PMEMask exclusionMask =
+    cSim.NLAtomsPerWarpMask >> (threadIdx.x & cSim.NLAtomsPerWarpBitsMask);
+#if (PME_ATOMS_PER_WARP == 16)
+  exclusionMask = exclusionMask | (exclusionMask << 16);
+#elif (PME_ATOMS_PER_WARP == 8)
+  exclusionMask = exclusionMask | (exclusionMask << 8) |
+                  (exclusionMask << 16) | (exclusionMask << 24);
+#endif
+#ifdef AMBER_PLATFORM_AMD_WARP64
+  exclusionMask = exclusionMask | (exclusionMask << 32);
+#endif
+
+  // This is the beginning of a loop that extends to the end of the library.
+  // The union shNlRecord is used for various pieces of information, both floats
+  // and unsigned integers, and in only one critical case is it ever used to
+  // interpret one as the other.
+  while (__SHFL(WARP_MASK, shNlRecord.u, NEIGHBOR_CELLS + 2) < cSim.NLRecords) {
+    unsigned int tgx = threadIdx.x & GRID_BITS_MASK;
+
+    // Read NLRecord information
+    unsigned int pos1 = __SHFL(WARP_MASK, shNlRecord.u, NEIGHBOR_CELLS + 2);
+    if (tgx < NEIGHBOR_CELLS + 2) {
+      shNlRecord.u = cSim.pNLRecord[pos1].array[tgx];
+    }
+
+    // Calculate Exclusion/neighbor list space required
+    int atomOffset = __SHFL(WARP_MASK, shNlRecord.u, NEIGHBOR_CELLS + 1) >> NLRECORD_YOFFSET_SHIFT;
+    uint2 homeCell = cSim.pNLNonbondCellStartEnd[__SHFL(WARP_MASK, shNlRecord.u, NEIGHBOR_CELLS)];
+
+    if (tgx == (NEIGHBOR_CELLS + 4)) {
+      shNlRecord.u = homeCell.x;
+    }
+    if (tgx == (NEIGHBOR_CELLS + 5)) {
+      shNlRecord.u = homeCell.y;
+    }
+    int ysize = max(0, (int)(__SHFL(WARP_MASK, shNlRecord.u, NEIGHBOR_CELLS + 5) -
+                             __SHFL(WARP_MASK, shNlRecord.u, NEIGHBOR_CELLS + 4) -
+                             atomOffset * cSim.NLAtomsPerWarp));
+    if (ysize > 0) {
+      ysize = 1 + max(0, ysize - 1) / (cSim.NLAtomsPerWarp * cSim.NLYDivisor);
+    }
+
+    // Calculate maximum required space: shNlRecord's unsigned int has been assigned to
+    // cSim.pNLRecord[pos1].array[tgx] for tgx < 16, homeCell.x for tgx == 18, and homeCell.y
+    // for tgx == 19.  The value broadcast for setting cells on all threads is going to be
+    // for tgx == 15, which is the final cell with which .
+    unsigned int cells = __SHFL(WARP_MASK, shNlRecord.u, NEIGHBOR_CELLS + 1) & NLRECORD_CELL_COUNT_MASK;
+    PMEMask mask1 = __BALLOT(WARP_MASK, (tgx < cells) );
+    if (tgx < cells) {
+      uint2 cell = cSim.pNLNonbondCellStartEnd[__SHFL(mask1, shNlRecord.u, tgx) >>
+                                               NLRECORD_CELL_SHIFT];
+      shCell.x = cell.x;
+      shCell.y = cell.y;
+      psWarp->atomList[tgx] = cell.y - cell.x;
+    }
+    else {
+      psWarp->atomList[tgx] = 0;
+    }
+    __SYNCWARP(WARP_MASK);
+    // Reduce xsize down to thread 0 of each warp
+    uint temp;
+    temp = psWarp->atomList[tgx];
+#if (NEIGHBOR_CELLS > 16)
+    temp += __SHFL_DOWN(WARP_MASK, temp, 16);
+#endif
+    for (int offset = 8; offset > 0; offset /= 2)
+      temp += __SHFL_DOWN(WARP_MASK, temp, offset);
+    psWarp->atomList[tgx]=temp;
+    __SYNCWARP(WARP_MASK);
+
+    if (tgx == (NEIGHBOR_CELLS + 2)) {
+      uint totalXSize = ((psWarp->atomList[0] + GRID - 1) >> GRID_BITS);
+      uint offset = atomicAdd(cSim.pNLTotalOffset,
+                              totalXSize*ysize*cSim.NLOffsetPerWarp + cSim.NLAtomsPerWarp);
+      psWarp->offset = offset;
+    }
+    __SYNCWARP(WARP_MASK);
+
+    // Generate actual neighbor list entry
+    uint ypos = __SHFL(WARP_MASK, shNlRecord.u, NEIGHBOR_CELLS + 4) +
+                ((__SHFL(WARP_MASK, shNlRecord.u,
+                         NEIGHBOR_CELLS + 1) >> NLRECORD_YOFFSET_SHIFT) * cSim.NLAtomsPerWarp);
+    uint homecellY = __SHFL(WARP_MASK, shNlRecord.u, NEIGHBOR_CELLS + 5);
+    while ( ypos < homecellY ) {
+
+      // Calculate y bounds and set to calculate homecell interaction
+      uint ymax = min(ypos + cSim.NLAtomsPerWarp, homecellY);
+      if (tgx == 0) {
+        psWarp->nlEntry.NL.ypos = ypos;
+        psWarp->nlEntry.NL.ymax = (ymax << NLENTRY_YMAX_SHIFT) | NLENTRY_HOME_CELL_MASK;
+        psWarp->nlEntry.NL.xatoms = 0;
+        psWarp->nlEntry.NL.offset = psWarp->offset;
+      }
+      __SYNCWARP(WARP_MASK);
+
+      // Read y atoms
+      float xi;
+      float yi;
+      float zi;
+      int xi_i32 = 0;
+      int yi_i32 = 0;
+      int zi_i32 = 0;
+      int validY = 0;
+      unsigned int index = ypos + (tgx & (cSim.NLAtomsPerWarpBitsMask));
+
+      if (index < ymax) {
+        int4 coord = cSim.pAtomCoord_M32[index];
+        xi_i32 = coord.x;
+        yi_i32 = coord.y;
+        zi_i32 = coord.z;
+        xi = (PMEFloat)xi_i32 * cSim.mint32_inv_scale.x;
+        yi = (PMEFloat)yi_i32 * cSim.mint32_inv_scale.y;
+        zi = (PMEFloat)zi_i32 * cSim.mint32_inv_scale.z;
+        validY = 1;
+      }
+      else {
+        xi = (float)10000.0 * index;
+        yi = (float)10000.0 * index;
+        zi = (float)10000.0 * index;
+        xi_i32 = 0;
+        yi_i32 = 0;
+        zi_i32 = 0;
+        validY = 0;
+      }
+
+#ifndef PME_IS_ORTHOGONAL
+      // Transform into cartesian space
+#ifdef PME_VIRIAL
+      xi = sUcellf[0]*xi + sUcellf[1]*yi + sUcellf[2]*zi;
+      yi = sUcellf[4]*yi + sUcellf[5]*zi;
+      zi = sUcellf[8]*zi;
+#else
+      xi = cSim.ucellf[0][0]*xi + cSim.ucellf[0][1]*yi + cSim.ucellf[0][2]*zi;
+      yi = cSim.ucellf[1][1]*yi + cSim.ucellf[1][2]*zi;
+      zi = cSim.ucellf[2][2]*zi;
+#endif
+#endif
+
+      // Calculate bounding box on SM 2.0 and up
+      float bmin = (index < ymax) ? 0.5f * xi :  999999.0f;
+      float bmax = (index < ymax) ? 0.5f * xi : -999999.0f;
+      bmin = min(__SHFL(WARP_MASK, bmin, tgx ^ 1), bmin);
+      bmin = min(__SHFL(WARP_MASK, bmin, tgx ^ 2), bmin);
+      bmin = min(__SHFL(WARP_MASK, bmin, tgx ^ 4), bmin);
+#if (PME_ATOMS_PER_WARP >= 16)
+      bmin = min(__SHFL(WARP_MASK, bmin, tgx ^ 8), bmin);
+#endif
+#if (PME_ATOMS_PER_WARP == 32)
+      bmin = min(__SHFL(WARP_MASK, bmin, tgx ^ 16), bmin);
+#endif
+      bmax = max(__SHFL(WARP_MASK, bmax, tgx ^ 1), bmax);
+      bmax = max(__SHFL(WARP_MASK, bmax, tgx ^ 2), bmax);
+      bmax = max(__SHFL(WARP_MASK, bmax, tgx ^ 4), bmax);
+
+#if (PME_ATOMS_PER_WARP >= 16)
+      bmax = max(__SHFL(WARP_MASK, bmax, tgx ^ 8), bmax);
+#endif
+#if (PME_ATOMS_PER_WARP == 32)
+      bmax = max(__SHFL(WARP_MASK, bmax, tgx ^ 16), bmax);
+#endif
+      if (tgx == NEIGHBOR_CELLS + 6) {
+        shNlRecord.f = bmax + bmin;
+      }
+      if (tgx == NEIGHBOR_CELLS + 7) {
+       shNlRecord.f = bmax - bmin;
+      }
+      bmin = (index < ymax) ? 0.5f * yi :  999999.0f;
+      bmax = (index < ymax) ? 0.5f * yi : -999999.0f;
+      bmin = min(__SHFL(WARP_MASK, bmin, tgx ^ 1), bmin);
+      bmin = min(__SHFL(WARP_MASK, bmin, tgx ^ 2), bmin);
+      bmin = min(__SHFL(WARP_MASK, bmin, tgx ^ 4), bmin);
+#if (PME_ATOMS_PER_WARP >= 16)
+      bmin = min(__SHFL(WARP_MASK, bmin, tgx ^ 8), bmin);
+#endif
+#if (PME_ATOMS_PER_WARP == 32)
+      bmin = min(__SHFL(WARP_MASK, bmin, tgx ^ 16), bmin);
+#endif
+      bmax = max(__SHFL(WARP_MASK, bmax, tgx ^ 1), bmax);
+      bmax = max(__SHFL(WARP_MASK, bmax, tgx ^ 2), bmax);
+      bmax = max(__SHFL(WARP_MASK, bmax, tgx ^ 4), bmax);
+#if (PME_ATOMS_PER_WARP >= 16)
+      bmax = max(__SHFL(WARP_MASK, bmax, tgx ^ 8), bmax);
+#endif
+#if (PME_ATOMS_PER_WARP == 32)
+      bmax = max(__SHFL(WARP_MASK, bmax, tgx ^ 16), bmax);
+#endif
+      if (tgx == NEIGHBOR_CELLS + 8) {
+        shNlRecord.f = bmax + bmin;
+      }
+      if (tgx == NEIGHBOR_CELLS + 9) {
+        shNlRecord.f = bmax - bmin;
+      }
+      bmin = (index < ymax) ? 0.5f * zi :  999999.0f;
+      bmax = (index < ymax) ? 0.5f * zi : -999999.0f;
+      bmin = min(__SHFL(WARP_MASK, bmin, tgx ^ 1), bmin);
+      bmin = min(__SHFL(WARP_MASK, bmin, tgx ^ 2), bmin);
+      bmin = min(__SHFL(WARP_MASK, bmin, tgx ^ 4), bmin);
+#if (PME_ATOMS_PER_WARP >= 16)
+      bmin = min(__SHFL(WARP_MASK, bmin, tgx ^ 8), bmin);
+#endif
+#if (PME_ATOMS_PER_WARP == 32)
+      bmin = min(__SHFL(WARP_MASK, bmin, tgx ^ 16), bmin);
+#endif
+      bmax = max(__SHFL(WARP_MASK, bmax, tgx ^ 1), bmax);
+      bmax = max(__SHFL(WARP_MASK, bmax, tgx ^ 2), bmax);
+      bmax = max(__SHFL(WARP_MASK, bmax, tgx ^ 4), bmax);
+#if (PME_ATOMS_PER_WARP >= 16)
+      bmax = max(__SHFL(WARP_MASK, bmax, tgx ^ 8), bmax);
+#endif
+#if (PME_ATOMS_PER_WARP == 32)
+      bmax = max(__SHFL(WARP_MASK, bmax, tgx ^ 16), bmax);
+#endif
+      // Write bbox to shared memory instead of using register shuffles
+      unsigned int warpId = threadIdx.x / GRID;
+      unsigned int bboxBase = warpId * 6;
+
+      if (tgx == 0) {
+        sBBox[bboxBase + 0] = bmax + bmin;  // bxc * 2
+        sBBox[bboxBase + 1] = bmax - bmin;  // bxr * 2
+      }
+      if (tgx == 1) {
+        sBBox[bboxBase + 2] = bmax + bmin;  // byc * 2
+        sBBox[bboxBase + 3] = bmax - bmin;  // byr * 2
+      }
+      if (tgx == 2) {
+        sBBox[bboxBase + 4] = bmax + bmin;  // bzc * 2
+        sBBox[bboxBase + 5] = bmax - bmin;  // bzr * 2
+      }
+      __SYNCWARP(WARP_MASK);
+
+      // Read exclusions into L1 or shared memory
+      if (tgx < ymax - ypos) {
+        uint atom = cSim.pImageAtom[index];
+        uint2 exclusionStartCount = cSim.pNLExclusionStartCount[atom];
+        psWarp->atomList[tgx] = exclusionStartCount.x;
+        psWarp->exclusionMask[tgx] = exclusionStartCount.y;
+      }
+      else {
+        psWarp->atomList[tgx] = 0;
+        psWarp->exclusionMask[tgx] = 0;
+      }
+      __SYNCWARP(WARP_MASK);
+
+      uint totalExclusions = 0;
+      unsigned int minExclusion = cSim.atoms;
+      unsigned int maxExclusion = 0;
+      uint limit = ymax - ypos;
+      for (int i = 0; i < limit; i++) {
+        uint start = psWarp->atomList[i];
+        uint count = psWarp->exclusionMask[i];
+        for (int j = tgx; j < count; j += GRID) {
+          int atom = cSim.pNLExclusionList[start + j];
+          int imageAtom = cSim.pImageAtomLookup[atom];
+          minExclusion = min(minExclusion, imageAtom);
+          maxExclusion = max(maxExclusion, imageAtom);
+          psExclusion[totalExclusions + j] = (imageAtom << NLEXCLUSION_SHIFT) | i;
+        }
+        totalExclusions += count;
+      }
+      minExclusion = min(__SHFL(WARP_MASK, minExclusion, tgx ^ 1), minExclusion);
+      minExclusion = min(__SHFL(WARP_MASK, minExclusion, tgx ^ 2), minExclusion);
+      minExclusion = min(__SHFL(WARP_MASK, minExclusion, tgx ^ 4), minExclusion);
+      minExclusion = min(__SHFL(WARP_MASK, minExclusion, tgx ^ 8), minExclusion);
+      minExclusion = min(__SHFL(WARP_MASK, minExclusion, tgx ^ 16), minExclusion);
+      maxExclusion = max(__SHFL(WARP_MASK, maxExclusion, tgx ^ 1), maxExclusion);
+      maxExclusion = max(__SHFL(WARP_MASK, maxExclusion, tgx ^ 2), maxExclusion);
+      maxExclusion = max(__SHFL(WARP_MASK, maxExclusion, tgx ^ 4), maxExclusion);
+      maxExclusion = max(__SHFL(WARP_MASK, maxExclusion, tgx ^ 8), maxExclusion);
+      maxExclusion = max(__SHFL(WARP_MASK, maxExclusion, tgx ^ 16), maxExclusion);
+#ifdef AMBER_PLATFORM_AMD_WARP64
+      minExclusion = min(__SHFL(WARP_MASK, minExclusion, tgx ^ 32), minExclusion);
+      maxExclusion = max(__SHFL(WARP_MASK, maxExclusion, tgx ^ 32), maxExclusion);
+#endif
+      if (tgx == NEIGHBOR_CELLS + 12) {
+        shNlRecord.u = minExclusion;
+      }
+      if (tgx == NEIGHBOR_CELLS + 13) {
+        shNlRecord.u = maxExclusion;
+      }
+
+      // Initialize Neighbor List variables for current line of entry
+      __SYNCWARP(0xFFFFFFFF);
+      psWarp->exclusionMask[tgx] = 0;
+      __SYNCWARP(WARP_MASK);
+      unsigned int cpos = 0;
+      unsigned int atoms = 0;
+      uint minAtom = cSim.atoms;
+      uint maxAtom = 0;
+      PMEMask threadmask = (PMEMask)1 << tgx;
+
+      while (cpos < cells) {
+
+        // Check for home cell
+        shCellID = __SHFL(WARP_MASK, shNlRecord.u, cpos) & NLRECORD_CELL_TYPE_MASK;
+        uint xpos;
+
+        // Cell 0 always starts along force matrix diagonal
+        if ((cpos == 0) && (psWarp->nlEntry.NL.ymax & NLENTRY_HOME_CELL_MASK)) {
+          // Calculate exclusions assuming all atoms are in range of each other
+          psWarp->exclusionMask[tgx] = 0;
+          __SYNCWARP(WARP_MASK);
+          for (int i = tgx; i < totalExclusions; i += GRID) {
+            uint atom = psExclusion[i] >> NLEXCLUSION_SHIFT;
+            if ((atom >= ypos) && (atom < ymax)) {
+              unsigned int pos = atom - ypos;
+              atomicOr((PMEMask*)&(psWarp->exclusionMask[pos]),
+                       ((PMEMask)1 << (psExclusion[i] & NLEXCLUSION_ATOM_MASK)));
+            }
+          }
+          __SYNCWARP(WARP_MASK);
+
+          // Output exclusion masks
+          if (tgx < cSim.NLAtomsPerWarp) {
+            PMEMask mask = psWarp->exclusionMask[tgx];
+            mask = ((mask >> (1 + tgx)) | (mask << (cSim.NLAtomsPerWarp - tgx - 1))) &
+                   cSim.NLAtomsPerWarpMask;
+            cSim.pNLAtomList[psWarp->offset + tgx] = mask;
+          }
+          __SYNCWARP(WARP_MASK);
+          if (tgx == 0)
+            psWarp->offset += cSim.NLAtomsPerWarp;
+          __SYNCWARP(WARP_MASK);
+          xpos = ypos + cSim.NLAtomsPerWarp;
+        }
+        else {
+          xpos = __SHFL(WARP_MASK, shCell.x, cpos);
+        }
+
+        // Read x atoms
+        unsigned int CELLY_cpos = __SHFL(WARP_MASK, shCell.y, cpos);
+        while (xpos < CELLY_cpos) {
+
+          // Calculate number of atoms in this iteration
+          uint xmax = min(xpos + GRID, CELLY_cpos) - xpos;
+          float sAtomx;
+          float sAtomy;
+          float sAtomz;
+          int sAtomx_i32 = 0;
+          int sAtomy_i32 = 0;
+          int sAtomz_i32 = 0;
+          int sAtomValid = 0;
+
+          // Read up to GRID atoms
+          if (tgx < xmax) {
+            int4 coord = cSim.pAtomCoord_M32[xpos + tgx];
+            sAtomx_i32 = coord.x;
+            sAtomy_i32 = coord.y;
+            sAtomz_i32 = coord.z;
+            sAtomx = (PMEFloat)sAtomx_i32 * cSim.mint32_inv_scale.x;
+            sAtomy = (PMEFloat)sAtomy_i32 * cSim.mint32_inv_scale.y;
+            sAtomz = (PMEFloat)sAtomz_i32 * cSim.mint32_inv_scale.z;
+            sAtomValid = 1;
+          }
+          else {
+            sAtomx = (float)-10000.0 * tgx;
+            sAtomy = (float)-10000.0 * tgx;
+            sAtomz = (float)-10000.0 * tgx;
+            sAtomx_i32 = 0;
+            sAtomy_i32 = 0;
+            sAtomz_i32 = 0;
+            sAtomValid = 0;
+          }
+
+          // Translate all atoms into a local coordinate system within one unit
+          // cell of the first atom read to avoid PBC handling within inner loops
+          unsigned int cellID = shCellID;
+#if defined(PME_VIRIAL) && defined(PME_IS_ORTHOGONAL)
+          sAtomx += sCellOffset[cellID].x;
+          sAtomy += sCellOffset[cellID].y;
+          sAtomz += sCellOffset[cellID].z;
+#else
+          sAtomx += cSim.cellOffset[cellID][0];
+          sAtomy += cSim.cellOffset[cellID][1];
+          sAtomz += cSim.cellOffset[cellID][2];
+#endif
+#ifndef PME_IS_ORTHOGONAL
+#  ifdef PME_VIRIAL
+          sAtomx = sUcellf[0]*sAtomx + sUcellf[1]*sAtomy + sUcellf[2]*sAtomz;
+          sAtomy = sUcellf[4]*sAtomy + sUcellf[5]*sAtomz;
+          sAtomz = sUcellf[8]*sAtomz;
+#  else
+          sAtomx = cSim.ucellf[0][0]*sAtomx + cSim.ucellf[0][1]*sAtomy +
+                   cSim.ucellf[0][2]*sAtomz;
+          sAtomy = cSim.ucellf[1][1]*sAtomy + cSim.ucellf[1][2]*sAtomz;
+          sAtomz = cSim.ucellf[2][2]*sAtomz;
+#  endif
+#endif
+          // Bounding box test on SM 2.0+
+          float trivialCut2 = 0.5625f * cutPlusSkin2;
+          // Read bbox from shared memory
+          unsigned int warpId = threadIdx.x / GRID;
+          unsigned int bboxBase = warpId * 6;
+          float bxc = sBBox[bboxBase + 0];
+          float bxr = sBBox[bboxBase + 1];
+          float byc = sBBox[bboxBase + 2];
+          float byr = sBBox[bboxBase + 3];
+          float bzc = sBBox[bboxBase + 4];
+          float bzr = sBBox[bboxBase + 5];
+          float tx = fabs(sAtomx - bxc);
+          float ty = fabs(sAtomy - byc);
+          float tz = fabs(sAtomz - bzc);
+          tx = tx - min(tx, bxr);
+          ty = ty - min(ty, byr);
+          tz = tz - min(tz, bzr);
+          float tr2 = tx*tx + ty*ty + tz*tz;
+
+          PMEMask bpred = __BALLOT(WARP_MASK, (tgx < xmax) && (tr2 < cutPlusSkin2));
+          PMEMask apred = __BALLOT(WARP_MASK, (tgx < xmax) && (tr2 < trivialCut2));
+
+          // Bitwise exclusive OR plus assignment compound operator ^=
+          // This is bpred = bpred ^ apred
+          bpred ^= apred;
+          // Perform tests on all non-trivial accepts in groups of (GRID / PME_ATOMS_PER_WARP)
+          PMEMask mask = (PMEMask)cSim.NLAtomsPerWarpMask <<
+                         (PME_ATOMS_PER_WARP * (tgx >> cSim.NLAtomsPerWarpBits));
+          while (bpred) {
+            PMEMask bpred_prev = bpred;
+            int pos = maskFfs(bpred) - 1;
+            const int bpredBitWidth = (int)(sizeof(PMEMask) * 8);
+            PMEMask bpred_after_clear = bpred_prev;
+            if (pos >= 0 && pos < bpredBitWidth) {
+              bpred_after_clear = bpred_prev & ~((PMEMask)1 << pos);
+            }
+            if (pos < 0 || pos >= bpredBitWidth) {
+              break;
+            }
+            bpred = bpred_after_clear;
+            if (bpred == bpred_prev) {
+              break;
+            }
+            // HIP-TODO: Support PME_ATOMS_PER_WARP = 32 and 8
+#if (PME_ATOMS_PER_WARP == 16)
+            int pos1 = maskFfs(bpred) - 1;
+            if (pos1 != -1) {
+              bpred &= ~(PMEMask)1 << pos1;
+            }
+            if (tgx >= 16) {
+              pos = pos1;
+            }
+#ifdef AMBER_PLATFORM_AMD_WARP64
+            pos1 = maskFfs(bpred) - 1;
+            if (pos1 != -1) {
+              bpred &= ~(PMEMask)1 << pos1;
+            }
+            if (tgx >= 32) {
+              pos = pos1;
+            }
+            pos1 = maskFfs(bpred) - 1;
+            if (pos1 != -1) {
+              bpred &= ~(PMEMask)1 << pos1;
+            }
+            if (tgx >= 48) {
+              pos = pos1;
+            }
+#endif
+#elif (PME_ATOMS_PER_WARP == 8)
+            int pos1 = maskFfs(bpred) - 1;
+            if (pos1 != -1) {
+              bpred &= ~(PMEMask)1 << pos1;
+            }
+            if (tgx >= 8) {
+              pos = pos1;
+            }
+            pos1 = maskFfs(bpred) - 1;
+            if (pos1 != -1) {
+              bpred &= ~(PMEMask)1 << pos1;
+            }
+            if (tgx >= 16) {
+              pos = pos1;
+            }
+            pos1 = maskFfs(bpred) - 1;
+            if (pos1 != -1) {
+              bpred &= ~(PMEMask)1 << pos1;
+            }
+            if (tgx >= 24) {
+              pos = pos1;
+            }
+#endif
+            int ax_i32 = __SHFL(WARP_MASK, sAtomx_i32, pos);
+            int ay_i32 = __SHFL(WARP_MASK, sAtomy_i32, pos);
+            int az_i32 = __SHFL(WARP_MASK, sAtomz_i32, pos);
+            int pred = 0;
+            int validX_val = 0;
+            float r2_val = 0.0f;
+            if (pos >= 0) {
+              int validX = __SHFL(WARP_MASK, sAtomValid, pos);
+              validX_val = validX;
+              if (validX && validY) {
+                float dx_frac = (float)(xi_i32 - ax_i32) * cSim.mint32_inv_scale.x;
+                float dy_frac = (float)(yi_i32 - ay_i32) * cSim.mint32_inv_scale.y;
+                float dz_frac = (float)(zi_i32 - az_i32) * cSim.mint32_inv_scale.z;
+                float dx = dx_frac;
+                float dy = dy_frac;
+                float dz = dz_frac;
+                float r2 = dx * dx + dy * dy + dz * dz;
+                r2_val = r2;
+                pred = (r2 < cutPlusSkin2);
+              }
+            }
+
+            // Signal acceptance or rejection of atoms
+            if (__BALLOT(WARP_MASK, pred) & mask) {
+              apred |= (PMEMask)1 << pos;
+            }
+          }
+          __SYNCWARP(WARP_MASK);
+          // HIP-TODO: Support PME_ATOMS_PER_WARP = 32 and 8
+#if (PME_ATOMS_PER_WARP < 32)
+          psWarp->exclusionMask[tgx] = apred;
+          __SYNCWARP(WARP_MASK);
+#if (PME_ATOMS_PER_WARP == 8)
+          psWarp->exclusionMask[tgx] |= psWarp->exclusionMask[tgx ^ 8];
+          __SYNCWARP(WARP_MASK);
+          psWarp->exclusionMask[tgx] |= psWarp->exclusionMask[tgx ^ 16];
+          __SYNCWARP(WARP_MASK);
+          apred = psWarp->exclusionMask[tgx];
+#else
+#ifdef AMBER_PLATFORM_AMD_WARP64
+          psWarp->exclusionMask[tgx] |= psWarp->exclusionMask[tgx ^ 16];
+          __SYNCWARP(WARP_MASK);
+          psWarp->exclusionMask[tgx] |= psWarp->exclusionMask[tgx ^ 32];
+          __SYNCWARP(WARP_MASK);
+          apred = psWarp->exclusionMask[tgx];
+#else
+          apred |= psWarp->exclusionMask[tgx ^ 16];
+#endif
+#endif
+#endif
+          bpred = apred;
+
+          // Add all accepted atoms to atom list
+          while (bpred) {
+            int maxAccepts = min(GRID - atoms, maskPopc(bpred));
+            int pos = -1;
+
+            // Find number of predecessor bits and determine if thread can add atom
+            if (bpred & threadmask) {
+              PMEMask mask = threadmask - 1;
+              pos = maskPopc(bpred & mask);
+              if (pos >= maxAccepts) {
+                pos = -1;
+              }
+            }
+
+            // Accept each atom if there's room
+            if (pos != -1) {
+              unsigned int atom = xpos + tgx;
+              minAtom = min(minAtom, atom);
+              maxAtom = max(maxAtom, atom);
+              psWarp->atomList[atoms + pos] = (atom << NLATOM_CELL_SHIFT) | shCellID;
+
+              // Bitwise exclusive OR plus assignment compound operator
+              bpred ^= threadmask;
+            }
+            __SYNCWARP(WARP_MASK);
+            atoms += maxAccepts;
+
+            // Output GRID atoms if ready
+            if (atoms == GRID) {
+
+              // Write swath of atoms to global memory
+              cSim.pNLAtomList[psWarp->offset + tgx] = psWarp->atomList[tgx];
+              __SYNCWARP(WARP_MASK);
+              if (tgx == 0) {
+                psWarp->offset += GRID;
+                psWarp->nlEntry.NL.xatoms += GRID;
+              }
+              __SYNCWARP(WARP_MASK);
+
+              // Clear used bits from bpred
+              bpred &= __SHFL(WARP_MASK, bpred, tgx ^ 1);
+              bpred &= __SHFL(WARP_MASK, bpred, tgx ^ 2);
+              bpred &= __SHFL(WARP_MASK, bpred, tgx ^ 4);
+              bpred &= __SHFL(WARP_MASK, bpred, tgx ^ 8);
+              bpred &= __SHFL(WARP_MASK, bpred, tgx ^ 16);
+#ifdef AMBER_PLATFORM_AMD_WARP64
+              bpred &= __SHFL(WARP_MASK, bpred, tgx ^ 32);
+#endif
+
+              // Reduce minatom and maxatom
+              minAtom = min(minAtom, __SHFL(WARP_MASK, minAtom, tgx ^ 1));
+              minAtom = min(minAtom, __SHFL(WARP_MASK, minAtom, tgx ^ 2));
+              minAtom = min(minAtom, __SHFL(WARP_MASK, minAtom, tgx ^ 4));
+              minAtom = min(minAtom, __SHFL(WARP_MASK, minAtom, tgx ^ 8));
+              minAtom = min(minAtom, __SHFL(WARP_MASK, minAtom, tgx ^ 16));
+              maxAtom = max(maxAtom, __SHFL(WARP_MASK, maxAtom, tgx ^ 1));
+              maxAtom = max(maxAtom, __SHFL(WARP_MASK, maxAtom, tgx ^ 2));
+              maxAtom = max(maxAtom, __SHFL(WARP_MASK, maxAtom, tgx ^ 4));
+              maxAtom = max(maxAtom, __SHFL(WARP_MASK, maxAtom, tgx ^ 8));
+              maxAtom = max(maxAtom, __SHFL(WARP_MASK, maxAtom, tgx ^ 16));
+#ifdef AMBER_PLATFORM_AMD_WARP64
+              minAtom = min(minAtom, __SHFL(WARP_MASK, minAtom, tgx ^ 32));
+              maxAtom = max(maxAtom, __SHFL(WARP_MASK, maxAtom, tgx ^ 32));
+#endif
+
+              // Search for y atom exclusions matching any x atom all at once (this should
+              // reduce exclusion tests by a factor of approximately 100 overall).
+              // But first, rule out skipping exclusion test.
+              uint minExclusion = __SHFL(WARP_MASK, shNlRecord.u, NEIGHBOR_CELLS + 12);
+              uint maxExclusion = __SHFL(WARP_MASK, shNlRecord.u, NEIGHBOR_CELLS + 13);
+
+              psWarp->exclusionMask[tgx] = 0;
+              __SYNCWARP(WARP_MASK);
+              if ((minAtom <= maxExclusion) && (maxAtom >= minExclusion)) {
+                uint atom = (psWarp->atomList[tgx] >> NLATOM_CELL_SHIFT);
+                for (int j = 0; j < totalExclusions; j += GRID) {
+                  int offset = j + tgx;
+                  unsigned int eatom = 0xffffffff;
+                  if (offset < totalExclusions) {
+                    eatom = psExclusion[offset] >> NLEXCLUSION_SHIFT;
+                  }
+                  PMEMask vote = __BALLOT(WARP_MASK, (eatom >= minAtom) && (eatom <= maxAtom));
+
+                  while (vote) {
+                    unsigned int k = maskFfs(vote) - 1;
+                    offset = k + j;
+                    eatom = psExclusion[offset] >> NLEXCLUSION_SHIFT;
+                    if (atom == eatom) {
+                      psWarp->exclusionMask[psExclusion[offset] & NLEXCLUSION_ATOM_MASK] |=
+                        threadmask;
+                    }
+                    __SYNCWARP(WARP_MASK);
+                    vote ^= (PMEMask)1 << k;
+                  }
+                }
+              }
+              __SYNCWARP(WARP_MASK);
+
+              // Output exclusion masks
+              if (tgx < cSim.NLAtomsPerWarp) {
+                PMEMask emask = psWarp->exclusionMask[tgx];
+                ((PMEMask*)&cSim.pNLAtomList[psWarp->offset])[tgx] =
+                  ((emask >> tgx) & exclusionMask) |
+                  ((emask << (cSim.NLAtomsPerWarp - tgx)) & ~exclusionMask);
+              }
+                    __SYNCWARP(WARP_MASK);
+              if (tgx == 0)
+                psWarp->offset += cSim.NLAtomsPerWarp * sizeof(PMEMask) / sizeof(unsigned int);
+              atoms = 0;
+              psWarp->atomList[tgx] = 0;
+              minAtom = cSim.atoms;
+              maxAtom = 0;
+              __SYNCWARP(WARP_MASK);
+
+              // Output neighbor list entry if width is sufficient
+              if (psWarp->nlEntry.NL.xatoms >= cSim.NLXEntryWidth) {
+                uint nlpos;
+                if (tgx == 0) {
+                  nlpos = atomicAdd(cSim.pNLEntries, 1);
+                }
+                nlpos = __SHFL(WARP_MASK, nlpos, 0);
+                if (tgx < 4) {
+                  cSim.pNLEntry[nlpos].array[tgx] = psWarp->nlEntry.array[tgx];
+                }
+                __SYNCWARP(WARP_MASK);
+                if (tgx == 0) {
+                  psWarp->nlEntry.NL.xatoms  = 0;
+                  psWarp->nlEntry.NL.ymax   &= ~NLENTRY_HOME_CELL_MASK;
+                  psWarp->nlEntry.NL.offset  = psWarp->offset;
+                }
+              }
+            }
+              else {
+                bpred = 0;
+              }
+              // End contingency for atoms == GRID
+            }
+            // End loop for adding atoms to the non-bonded list while bpred != 0
+
+          // Move to next swath of atoms
+          xpos += GRID;
+        }
+
+        // Move to next cell
+        cpos++;
+      }
+
+      // Output last batch of atoms for this swath
+      if (atoms > 0) {
+        // Reduce minatom and maxatom
+        minAtom = min(minAtom, __SHFL(WARP_MASK, minAtom, tgx ^ 1));
+        minAtom = min(minAtom, __SHFL(WARP_MASK, minAtom, tgx ^ 2));
+        minAtom = min(minAtom, __SHFL(WARP_MASK, minAtom, tgx ^ 4));
+        minAtom = min(minAtom, __SHFL(WARP_MASK, minAtom, tgx ^ 8));
+        minAtom = min(minAtom, __SHFL(WARP_MASK, minAtom, tgx ^ 16));
+        maxAtom = max(maxAtom, __SHFL(WARP_MASK, maxAtom, tgx ^ 1));
+        maxAtom = max(maxAtom, __SHFL(WARP_MASK, maxAtom, tgx ^ 2));
+        maxAtom = max(maxAtom, __SHFL(WARP_MASK, maxAtom, tgx ^ 4));
+        maxAtom = max(maxAtom, __SHFL(WARP_MASK, maxAtom, tgx ^ 8));
+        maxAtom = max(maxAtom, __SHFL(WARP_MASK, maxAtom, tgx ^ 16));
+#ifdef AMBER_PLATFORM_AMD_WARP64
+        minAtom = min(minAtom, __SHFL(WARP_MASK, minAtom, tgx ^ 32));
+        maxAtom = max(maxAtom, __SHFL(WARP_MASK, maxAtom, tgx ^ 32));
+#endif
+        if (tgx < atoms) {
+          cSim.pNLAtomList[psWarp->offset + tgx] = psWarp->atomList[tgx];
+        }
+        else {
+          cSim.pNLAtomList[psWarp->offset + tgx] = 0;
+        }
+        __SYNCWARP(WARP_MASK);
+        if (tgx == 0)
+          psWarp->offset += GRID;
+
+        // Search for y atom exclusions matching any x atom all at once (this should
+        // reduce exclusion tests by a factor of approximately 100 overall)
+        uint minExclusion = __SHFL(WARP_MASK, shNlRecord.u, NEIGHBOR_CELLS + 12);
+        uint maxExclusion = __SHFL(WARP_MASK, shNlRecord.u, NEIGHBOR_CELLS + 13);
+        psWarp->exclusionMask[tgx] = 0;
+        __SYNCWARP(WARP_MASK);
+        if ((tgx < atoms) && ((minAtom <= maxExclusion) && (maxAtom >= minExclusion))) {
+          unsigned int atom = (psWarp->atomList[tgx] >> NLATOM_CELL_SHIFT);
+          for (int j = 0; j < totalExclusions; j++) {
+            if ((psExclusion[j] >> NLEXCLUSION_SHIFT) == atom) {
+              atomicOr((PMEMask*)(&psWarp->exclusionMask[psExclusion[j] &
+                                  NLEXCLUSION_ATOM_MASK]), threadmask);
+            }
+          }
+        }
+        __SYNCWARP(WARP_MASK);
+
+        // Output exclusion masks
+        if (tgx < cSim.NLAtomsPerWarp) {
+          PMEMask emask = psWarp->exclusionMask[tgx];
+          ((PMEMask*)&cSim.pNLAtomList[psWarp->offset])[tgx] =
+            ((emask >> tgx) & exclusionMask) |
+            ((emask << (cSim.NLAtomsPerWarp - tgx)) & ~exclusionMask);
+        }
+              __SYNCWARP(WARP_MASK);
+        if (tgx == 0) {
+          psWarp->nlEntry.NL.xatoms += atoms;
+          psWarp->offset += cSim.NLAtomsPerWarp * sizeof(PMEMask) / sizeof(unsigned int);
+        }
+        atoms  = 0;
+        psWarp->atomList[tgx] = 0;
+      }
+      __SYNCWARP(WARP_MASK);
+      // End contingency for committing the final batch of atoms
+
+      // Output final neighbor list entry if xatoms > 0
+      if ((psWarp->nlEntry.NL.xatoms > 0) || (psWarp->nlEntry.NL.ymax & NLENTRY_HOME_CELL_MASK)) {
+        uint nlpos;
+        if (tgx == 0) {
+          nlpos = atomicAdd(cSim.pNLEntries, 1);
+        }
+        nlpos = __SHFL(WARP_MASK, nlpos, 0);
+        if (tgx < 4) {
+          cSim.pNLEntry[nlpos].array[tgx] = psWarp->nlEntry.array[tgx];
+        }
+      }
+      __SYNCWARP(WARP_MASK);
+
+      // Advance to next swath of atoms
+      ypos += cSim.NLYDivisor * cSim.NLAtomsPerWarp;
+    }
+
+    // Advance to next NLRecord entry
+    if (tgx == (NEIGHBOR_CELLS + 2)) {
+      shNlRecord.u = atomicAdd(&cSim.pFrcBlkCounters[0], 1);
+    }
+  }
+#undef VOLATILE
+}
diff --git "a/C:\\amber\\amber_mint\\src\\pmemd\\src/cuda/kBNL.h.master_backup" "b/C:\\amber\\amber_mint\\src\\pmemd\\src/cuda/kBNL.h.master_backup"
new file mode 100644
index 0000000..7857ba1
--- /dev/null
+++ "b/C:\\amber\\amber_mint\\src\\pmemd\\src/cuda/kBNL.h.master_backup"
@@ -0,0 +1,786 @@
+#include "copyright.i"
+
+//---------------------------------------------------------------------------------------------
+// AMBER NVIDIA CUDA GPU IMPLEMENTATION: PMEMD VERSION
+//
+// July 2017, by Scott Le Grand, David S. Cerutti, Daniel J. Mermelstein, Charles Lin, and
+//               Ross C. Walker
+//---------------------------------------------------------------------------------------------
+
+//---------------------------------------------------------------------------------------------
+// This file is included by kNeighborList.cu multiple times with different pre-processor
+// definitions (PME_VIRIAL, PME_IS_ORTHOGONAL, PME_ATOMS_PER_WARP) to generate multiple
+// implementations of kNLBuildNeighborList_????_kernel().
+//---------------------------------------------------------------------------------------------
+{
+#if !defined(AMBER_PLATFORM_AMD)
+#  define VOLATILE volatile
+#else
+#  define VOLATILE
+#endif
+  struct BNLAtom {
+    float x;     //
+    float y;     // Coordinates of the atom
+    float z;     //
+  };
+
+  union uintfloat {
+    uint u;      // Makes it possible to read a floating point number
+    float f;     //   as an unsigned integer
+  };
+
+  struct BNLWarp {
+    uint offset;
+    uint atomList[GRID];
+    PMEMask exclusionMask[GRID];
+    NLEntry nlEntry;
+  };
+
+  uint2 shCell;
+  uint shCellID;
+  VOLATILE uintfloat shNlRecord;
+#if (PME_ATOMS_PER_WARP == 32)
+  const int THREADS_PER_BLOCK = NLBUILD_NEIGHBORLIST32_THREADS_PER_BLOCK;
+#elif (PME_ATOMS_PER_WARP == 16)
+  const int THREADS_PER_BLOCK = NLBUILD_NEIGHBORLIST16_THREADS_PER_BLOCK;
+#else
+  const int THREADS_PER_BLOCK = NLBUILD_NEIGHBORLIST8_THREADS_PER_BLOCK;
+#endif
+  __shared__ VOLATILE BNLWarp sNLWarp[THREADS_PER_BLOCK / GRID];
+#ifdef PME_VIRIAL
+  __shared__ float sCutPlusSkin2;
+  __shared__ float sUcellf[9];
+#  ifdef PME_IS_ORTHOGONAL
+  __shared__ float3 sCellOffset[NEIGHBOR_CELLS];
+#  endif
+#endif
+#ifdef PME_VIRIAL
+  if (threadIdx.x < 9) {
+    sUcellf[threadIdx.x] = cSim.pNTPData->ucellf[threadIdx.x];
+  }
+  if (threadIdx.x == 32) {
+    sCutPlusSkin2 = cSim.pNTPData->cutPlusSkin2;
+  }
+#ifdef PME_IS_ORTHOGONAL
+  __syncthreads();
+  if (threadIdx.x < NEIGHBOR_CELLS) {
+    sCellOffset[threadIdx.x].x = sUcellf[0] * cSim.cellOffset[threadIdx.x][0];
+    sCellOffset[threadIdx.x].y = sUcellf[4] * cSim.cellOffset[threadIdx.x][1];
+    sCellOffset[threadIdx.x].z = sUcellf[8] * cSim.cellOffset[threadIdx.x][2];
+  }
+#endif
+  __syncthreads();
+  float cutPlusSkin2 = sCutPlusSkin2;
+#else
+  float cutPlusSkin2 = cSim.cutPlusSkin2;
+#endif
+  unsigned int warp = threadIdx.x >> GRID_BITS;
+  VOLATILE BNLWarp* psWarp = &sNLWarp[warp];
+  unsigned int globalWarp = warp + ((blockIdx.x * blockDim.x) >> GRID_BITS);
+  unsigned int* psExclusion = &cSim.pBNLExclusionBuffer[globalWarp * cSim.NLExclusionBufferSize];
+  if ((threadIdx.x & GRID_BITS_MASK) == (NEIGHBOR_CELLS + 2)) {
+    shNlRecord.u = (blockIdx.x * blockDim.x + threadIdx.x) >> GRID_BITS;
+  }
+  PMEMask exclusionMask =
+    cSim.NLAtomsPerWarpMask >> (threadIdx.x & cSim.NLAtomsPerWarpBitsMask);
+#if (PME_ATOMS_PER_WARP == 16)
+  exclusionMask = exclusionMask | (exclusionMask << 16);
+#elif (PME_ATOMS_PER_WARP == 8)
+  exclusionMask = exclusionMask | (exclusionMask << 8) |
+                  (exclusionMask << 16) | (exclusionMask << 24);
+#endif
+#ifdef AMBER_PLATFORM_AMD_WARP64
+  exclusionMask = exclusionMask | (exclusionMask << 32);
+#endif
+
+  // This is the beginning of a loop that extends to the end of the library.
+  // The union shNlRecord is used for various pieces of information, both floats
+  // and unsigned integers, and in only one critical case is it ever used to
+  // interpret one as the other.
+  while (__SHFL(WARP_MASK, shNlRecord.u, NEIGHBOR_CELLS + 2) < cSim.NLRecords) {
+    unsigned int tgx = threadIdx.x & GRID_BITS_MASK;
+
+    // Read NLRecord information
+    unsigned int pos1 = __SHFL(WARP_MASK, shNlRecord.u, NEIGHBOR_CELLS + 2);
+    if (tgx < NEIGHBOR_CELLS + 2) {
+      shNlRecord.u = cSim.pNLRecord[pos1].array[tgx];
+    }
+
+    // Calculate Exclusion/neighbor list space required
+    int atomOffset = __SHFL(WARP_MASK, shNlRecord.u, NEIGHBOR_CELLS + 1) >> NLRECORD_YOFFSET_SHIFT;
+    uint2 homeCell = cSim.pNLNonbondCellStartEnd[__SHFL(WARP_MASK, shNlRecord.u, NEIGHBOR_CELLS)];
+
+    if (tgx == (NEIGHBOR_CELLS + 4)) {
+      shNlRecord.u = homeCell.x;
+    }
+    if (tgx == (NEIGHBOR_CELLS + 5)) {
+      shNlRecord.u = homeCell.y;
+    }
+    int ysize = max(0, (int)(__SHFL(WARP_MASK, shNlRecord.u, NEIGHBOR_CELLS + 5) -
+                             __SHFL(WARP_MASK, shNlRecord.u, NEIGHBOR_CELLS + 4) -
+                             atomOffset * cSim.NLAtomsPerWarp));
+    if (ysize > 0) {
+      ysize = 1 + max(0, ysize - 1) / (cSim.NLAtomsPerWarp * cSim.NLYDivisor);
+    }
+
+    // Calculate maximum required space: shNlRecord's unsigned int has been assigned to
+    // cSim.pNLRecord[pos1].array[tgx] for tgx < 16, homeCell.x for tgx == 18, and homeCell.y
+    // for tgx == 19.  The value broadcast for setting cells on all threads is going to be
+    // for tgx == 15, which is the final cell with which .
+    unsigned int cells = __SHFL(WARP_MASK, shNlRecord.u, NEIGHBOR_CELLS + 1) & NLRECORD_CELL_COUNT_MASK;
+    PMEMask mask1 = __BALLOT(WARP_MASK, (tgx < cells) );
+    if (tgx < cells) {
+      uint2 cell = cSim.pNLNonbondCellStartEnd[__SHFL(mask1, shNlRecord.u, tgx) >>
+                                               NLRECORD_CELL_SHIFT];
+      shCell.x = cell.x;
+      shCell.y = cell.y;
+      psWarp->atomList[tgx] = cell.y - cell.x;
+    }
+    else {
+      psWarp->atomList[tgx] = 0;
+    }
+    __SYNCWARP(WARP_MASK);
+    // Reduce xsize down to thread 0 of each warp
+    uint temp;
+    temp = psWarp->atomList[tgx];
+#if (NEIGHBOR_CELLS > 16)
+    temp += __SHFL_DOWN(WARP_MASK, temp, 16);
+#endif
+    for (int offset = 8; offset > 0; offset /= 2)
+      temp += __SHFL_DOWN(WARP_MASK, temp, offset);
+    psWarp->atomList[tgx]=temp;
+    __SYNCWARP(WARP_MASK);
+
+    if (tgx == (NEIGHBOR_CELLS + 2)) {
+      uint totalXSize = ((psWarp->atomList[0] + GRID - 1) >> GRID_BITS);
+      uint offset = atomicAdd(cSim.pNLTotalOffset,
+                              totalXSize*ysize*cSim.NLOffsetPerWarp + cSim.NLAtomsPerWarp);
+      psWarp->offset = offset;
+    }
+    __SYNCWARP(WARP_MASK);
+
+    // Generate actual neighbor list entry
+    uint ypos = __SHFL(WARP_MASK, shNlRecord.u, NEIGHBOR_CELLS + 4) +
+                ((__SHFL(WARP_MASK, shNlRecord.u,
+                         NEIGHBOR_CELLS + 1) >> NLRECORD_YOFFSET_SHIFT) * cSim.NLAtomsPerWarp);
+    uint homecellY = __SHFL(WARP_MASK, shNlRecord.u, NEIGHBOR_CELLS + 5);
+    while ( ypos < homecellY ) {
+
+      // Calculate y bounds and set to calculate homecell interaction
+      uint ymax = min(ypos + cSim.NLAtomsPerWarp, homecellY);
+      if (tgx == 0) {
+        psWarp->nlEntry.NL.ypos = ypos;
+        psWarp->nlEntry.NL.ymax = (ymax << NLENTRY_YMAX_SHIFT) | NLENTRY_HOME_CELL_MASK;
+        psWarp->nlEntry.NL.xatoms = 0;
+        psWarp->nlEntry.NL.offset = psWarp->offset;
+      }
+      __SYNCWARP(WARP_MASK);
+
+      // Read y atoms
+      float xi;
+      float yi;
+      float zi;
+      unsigned int index = ypos + (tgx & (cSim.NLAtomsPerWarpBitsMask));
+      if (index < ymax) {
+        PMEFloat2 xy = cSim.pAtomXYSP[index];
+        zi = cSim.pAtomZSP[index];
+        xi = xy.x;
+        yi = xy.y;
+      }
+      else {
+        xi = (float)10000.0 * index;
+        yi = (float)10000.0 * index;
+        zi = (float)10000.0 * index;
+      }
+
+#ifndef PME_IS_ORTHOGONAL
+      // Transform into cartesian space
+#ifdef PME_VIRIAL
+      xi = sUcellf[0]*xi + sUcellf[1]*yi + sUcellf[2]*zi;
+      yi = sUcellf[4]*yi + sUcellf[5]*zi;
+      zi = sUcellf[8]*zi;
+#else
+      xi = cSim.ucellf[0][0]*xi + cSim.ucellf[0][1]*yi + cSim.ucellf[0][2]*zi;
+      yi = cSim.ucellf[1][1]*yi + cSim.ucellf[1][2]*zi;
+      zi = cSim.ucellf[2][2]*zi;
+#endif
+#endif
+
+      // Calculate bounding box on SM 2.0 and up
+      float bmin = (index < ymax) ? 0.5f * xi :  999999.0f;
+      float bmax = (index < ymax) ? 0.5f * xi : -999999.0f;
+      bmin = min(__SHFL(WARP_MASK, bmin, tgx ^ 1), bmin);
+      bmin = min(__SHFL(WARP_MASK, bmin, tgx ^ 2), bmin);
+      bmin = min(__SHFL(WARP_MASK, bmin, tgx ^ 4), bmin);
+#if (PME_ATOMS_PER_WARP >= 16)
+      bmin = min(__SHFL(WARP_MASK, bmin, tgx ^ 8), bmin);
+#endif
+#if (PME_ATOMS_PER_WARP == 32)
+      bmin = min(__SHFL(WARP_MASK, bmin, tgx ^ 16), bmin);
+#endif
+      bmax = max(__SHFL(WARP_MASK, bmax, tgx ^ 1), bmax);
+      bmax = max(__SHFL(WARP_MASK, bmax, tgx ^ 2), bmax);
+      bmax = max(__SHFL(WARP_MASK, bmax, tgx ^ 4), bmax);
+
+#if (PME_ATOMS_PER_WARP >= 16)
+      bmax = max(__SHFL(WARP_MASK, bmax, tgx ^ 8), bmax);
+#endif
+#if (PME_ATOMS_PER_WARP == 32)
+      bmax = max(__SHFL(WARP_MASK, bmax, tgx ^ 16), bmax);
+#endif
+      if (tgx == NEIGHBOR_CELLS + 6) {
+        shNlRecord.f = bmax + bmin;
+      }
+      if (tgx == NEIGHBOR_CELLS + 7) {
+       shNlRecord.f = bmax - bmin;
+      }
+      bmin = (index < ymax) ? 0.5f * yi :  999999.0f;
+      bmax = (index < ymax) ? 0.5f * yi : -999999.0f;
+      bmin = min(__SHFL(WARP_MASK, bmin, tgx ^ 1), bmin);
+      bmin = min(__SHFL(WARP_MASK, bmin, tgx ^ 2), bmin);
+      bmin = min(__SHFL(WARP_MASK, bmin, tgx ^ 4), bmin);
+#if (PME_ATOMS_PER_WARP >= 16)
+      bmin = min(__SHFL(WARP_MASK, bmin, tgx ^ 8), bmin);
+#endif
+#if (PME_ATOMS_PER_WARP == 32)
+      bmin = min(__SHFL(WARP_MASK, bmin, tgx ^ 16), bmin);
+#endif
+      bmax = max(__SHFL(WARP_MASK, bmax, tgx ^ 1), bmax);
+      bmax = max(__SHFL(WARP_MASK, bmax, tgx ^ 2), bmax);
+      bmax = max(__SHFL(WARP_MASK, bmax, tgx ^ 4), bmax);
+#if (PME_ATOMS_PER_WARP >= 16)
+      bmax = max(__SHFL(WARP_MASK, bmax, tgx ^ 8), bmax);
+#endif
+#if (PME_ATOMS_PER_WARP == 32)
+      bmax = max(__SHFL(WARP_MASK, bmax, tgx ^ 16), bmax);
+#endif
+      if (tgx == NEIGHBOR_CELLS + 8) {
+        shNlRecord.f = bmax + bmin;
+      }
+      if (tgx == NEIGHBOR_CELLS + 9) {
+        shNlRecord.f = bmax - bmin;
+      }
+      bmin = (index < ymax) ? 0.5f * zi :  999999.0f;
+      bmax = (index < ymax) ? 0.5f * zi : -999999.0f;
+      bmin = min(__SHFL(WARP_MASK, bmin, tgx ^ 1), bmin);
+      bmin = min(__SHFL(WARP_MASK, bmin, tgx ^ 2), bmin);
+      bmin = min(__SHFL(WARP_MASK, bmin, tgx ^ 4), bmin);
+#if (PME_ATOMS_PER_WARP >= 16)
+      bmin = min(__SHFL(WARP_MASK, bmin, tgx ^ 8), bmin);
+#endif
+#if (PME_ATOMS_PER_WARP == 32)
+      bmin = min(__SHFL(WARP_MASK, bmin, tgx ^ 16), bmin);
+#endif
+      bmax = max(__SHFL(WARP_MASK, bmax, tgx ^ 1), bmax);
+      bmax = max(__SHFL(WARP_MASK, bmax, tgx ^ 2), bmax);
+      bmax = max(__SHFL(WARP_MASK, bmax, tgx ^ 4), bmax);
+#if (PME_ATOMS_PER_WARP >= 16)
+      bmax = max(__SHFL(WARP_MASK, bmax, tgx ^ 8), bmax);
+#endif
+#if (PME_ATOMS_PER_WARP == 32)
+      bmax = max(__SHFL(WARP_MASK, bmax, tgx ^ 16), bmax);
+#endif
+      if (tgx == NEIGHBOR_CELLS + 10) {
+        shNlRecord.f = bmax + bmin;
+      }
+      if (tgx == NEIGHBOR_CELLS + 11) {
+        shNlRecord.f = bmax - bmin;
+      }
+
+      // Read exclusions into L1 or shared memory
+      if (tgx < ymax - ypos) {
+        uint atom = cSim.pImageAtom[index];
+        uint2 exclusionStartCount = cSim.pNLExclusionStartCount[atom];
+        psWarp->atomList[tgx] = exclusionStartCount.x;
+        psWarp->exclusionMask[tgx] = exclusionStartCount.y;
+      }
+      else {
+        psWarp->atomList[tgx] = 0;
+        psWarp->exclusionMask[tgx] = 0;
+      }
+      __SYNCWARP(WARP_MASK);
+
+      uint totalExclusions = 0;
+      unsigned int minExclusion = cSim.atoms;
+      unsigned int maxExclusion = 0;
+      uint limit = ymax - ypos;
+      for (int i = 0; i < limit; i++) {
+        uint start = psWarp->atomList[i];
+        uint count = psWarp->exclusionMask[i];
+        for (int j = tgx; j < count; j += GRID) {
+          int atom = cSim.pNLExclusionList[start + j];
+          int imageAtom = cSim.pImageAtomLookup[atom];
+          minExclusion = min(minExclusion, imageAtom);
+          maxExclusion = max(maxExclusion, imageAtom);
+          psExclusion[totalExclusions + j] = (imageAtom << NLEXCLUSION_SHIFT) | i;
+        }
+        totalExclusions += count;
+      }
+      minExclusion = min(__SHFL(WARP_MASK, minExclusion, tgx ^ 1), minExclusion);
+      minExclusion = min(__SHFL(WARP_MASK, minExclusion, tgx ^ 2), minExclusion);
+      minExclusion = min(__SHFL(WARP_MASK, minExclusion, tgx ^ 4), minExclusion);
+      minExclusion = min(__SHFL(WARP_MASK, minExclusion, tgx ^ 8), minExclusion);
+      minExclusion = min(__SHFL(WARP_MASK, minExclusion, tgx ^ 16), minExclusion);
+      maxExclusion = max(__SHFL(WARP_MASK, maxExclusion, tgx ^ 1), maxExclusion);
+      maxExclusion = max(__SHFL(WARP_MASK, maxExclusion, tgx ^ 2), maxExclusion);
+      maxExclusion = max(__SHFL(WARP_MASK, maxExclusion, tgx ^ 4), maxExclusion);
+      maxExclusion = max(__SHFL(WARP_MASK, maxExclusion, tgx ^ 8), maxExclusion);
+      maxExclusion = max(__SHFL(WARP_MASK, maxExclusion, tgx ^ 16), maxExclusion);
+#ifdef AMBER_PLATFORM_AMD_WARP64
+      minExclusion = min(__SHFL(WARP_MASK, minExclusion, tgx ^ 32), minExclusion);
+      maxExclusion = max(__SHFL(WARP_MASK, maxExclusion, tgx ^ 32), maxExclusion);
+#endif
+      if (tgx == NEIGHBOR_CELLS + 12) {
+        shNlRecord.u = minExclusion;
+      }
+      if (tgx == NEIGHBOR_CELLS + 13) {
+        shNlRecord.u = maxExclusion;
+      }
+
+      // Initialize Neighbor List variables for current line of entry
+      __SYNCWARP(0xFFFFFFFF);
+      psWarp->exclusionMask[tgx] = 0;
+      __SYNCWARP(WARP_MASK);
+      unsigned int cpos = 0;
+      unsigned int atoms = 0;
+      uint minAtom = cSim.atoms;
+      uint maxAtom = 0;
+      PMEMask threadmask = (PMEMask)1 << tgx;
+
+      while (cpos < cells) {
+
+        // Check for home cell
+        shCellID = __SHFL(WARP_MASK, shNlRecord.u, cpos) & NLRECORD_CELL_TYPE_MASK;
+        uint xpos;
+
+        // Cell 0 always starts along force matrix diagonal
+        if ((cpos == 0) && (psWarp->nlEntry.NL.ymax & NLENTRY_HOME_CELL_MASK)) {
+          // Calculate exclusions assuming all atoms are in range of each other
+          psWarp->exclusionMask[tgx] = 0;
+          __SYNCWARP(WARP_MASK);
+          for (int i = tgx; i < totalExclusions; i += GRID) {
+            uint atom = psExclusion[i] >> NLEXCLUSION_SHIFT;
+            if ((atom >= ypos) && (atom < ymax)) {
+              unsigned int pos = atom - ypos;
+              atomicOr((PMEMask*)&(psWarp->exclusionMask[pos]),
+                       ((PMEMask)1 << (psExclusion[i] & NLEXCLUSION_ATOM_MASK)));
+            }
+          }
+          __SYNCWARP(WARP_MASK);
+
+          // Output exclusion masks
+          if (tgx < cSim.NLAtomsPerWarp) {
+            PMEMask mask = psWarp->exclusionMask[tgx];
+            mask = ((mask >> (1 + tgx)) | (mask << (cSim.NLAtomsPerWarp - tgx - 1))) &
+                   cSim.NLAtomsPerWarpMask;
+            cSim.pNLAtomList[psWarp->offset + tgx] = mask;
+          }
+          __SYNCWARP(WARP_MASK);
+          if (tgx == 0)
+            psWarp->offset += cSim.NLAtomsPerWarp;
+          __SYNCWARP(WARP_MASK);
+          xpos = ypos + cSim.NLAtomsPerWarp;
+        }
+        else {
+          xpos = __SHFL(WARP_MASK, shCell.x, cpos);
+        }
+
+        // Read x atoms
+        unsigned int CELLY_cpos = __SHFL(WARP_MASK, shCell.y, cpos);
+        while (xpos < CELLY_cpos) {
+
+          // Calculate number of atoms in this iteration
+          uint xmax = min(xpos + GRID, CELLY_cpos) - xpos;
+          float sAtomx;
+          float sAtomy;
+          float sAtomz;
+
+          // Read up to GRID atoms
+          if (tgx < xmax) {
+            PMEFloat2 xy = cSim.pAtomXYSP[xpos + tgx];
+            sAtomz = cSim.pAtomZSP[xpos + tgx];
+            sAtomx = xy.x;
+            sAtomy = xy.y;
+          }
+          else {
+            sAtomx = (float)-10000.0 * tgx;
+            sAtomy = (float)-10000.0 * tgx;
+            sAtomz = (float)-10000.0 * tgx;
+          }
+
+          // Translate all atoms into a local coordinate system within one unit
+          // cell of the first atom read to avoid PBC handling within inner loops
+          unsigned int cellID = shCellID;
+#if defined(PME_VIRIAL) && defined(PME_IS_ORTHOGONAL)
+          sAtomx += sCellOffset[cellID].x;
+          sAtomy += sCellOffset[cellID].y;
+          sAtomz += sCellOffset[cellID].z;
+#else
+          sAtomx += cSim.cellOffset[cellID][0];
+          sAtomy += cSim.cellOffset[cellID][1];
+          sAtomz += cSim.cellOffset[cellID][2];
+#endif
+#ifndef PME_IS_ORTHOGONAL
+#  ifdef PME_VIRIAL
+          sAtomx = sUcellf[0]*sAtomx + sUcellf[1]*sAtomy + sUcellf[2]*sAtomz;
+          sAtomy = sUcellf[4]*sAtomy + sUcellf[5]*sAtomz;
+          sAtomz = sUcellf[8]*sAtomz;
+#  else
+          sAtomx = cSim.ucellf[0][0]*sAtomx + cSim.ucellf[0][1]*sAtomy +
+                   cSim.ucellf[0][2]*sAtomz;
+          sAtomy = cSim.ucellf[1][1]*sAtomy + cSim.ucellf[1][2]*sAtomz;
+          sAtomz = cSim.ucellf[2][2]*sAtomz;
+#  endif
+#endif
+          // Bounding box test on SM 2.0+
+          float trivialCut2 = 0.5625f * cutPlusSkin2;
+          float bxc = __SHFL(WARP_MASK, shNlRecord.f, NEIGHBOR_CELLS + 6);
+          float bxr = __SHFL(WARP_MASK, shNlRecord.f, NEIGHBOR_CELLS + 7);
+          float byc = __SHFL(WARP_MASK, shNlRecord.f, NEIGHBOR_CELLS + 8);
+          float byr = __SHFL(WARP_MASK, shNlRecord.f, NEIGHBOR_CELLS + 9);
+          float bzc = __SHFL(WARP_MASK, shNlRecord.f, NEIGHBOR_CELLS + 10);
+          float bzr = __SHFL(WARP_MASK, shNlRecord.f, NEIGHBOR_CELLS + 11);
+          float tx = fabs(sAtomx - bxc);
+          float ty = fabs(sAtomy - byc);
+          float tz = fabs(sAtomz - bzc);
+          tx = tx - min(tx, bxr);
+          ty = ty - min(ty, byr);
+          tz = tz - min(tz, bzr);
+          float tr2 = tx*tx + ty*ty + tz*tz;
+          PMEMask bpred = __BALLOT(WARP_MASK, (tgx < xmax) && (tr2 < cutPlusSkin2));
+          PMEMask apred = __BALLOT(WARP_MASK, (tgx < xmax) && (tr2 < trivialCut2));
+
+          // Bitwise exclusive OR plus assignment compound operator ^=
+          // This is bpred = bpred ^ apred
+          bpred ^= apred;
+
+          // Perform tests on all non-trivial accepts in groups of (GRID / PME_ATOMS_PER_WARP)
+          PMEMask mask = (PMEMask)cSim.NLAtomsPerWarpMask <<
+                         (PME_ATOMS_PER_WARP * (tgx >> cSim.NLAtomsPerWarpBits));
+          while (bpred) {
+            int pos = maskFfs(bpred) - 1;
+            bpred &= ~(PMEMask)1 << pos;
+            // HIP-TODO: Support PME_ATOMS_PER_WARP = 32 and 8
+#if (PME_ATOMS_PER_WARP == 16)
+            int pos1 = maskFfs(bpred) - 1;
+            if (pos1 != -1) {
+              bpred &= ~(PMEMask)1 << pos1;
+            }
+            if (tgx >= 16) {
+              pos = pos1;
+            }
+#ifdef AMBER_PLATFORM_AMD_WARP64
+            pos1 = maskFfs(bpred) - 1;
+            if (pos1 != -1) {
+              bpred &= ~(PMEMask)1 << pos1;
+            }
+            if (tgx >= 32) {
+              pos = pos1;
+            }
+            pos1 = maskFfs(bpred) - 1;
+            if (pos1 != -1) {
+              bpred &= ~(PMEMask)1 << pos1;
+            }
+            if (tgx >= 48) {
+              pos = pos1;
+            }
+#endif
+#elif (PME_ATOMS_PER_WARP == 8)
+            int pos1 = maskFfs(bpred) - 1;
+            if (pos1 != -1) {
+              bpred &= ~(PMEMask)1 << pos1;
+            }
+            if (tgx >= 8) {
+              pos = pos1;
+            }
+            pos1 = maskFfs(bpred) - 1;
+            if (pos1 != -1) {
+              bpred &= ~(PMEMask)1 << pos1;
+            }
+            if (tgx >= 16) {
+              pos = pos1;
+            }
+            pos1 = maskFfs(bpred) - 1;
+            if (pos1 != -1) {
+              bpred &= ~(PMEMask)1 << pos1;
+            }
+            if (tgx >= 24) {
+              pos = pos1;
+            }
+#endif
+
+            float ax = __SHFL(WARP_MASK, sAtomx, pos);
+            float ay = __SHFL(WARP_MASK, sAtomy, pos);
+            float az = __SHFL(WARP_MASK, sAtomz, pos);
+            int pred = 0;
+            if (pos >= 0) {
+              float dx = xi - ax;
+              float dy = yi - ay;
+              float dz = zi - az;
+              float r2 = dx * dx + dy * dy + dz * dz;
+              pred = (r2 < cutPlusSkin2);
+            }
+
+            // Signal acceptance or rejection of atoms
+            if (__BALLOT(WARP_MASK, pred) & mask) {
+              apred |= (PMEMask)1 << pos;
+            }
+          }
+          __SYNCWARP(WARP_MASK);
+          // HIP-TODO: Support PME_ATOMS_PER_WARP = 32 and 8
+#if (PME_ATOMS_PER_WARP < 32)
+          psWarp->exclusionMask[tgx] = apred;
+          __SYNCWARP(WARP_MASK);
+#if (PME_ATOMS_PER_WARP == 8)
+          psWarp->exclusionMask[tgx] |= psWarp->exclusionMask[tgx ^ 8];
+          __SYNCWARP(WARP_MASK);
+          psWarp->exclusionMask[tgx] |= psWarp->exclusionMask[tgx ^ 16];
+          __SYNCWARP(WARP_MASK);
+          apred = psWarp->exclusionMask[tgx];
+#else
+#ifdef AMBER_PLATFORM_AMD_WARP64
+          psWarp->exclusionMask[tgx] |= psWarp->exclusionMask[tgx ^ 16];
+          __SYNCWARP(WARP_MASK);
+          psWarp->exclusionMask[tgx] |= psWarp->exclusionMask[tgx ^ 32];
+          __SYNCWARP(WARP_MASK);
+          apred = psWarp->exclusionMask[tgx];
+#else
+          apred |= psWarp->exclusionMask[tgx ^ 16];
+#endif
+#endif
+#endif
+          bpred = apred;
+
+          // Add all accepted atoms to atom list
+          while (bpred) {
+            int maxAccepts = min(GRID - atoms, maskPopc(bpred));
+            int pos = -1;
+
+            // Find number of predecessor bits and determine if thread can add atom
+            if (bpred & threadmask) {
+              PMEMask mask = threadmask - 1;
+              pos = maskPopc(bpred & mask);
+              if (pos >= maxAccepts) {
+                pos = -1;
+              }
+            }
+
+            // Accept each atom if there's room
+            if (pos != -1) {
+              unsigned int atom = xpos + tgx;
+              minAtom = min(minAtom, atom);
+              maxAtom = max(maxAtom, atom);
+              psWarp->atomList[atoms + pos] = (atom << NLATOM_CELL_SHIFT) | shCellID;
+
+              // Bitwise exclusive OR plus assignment compound operator
+              bpred ^= threadmask;
+            }
+            __SYNCWARP(WARP_MASK);
+            atoms += maxAccepts;
+
+            // Output GRID atoms if ready
+            if (atoms == GRID) {
+
+              // Write swath of atoms to global memory
+              cSim.pNLAtomList[psWarp->offset + tgx] = psWarp->atomList[tgx];
+              __SYNCWARP(WARP_MASK);
+              if (tgx == 0) {
+                psWarp->offset += GRID;
+                psWarp->nlEntry.NL.xatoms += GRID;
+              }
+              __SYNCWARP(WARP_MASK);
+
+              // Clear used bits from bpred
+              bpred &= __SHFL(WARP_MASK, bpred, tgx ^ 1);
+              bpred &= __SHFL(WARP_MASK, bpred, tgx ^ 2);
+              bpred &= __SHFL(WARP_MASK, bpred, tgx ^ 4);
+              bpred &= __SHFL(WARP_MASK, bpred, tgx ^ 8);
+              bpred &= __SHFL(WARP_MASK, bpred, tgx ^ 16);
+#ifdef AMBER_PLATFORM_AMD_WARP64
+              bpred &= __SHFL(WARP_MASK, bpred, tgx ^ 32);
+#endif
+
+              // Reduce minatom and maxatom
+              minAtom = min(minAtom, __SHFL(WARP_MASK, minAtom, tgx ^ 1));
+              minAtom = min(minAtom, __SHFL(WARP_MASK, minAtom, tgx ^ 2));
+              minAtom = min(minAtom, __SHFL(WARP_MASK, minAtom, tgx ^ 4));
+              minAtom = min(minAtom, __SHFL(WARP_MASK, minAtom, tgx ^ 8));
+              minAtom = min(minAtom, __SHFL(WARP_MASK, minAtom, tgx ^ 16));
+              maxAtom = max(maxAtom, __SHFL(WARP_MASK, maxAtom, tgx ^ 1));
+              maxAtom = max(maxAtom, __SHFL(WARP_MASK, maxAtom, tgx ^ 2));
+              maxAtom = max(maxAtom, __SHFL(WARP_MASK, maxAtom, tgx ^ 4));
+              maxAtom = max(maxAtom, __SHFL(WARP_MASK, maxAtom, tgx ^ 8));
+              maxAtom = max(maxAtom, __SHFL(WARP_MASK, maxAtom, tgx ^ 16));
+#ifdef AMBER_PLATFORM_AMD_WARP64
+              minAtom = min(minAtom, __SHFL(WARP_MASK, minAtom, tgx ^ 32));
+              maxAtom = max(maxAtom, __SHFL(WARP_MASK, maxAtom, tgx ^ 32));
+#endif
+
+              // Search for y atom exclusions matching any x atom all at once (this should
+              // reduce exclusion tests by a factor of approximately 100 overall).
+              // But first, rule out skipping exclusion test.
+              uint minExclusion = __SHFL(WARP_MASK, shNlRecord.u, NEIGHBOR_CELLS + 12);
+              uint maxExclusion = __SHFL(WARP_MASK, shNlRecord.u, NEIGHBOR_CELLS + 13);
+
+              psWarp->exclusionMask[tgx] = 0;
+              __SYNCWARP(WARP_MASK);
+              if ((minAtom <= maxExclusion) && (maxAtom >= minExclusion)) {
+                uint atom = (psWarp->atomList[tgx] >> NLATOM_CELL_SHIFT);
+                for (int j = 0; j < totalExclusions; j += GRID) {
+                  int offset = j + tgx;
+                  unsigned int eatom = 0xffffffff;
+                  if (offset < totalExclusions) {
+                    eatom = psExclusion[offset] >> NLEXCLUSION_SHIFT;
+                  }
+                  PMEMask vote = __BALLOT(WARP_MASK, (eatom >= minAtom) && (eatom <= maxAtom));
+
+                  while (vote) {
+                    unsigned int k = maskFfs(vote) - 1;
+                    offset = k + j;
+                    eatom = psExclusion[offset] >> NLEXCLUSION_SHIFT;
+                    if (atom == eatom) {
+                      psWarp->exclusionMask[psExclusion[offset] & NLEXCLUSION_ATOM_MASK] |=
+                        threadmask;
+                    }
+                    __SYNCWARP(WARP_MASK);
+                    vote ^= (PMEMask)1 << k;
+                  }
+                }
+              }
+              __SYNCWARP(WARP_MASK);
+
+              // Output exclusion masks
+              if (tgx < cSim.NLAtomsPerWarp) {
+                PMEMask emask = psWarp->exclusionMask[tgx];
+                ((PMEMask*)&cSim.pNLAtomList[psWarp->offset])[tgx] =
+                  ((emask >> tgx) & exclusionMask) |
+                  ((emask << (cSim.NLAtomsPerWarp - tgx)) & ~exclusionMask);
+              }
+              __SYNCWARP(WARP_MASK);
+              if (tgx == 0)
+                psWarp->offset += cSim.NLAtomsPerWarp * sizeof(PMEMask) / sizeof(unsigned int);
+              atoms = 0;
+              psWarp->atomList[tgx] = 0;
+              minAtom = cSim.atoms;
+              maxAtom = 0;
+              __SYNCWARP(WARP_MASK);
+
+              // Output neighbor list entry if width is sufficient
+              if (psWarp->nlEntry.NL.xatoms >= cSim.NLXEntryWidth) {
+                uint nlpos;
+                if (tgx == 0) {
+                  nlpos = atomicAdd(cSim.pNLEntries, 1);
+                }
+                nlpos = __SHFL(WARP_MASK, nlpos, 0);
+                if (tgx < 4) {
+                  cSim.pNLEntry[nlpos].array[tgx] = psWarp->nlEntry.array[tgx];
+                }
+                __SYNCWARP(0xFFFFFFFF);
+                if (tgx == 0) {
+                  psWarp->nlEntry.NL.xatoms  = 0;
+                  psWarp->nlEntry.NL.ymax   &= ~NLENTRY_HOME_CELL_MASK;
+                  psWarp->nlEntry.NL.offset  = psWarp->offset;
+                }
+              }
+            }
+            else {
+              bpred = 0;
+            }
+            // End contingency for atoms == GRID
+          }
+          // End loop for adding atoms to the non-bonded list while bpred != 0
+
+          // Move to next swath of atoms
+          xpos += GRID;
+        }
+
+        // Move to next cell
+        cpos++;
+      }
+
+      // Output last batch of atoms for this swath
+      if (atoms > 0) {
+        // Reduce minatom and maxatom
+        minAtom = min(minAtom, __SHFL(WARP_MASK, minAtom, tgx ^ 1));
+        minAtom = min(minAtom, __SHFL(WARP_MASK, minAtom, tgx ^ 2));
+        minAtom = min(minAtom, __SHFL(WARP_MASK, minAtom, tgx ^ 4));
+        minAtom = min(minAtom, __SHFL(WARP_MASK, minAtom, tgx ^ 8));
+        minAtom = min(minAtom, __SHFL(WARP_MASK, minAtom, tgx ^ 16));
+        maxAtom = max(maxAtom, __SHFL(WARP_MASK, maxAtom, tgx ^ 1));
+        maxAtom = max(maxAtom, __SHFL(WARP_MASK, maxAtom, tgx ^ 2));
+        maxAtom = max(maxAtom, __SHFL(WARP_MASK, maxAtom, tgx ^ 4));
+        maxAtom = max(maxAtom, __SHFL(WARP_MASK, maxAtom, tgx ^ 8));
+        maxAtom = max(maxAtom, __SHFL(WARP_MASK, maxAtom, tgx ^ 16));
+#ifdef AMBER_PLATFORM_AMD_WARP64
+        minAtom = min(minAtom, __SHFL(WARP_MASK, minAtom, tgx ^ 32));
+        maxAtom = max(maxAtom, __SHFL(WARP_MASK, maxAtom, tgx ^ 32));
+#endif
+        if (tgx < atoms) {
+          cSim.pNLAtomList[psWarp->offset + tgx] = psWarp->atomList[tgx];
+        }
+        else {
+          cSim.pNLAtomList[psWarp->offset + tgx] = 0;
+        }
+        __SYNCWARP(WARP_MASK);
+        if (tgx == 0)
+          psWarp->offset += GRID;
+
+        // Search for y atom exclusions matching any x atom all at once (this should
+        // reduce exclusion tests by a factor of approximately 100 overall)
+        uint minExclusion = __SHFL(WARP_MASK, shNlRecord.u, NEIGHBOR_CELLS + 12);
+        uint maxExclusion = __SHFL(WARP_MASK, shNlRecord.u, NEIGHBOR_CELLS + 13);
+        psWarp->exclusionMask[tgx] = 0;
+        __SYNCWARP(WARP_MASK);
+        if ((tgx < atoms) && ((minAtom <= maxExclusion) && (maxAtom >= minExclusion))) {
+          unsigned int atom = (psWarp->atomList[tgx] >> NLATOM_CELL_SHIFT);
+          for (int j = 0; j < totalExclusions; j++) {
+            if ((psExclusion[j] >> NLEXCLUSION_SHIFT) == atom) {
+              atomicOr((PMEMask*)(&psWarp->exclusionMask[psExclusion[j] &
+                                  NLEXCLUSION_ATOM_MASK]), threadmask);
+            }
+          }
+        }
+        __SYNCWARP(WARP_MASK);
+
+        // Output exclusion masks
+        if (tgx < cSim.NLAtomsPerWarp) {
+          PMEMask emask = psWarp->exclusionMask[tgx];
+          ((PMEMask*)&cSim.pNLAtomList[psWarp->offset])[tgx] =
+            ((emask >> tgx) & exclusionMask) |
+            ((emask << (cSim.NLAtomsPerWarp - tgx)) & ~exclusionMask);
+        }
+        __SYNCWARP(WARP_MASK);
+        if (tgx == 0) {
+          psWarp->nlEntry.NL.xatoms += atoms;
+          psWarp->offset += cSim.NLAtomsPerWarp * sizeof(PMEMask) / sizeof(unsigned int);
+        }
+        atoms  = 0;
+        psWarp->atomList[tgx] = 0;
+      }
+      __SYNCWARP(0xFFFFFFFF);
+      // End contingency for committing the final batch of atoms
+
+      // Output final neighbor list entry if xatoms > 0
+      if ((psWarp->nlEntry.NL.xatoms > 0) || (psWarp->nlEntry.NL.ymax & NLENTRY_HOME_CELL_MASK)) {
+        uint nlpos;
+        if (tgx == 0) {
+          nlpos = atomicAdd(cSim.pNLEntries, 1);
+        }
+        nlpos = __SHFL(WARP_MASK, nlpos, 0);
+        if (tgx < 4) {
+          cSim.pNLEntry[nlpos].array[tgx] = psWarp->nlEntry.array[tgx];
+        }
+      }
+      __SYNCWARP(WARP_MASK);
+
+      // Advance to next swath of atoms
+      ypos += cSim.NLYDivisor * cSim.NLAtomsPerWarp;
+    }
+
+    // Advance to next NLRecord entry
+    if (tgx == (NEIGHBOR_CELLS + 2)) {
+      shNlRecord.u = atomicAdd(&cSim.pFrcBlkCounters[0], 1);
+    }
+  }
+#undef VOLATILE
+}
diff --git "a/C:\\amber\\amber_master\\src\\pmemd\\src/cuda/kBNL_AMD.h" "b/C:\\amber\\amber_mint\\src\\pmemd\\src/cuda/kBNL_AMD.h"
index 3d32247..b8f2ae7 100644
--- "a/C:\\amber\\amber_master\\src\\pmemd\\src/cuda/kBNL_AMD.h"
+++ "b/C:\\amber\\amber_mint\\src\\pmemd\\src/cuda/kBNL_AMD.h"
@@ -130,22 +130,29 @@
       float xi;
       float yi;
       float zi;
+      int xi_i32 = 0;
+      int yi_i32 = 0;
+      int zi_i32 = 0;
+      int validY = 0;
       unsigned int index = ypos + (tgx & cSim.NLAtomsPerWarpBitsMask);
       if (index < ymax) {
-#ifndef use_DPFP
-        PMEFloat2 xy = tex1Dfetch<float2>(cSim.texAtomXYSP, index);
-        zi = tex1Dfetch<float>(cSim.texAtomZSP, index);
-#else
-        PMEFloat2 xy = cSim.pAtomXYSP[index];
-        zi = cSim.pAtomZSP[index];
-#endif
-        xi = xy.x;
-        yi = xy.y;
+        int4 coord = cSim.pAtomCoord_M32[index];
+        xi_i32 = coord.x;
+        yi_i32 = coord.y;
+        zi_i32 = coord.z;
+        xi = (PMEFloat)xi_i32 * cSim.mint32_inv_scale.x;
+        yi = (PMEFloat)yi_i32 * cSim.mint32_inv_scale.y;
+        zi = (PMEFloat)zi_i32 * cSim.mint32_inv_scale.z;
+        validY = 1;
       }
       else {
         xi = (float)10000.0 * index;
         yi = (float)10000.0 * index;
         zi = (float)10000.0 * index;
+        xi_i32 = 0;
+        yi_i32 = 0;
+        zi_i32 = 0;
+        validY = 0;
       }
 
 #ifndef PME_IS_ORTHOGONAL
@@ -292,23 +299,30 @@
           float sAtomx;
           float sAtomy;
           float sAtomz;
+          int sAtomx_i32 = 0;
+          int sAtomy_i32 = 0;
+          int sAtomz_i32 = 0;
+          int sAtomValid = 0;
 
           // Read up to GRID atoms
           if (tgx < xmax) {
-#ifndef use_DPFP
-            PMEFloat2 xy = tex1Dfetch<float2>(cSim.texAtomXYSP, xpos + tgx);
-            sAtomz = tex1Dfetch<float>(cSim.texAtomZSP, xpos + tgx);
-#else
-            PMEFloat2 xy = cSim.pAtomXYSP[xpos + tgx];
-            sAtomz = cSim.pAtomZSP[xpos + tgx];
-#endif
-            sAtomx = xy.x;
-            sAtomy = xy.y;
+            int4 coord = cSim.pAtomCoord_M32[xpos + tgx];
+            sAtomx_i32 = coord.x;
+            sAtomy_i32 = coord.y;
+            sAtomz_i32 = coord.z;
+            sAtomx = (PMEFloat)sAtomx_i32 * cSim.mint32_inv_scale.x;
+            sAtomy = (PMEFloat)sAtomy_i32 * cSim.mint32_inv_scale.y;
+            sAtomz = (PMEFloat)sAtomz_i32 * cSim.mint32_inv_scale.z;
+            sAtomValid = 1;
           }
           else {
             sAtomx = INFINITY;
             sAtomy = INFINITY;
             sAtomz = INFINITY;
+            sAtomx_i32 = 0;
+            sAtomy_i32 = 0;
+            sAtomz_i32 = 0;
+            sAtomValid = 0;
           }
 
           // Translate all atoms into a local coordinate system within one unit
@@ -361,16 +375,20 @@
                groupOffset += GRID / PME_ATOMS_PER_WARP) {
             int pos = psWarp->setBitPos[groupOffset + groupI];
 
-            float ax = __SHFL(WARP_MASK, sAtomx, pos);
-            float ay = __SHFL(WARP_MASK, sAtomy, pos);
-            float az = __SHFL(WARP_MASK, sAtomz, pos);
-            float dx = xi - ax;
-            float dy = yi - ay;
-            float dz = zi - az;
+            int ax_i32 = __SHFL(WARP_MASK, sAtomx_i32, pos);
+            int ay_i32 = __SHFL(WARP_MASK, sAtomy_i32, pos);
+            int az_i32 = __SHFL(WARP_MASK, sAtomz_i32, pos);
+            int axValid = __SHFL(WARP_MASK, sAtomValid, pos);
+            float dx_frac = (float)(xi_i32 - ax_i32) * cSim.mint32_inv_scale.x;
+            float dy_frac = (float)(yi_i32 - ay_i32) * cSim.mint32_inv_scale.y;
+            float dz_frac = (float)(zi_i32 - az_i32) * cSim.mint32_inv_scale.z;
+            float dx = dx_frac;
+            float dy = dy_frac;
+            float dz = dz_frac;
             float r2 = dx * dx + dy * dy + dz * dz;
 
             // Signal acceptance or rejection of atoms
-            if ((groupOffset + groupI < setBitCount) && (r2 < cutPlusSkin2)) {
+            if ((groupOffset + groupI < setBitCount) && validY && axValid && (r2 < cutPlusSkin2)) {
               psWarp->accepted[pos] = true;
             }
           }
diff --git "a/C:\\amber\\amber_master\\src\\pmemd\\src/cuda/kBWU.h" "b/C:\\amber\\amber_mint\\src\\pmemd\\src/cuda/kBWU.h"
index 8dc3322..3c04f8a 100644
--- "a/C:\\amber\\amber_master\\src\\pmemd\\src/cuda/kBWU.h"
+++ "b/C:\\amber\\amber_mint\\src\\pmemd\\src/cuda/kBWU.h"
@@ -16,9 +16,10 @@
 #else
 #  define VOLATILE
 #endif
-  __shared__ volatile PMEDouble atmcrdx[BOND_WORK_UNIT_THREADS_PER_BLOCK];
-  __shared__ volatile PMEDouble atmcrdy[BOND_WORK_UNIT_THREADS_PER_BLOCK];
-  __shared__ volatile PMEDouble atmcrdz[BOND_WORK_UNIT_THREADS_PER_BLOCK];
+  // MINT32: Store integer coordinates for precise distance calculations
+  __shared__ volatile int atmcrdx[BOND_WORK_UNIT_THREADS_PER_BLOCK];
+  __shared__ volatile int atmcrdy[BOND_WORK_UNIT_THREADS_PER_BLOCK];
+  __shared__ volatile int atmcrdz[BOND_WORK_UNIT_THREADS_PER_BLOCK];
 #ifdef PHMD
   __shared__ volatile PMEFloat charges[BOND_WORK_UNIT_THREADS_PER_BLOCK];
   __shared__ volatile int indices[BOND_WORK_UNIT_THREADS_PER_BLOCK];
@@ -72,23 +73,30 @@
     int readpos = (3 * BOND_WORK_UNIT_THREADS_PER_BLOCK * wuidx) + threadIdx.x;
     instructions[threadIdx.x] = cSim.pBwuInstructions[readpos];
     __syncthreads();
-
-    // Read coordinates
+    // Read coordinates - MINT32: Store integer coordinates for precise distance calculations
+    // pAtomCoord is in sorted/remapped order after gpu_build_neighbor_list_
+    // Distance deltas computed as int subtraction, then converted to real units
     if (threadIdx.x < instructions[ATOM_IMPORT_COUNT_IDX]) {
 #ifdef LOCAL_NEIGHBORLIST
-      readpos = cSim.pBwuInstructions[readpos + 2*BOND_WORK_UNIT_THREADS_PER_BLOCK];
-      atmcrdx[threadIdx.x] = cSim.pImageX[readpos];
-      atmcrdy[threadIdx.x] = cSim.pImageY[readpos]; 
-      atmcrdz[threadIdx.x] = cSim.pImageZ[readpos]; 
+      // Read original atom ID and sorted position for debug
+      int origAtomID = cSim.pBwuInstructions[(3 * BOND_WORK_UNIT_THREADS_PER_BLOCK * wuidx) + threadIdx.x + BOND_WORK_UNIT_THREADS_PER_BLOCK];
+      readpos = cSim.pBwuInstructions[(3 * BOND_WORK_UNIT_THREADS_PER_BLOCK * wuidx) + threadIdx.x + 2*BOND_WORK_UNIT_THREADS_PER_BLOCK];
+      // Read fixed-point coordinates directly (current sorted buffer)
+      int4 coord = cSim.pAtomCoord_M32[readpos];
+      atmcrdx[threadIdx.x] = coord.x;
+      atmcrdy[threadIdx.x] = coord.y;
+      atmcrdz[threadIdx.x] = coord.z;
 #ifdef PHMD
       indices[threadIdx.x] = readpos;
       charges[threadIdx.x] = cSim.pImageCharge[readpos];
 #endif
 #else
       readpos = cSim.pBwuInstructions[readpos + BOND_WORK_UNIT_THREADS_PER_BLOCK];
-      atmcrdx[threadIdx.x] = cSim.pAtomX[readpos];
-      atmcrdy[threadIdx.x] = cSim.pAtomY[readpos];
-      atmcrdz[threadIdx.x] = cSim.pAtomZ[readpos];
+      // Read fixed-point coordinates directly (current sorted buffer)
+      int4 coord = cSim.pAtomCoord[readpos];
+      atmcrdx[threadIdx.x] = coord.x;
+      atmcrdy[threadIdx.x] = coord.y;
+      atmcrdz[threadIdx.x] = coord.z;
 #ifdef PHMD
       indices[threadIdx.x] = readpos;
       charges[threadIdx.x] = cSim.pAtomChargeSP[readpos];
diff --git "a/C:\\amber\\amber_master\\src\\pmemd\\src/cuda/kBWU_angl.h" "b/C:\\amber\\amber_mint\\src\\pmemd\\src/cuda/kBWU_angl.h"
index f8e13fa..bf28797 100644
--- "a/C:\\amber\\amber_master\\src\\pmemd\\src/cuda/kBWU_angl.h"
+++ "b/C:\\amber\\amber_mint\\src\\pmemd\\src/cuda/kBWU_angl.h"
@@ -40,12 +40,19 @@
     unsigned int atmI = rawID >> 16;
     unsigned int atmJ = (rawID >> 8) & 0xff;
     unsigned int atmK = rawID & 0xff;
-    PMEDouble xij     = atmcrdx[atmI] - atmcrdx[atmJ];
-    PMEDouble yij     = atmcrdy[atmI] - atmcrdy[atmJ];
-    PMEDouble zij     = atmcrdz[atmI] - atmcrdz[atmJ];
-    PMEDouble xkj     = atmcrdx[atmK] - atmcrdx[atmJ];
-    PMEDouble ykj     = atmcrdy[atmK] - atmcrdy[atmJ];
-    PMEDouble zkj     = atmcrdz[atmK] - atmcrdz[atmJ];
+    // MINT32: Compute distance deltas as integer subtraction, then convert to real units
+    int dxij          = atmcrdx[atmI] - atmcrdx[atmJ];
+    int dyij          = atmcrdy[atmI] - atmcrdy[atmJ];
+    int dzij          = atmcrdz[atmI] - atmcrdz[atmJ];
+    int dxkj          = atmcrdx[atmK] - atmcrdx[atmJ];
+    int dykj          = atmcrdy[atmK] - atmcrdy[atmJ];
+    int dzkj          = atmcrdz[atmK] - atmcrdz[atmJ];
+    PMEDouble xij     = (PMEDouble)((double)dxij * cSim.mint32_inv_scale.x);
+    PMEDouble yij     = (PMEDouble)((double)dyij * cSim.mint32_inv_scale.y);
+    PMEDouble zij     = (PMEDouble)((double)dzij * cSim.mint32_inv_scale.z);
+    PMEDouble xkj     = (PMEDouble)((double)dxkj * cSim.mint32_inv_scale.x);
+    PMEDouble ykj     = (PMEDouble)((double)dykj * cSim.mint32_inv_scale.y);
+    PMEDouble zkj     = (PMEDouble)((double)dzkj * cSim.mint32_inv_scale.z);
     PMEDouble rij     = xij*xij + yij*yij + zij*zij;
     PMEDouble rkj     = xkj*xkj + ykj*ykj + zkj*zkj;
     PMEDouble rik     = sqrt(rij * rkj);
diff --git "a/C:\\amber\\amber_master\\src\\pmemd\\src/cuda/kBWU_bond.h" "b/C:\\amber\\amber_mint\\src\\pmemd\\src/cuda/kBWU_bond.h"
index 6e8c4e0..575c628 100644
--- "a/C:\\amber\\amber_master\\src\\pmemd\\src/cuda/kBWU_bond.h"
+++ "b/C:\\amber\\amber_mint\\src\\pmemd\\src/cuda/kBWU_bond.h"
@@ -43,11 +43,23 @@
 #endif
     unsigned int atmI  = rawID >> 8;
     unsigned int atmJ  = rawID & 0xff;
-    PMEDouble xij      = atmcrdx[atmI] - atmcrdx[atmJ];
-    PMEDouble yij      = atmcrdy[atmI] - atmcrdy[atmJ];
-    PMEDouble zij      = atmcrdz[atmI] - atmcrdz[atmJ];
+    // MINT32: Compute distance delta as integer subtraction (handles wrapping via 2's complement)
+    // Then convert to real units for energy/force calculation
+    int dxij           = atmcrdx[atmI] - atmcrdx[atmJ];
+    int dyij           = atmcrdy[atmI] - atmcrdy[atmJ];
+    int dzij           = atmcrdz[atmI] - atmcrdz[atmJ];
+    PMEDouble xij      = (PMEDouble)((double)dxij * cSim.mint32_inv_scale.x);
+    PMEDouble yij      = (PMEDouble)((double)dyij * cSim.mint32_inv_scale.y);
+    PMEDouble zij      = (PMEDouble)((double)dzij * cSim.mint32_inv_scale.z);
     PMEDouble rij      = sqrt(xij*xij + yij*yij + zij*zij);
     PMEDouble2 bond    = cSim.pBwuBond[startidx + tgx];
+#ifdef LOCAL_NEIGHBORLIST
+    int offset = (3*curr_wuidx + 2) * BOND_WORK_UNIT_THREADS_PER_BLOCK;
+#else
+    int offset = (3*curr_wuidx + 1) * BOND_WORK_UNIT_THREADS_PER_BLOCK;
+#endif
+    int globalPosI = cSim.pBwuInstructions[offset + atmI];
+    int globalPosJ = cSim.pBwuInstructions[offset + atmJ];
     PMEDouble da       = rij - bond.y;
     PMEDouble df       = bond.x * da;
     PMEDouble dfw      = (df + df) / rij;
@@ -61,15 +73,6 @@
     PMEDouble fx = dfw * xij;
     PMEDouble fy = dfw * yij;
     PMEDouble fz = dfw * zij;
-#ifdef LOCAL_NEIGHBORLIST
-    int offset = (3*curr_wuidx + 2) * BOND_WORK_UNIT_THREADS_PER_BLOCK;
-    int globalPosI = cSim.pBwuInstructions[offset + atmI];
-    int globalPosJ = cSim.pBwuInstructions[offset + atmJ];
-#else
-    int offset = (3*curr_wuidx + 1) * BOND_WORK_UNIT_THREADS_PER_BLOCK;
-    int globalPosI = cSim.pBwuInstructions[offset + atmI];
-    int globalPosJ = cSim.pBwuInstructions[offset + atmJ];
-#endif
     PMEAccumulator ifx = llrint(fx * FORCESCALE);
     PMEAccumulator ify = llrint(fy * FORCESCALE);
     PMEAccumulator ifz = llrint(fz * FORCESCALE);
diff --git "a/C:\\amber\\amber_master\\src\\pmemd\\src/cuda/kBWU_cimp.h" "b/C:\\amber\\amber_mint\\src\\pmemd\\src/cuda/kBWU_cimp.h"
index 29ac716..cd31899 100644
--- "a/C:\\amber\\amber_master\\src\\pmemd\\src/cuda/kBWU_cimp.h"
+++ "b/C:\\amber\\amber_mint\\src\\pmemd\\src/cuda/kBWU_cimp.h"
@@ -35,15 +35,25 @@
     unsigned int atmK = (rawID >>  8) & 0xff;
     unsigned int atmL = rawID & 0xff;
     PMEDouble2 impDihedral = cSim.pBwuCImp[startidx + tgx];
-    PMEDouble xij      = atmcrdx[atmI] - atmcrdx[atmJ];
-    PMEDouble yij      = atmcrdy[atmI] - atmcrdy[atmJ];
-    PMEDouble zij      = atmcrdz[atmI] - atmcrdz[atmJ];
-    PMEDouble xkj      = atmcrdx[atmK] - atmcrdx[atmJ];
-    PMEDouble ykj      = atmcrdy[atmK] - atmcrdy[atmJ];
-    PMEDouble zkj      = atmcrdz[atmK] - atmcrdz[atmJ];
-    PMEDouble xlk      = atmcrdx[atmL] - atmcrdx[atmK];
-    PMEDouble ylk      = atmcrdy[atmL] - atmcrdy[atmK];
-    PMEDouble zlk      = atmcrdz[atmL] - atmcrdz[atmK];
+    // MINT32: Compute distance deltas as integer subtraction, then convert to real units
+    int dxij           = atmcrdx[atmI] - atmcrdx[atmJ];
+    int dyij           = atmcrdy[atmI] - atmcrdy[atmJ];
+    int dzij           = atmcrdz[atmI] - atmcrdz[atmJ];
+    int dxkj           = atmcrdx[atmK] - atmcrdx[atmJ];
+    int dykj           = atmcrdy[atmK] - atmcrdy[atmJ];
+    int dzkj           = atmcrdz[atmK] - atmcrdz[atmJ];
+    int dxlk           = atmcrdx[atmL] - atmcrdx[atmK];
+    int dylk           = atmcrdy[atmL] - atmcrdy[atmK];
+    int dzlk           = atmcrdz[atmL] - atmcrdz[atmK];
+    PMEDouble xij      = (PMEDouble)dxij * cSim.mint32_inv_scale.x;
+    PMEDouble yij      = (PMEDouble)dyij * cSim.mint32_inv_scale.y;
+    PMEDouble zij      = (PMEDouble)dzij * cSim.mint32_inv_scale.z;
+    PMEDouble xkj      = (PMEDouble)dxkj * cSim.mint32_inv_scale.x;
+    PMEDouble ykj      = (PMEDouble)dykj * cSim.mint32_inv_scale.y;
+    PMEDouble zkj      = (PMEDouble)dzkj * cSim.mint32_inv_scale.z;
+    PMEDouble xlk      = (PMEDouble)dxlk * cSim.mint32_inv_scale.x;
+    PMEDouble ylk      = (PMEDouble)dylk * cSim.mint32_inv_scale.y;
+    PMEDouble zlk      = (PMEDouble)dzlk * cSim.mint32_inv_scale.z;
 
     // Calculate phi and quantities required for gradient
     PMEDouble oneOverRKJ = rsqrt(xkj*xkj + ykj*ykj + zkj*zkj);
diff --git "a/C:\\amber\\amber_master\\src\\pmemd\\src/cuda/kBWU_cmap.h" "b/C:\\amber\\amber_mint\\src\\pmemd\\src/cuda/kBWU_cmap.h"
index 7eccfe7..dae4bdf 100644
--- "a/C:\\amber\\amber_master\\src\\pmemd\\src/cuda/kBWU_cmap.h"
+++ "b/C:\\amber\\amber_mint\\src\\pmemd\\src/cuda/kBWU_cmap.h"
@@ -34,15 +34,25 @@
     unsigned int atmJ = (rawID >> 16) & 0xff;
     unsigned int atmK = (rawID >>  8) & 0xff;
     unsigned int atmL = rawID & 0xff;
-    PMEFloat xij     = atmcrdx[atmI] - atmcrdx[atmJ];
-    PMEFloat yij     = atmcrdy[atmI] - atmcrdy[atmJ];
-    PMEFloat zij     = atmcrdz[atmI] - atmcrdz[atmJ];
-    PMEFloat xkj     = atmcrdx[atmK] - atmcrdx[atmJ];
-    PMEFloat ykj     = atmcrdy[atmK] - atmcrdy[atmJ];
-    PMEFloat zkj     = atmcrdz[atmK] - atmcrdz[atmJ];
-    PMEFloat xlk     = atmcrdx[atmL] - atmcrdx[atmK];
-    PMEFloat ylk     = atmcrdy[atmL] - atmcrdy[atmK];
-    PMEFloat zlk     = atmcrdz[atmL] - atmcrdz[atmK];
+    // MINT32: Compute distance deltas as integer subtraction, then convert to real units
+    int dxij         = atmcrdx[atmI] - atmcrdx[atmJ];
+    int dyij         = atmcrdy[atmI] - atmcrdy[atmJ];
+    int dzij         = atmcrdz[atmI] - atmcrdz[atmJ];
+    int dxkj         = atmcrdx[atmK] - atmcrdx[atmJ];
+    int dykj         = atmcrdy[atmK] - atmcrdy[atmJ];
+    int dzkj         = atmcrdz[atmK] - atmcrdz[atmJ];
+    int dxlk         = atmcrdx[atmL] - atmcrdx[atmK];
+    int dylk         = atmcrdy[atmL] - atmcrdy[atmK];
+    int dzlk         = atmcrdz[atmL] - atmcrdz[atmK];
+    PMEFloat xij     = (PMEFloat)((double)dxij * cSim.mint32_inv_scale.x);
+    PMEFloat yij     = (PMEFloat)((double)dyij * cSim.mint32_inv_scale.y);
+    PMEFloat zij     = (PMEFloat)((double)dzij * cSim.mint32_inv_scale.z);
+    PMEFloat xkj     = (PMEFloat)((double)dxkj * cSim.mint32_inv_scale.x);
+    PMEFloat ykj     = (PMEFloat)((double)dykj * cSim.mint32_inv_scale.y);
+    PMEFloat zkj     = (PMEFloat)((double)dzkj * cSim.mint32_inv_scale.z);
+    PMEFloat xlk     = (PMEFloat)((double)dxlk * cSim.mint32_inv_scale.x);
+    PMEFloat ylk     = (PMEFloat)((double)dylk * cSim.mint32_inv_scale.y);
+    PMEFloat zlk     = (PMEFloat)((double)dzlk * cSim.mint32_inv_scale.z);
 
     // Calculate phi and quantities required for gradient
     PMEFloat oneOverRKJ = rsqrt(xkj*xkj + ykj*ykj + zkj*zkj);
@@ -100,9 +110,13 @@
     upxjk               *= oneOverRUJK;
     upyjk               *= oneOverRUJK;
     upzjk               *= oneOverRUJK;
-    PMEFloat xml         = atmcrdx[atmM] - atmcrdx[atmL];
-    PMEFloat yml         = atmcrdy[atmM] - atmcrdy[atmL];
-    PMEFloat zml         = atmcrdz[atmM] - atmcrdz[atmL];
+    // MINT32: Compute distance deltas as integer subtraction, then convert to real units
+    int dxml             = atmcrdx[atmM] - atmcrdx[atmL];
+    int dyml             = atmcrdy[atmM] - atmcrdy[atmL];
+    int dzml             = atmcrdz[atmM] - atmcrdz[atmL];
+    PMEFloat xml         = (PMEFloat)((double)dxml * cSim.mint32_inv_scale.x);
+    PMEFloat yml         = (PMEFloat)((double)dyml * cSim.mint32_inv_scale.y);
+    PMEFloat zml         = (PMEFloat)((double)dzml * cSim.mint32_inv_scale.z);
     PMEFloat dotMLLK     = xml*uxlk + yml*uylk + zml*uzlk;
     PMEFloat upxml       = xml - dotMLLK*uxlk;
     PMEFloat upyml       = yml - dotMLLK*uylk;
diff --git "a/C:\\amber\\amber_master\\src\\pmemd\\src/cuda/kBWU_cnst.h" "b/C:\\amber\\amber_mint\\src\\pmemd\\src/cuda/kBWU_cnst.h"
index 7d0fc69..8363cca 100644
--- "a/C:\\amber\\amber_master\\src\\pmemd\\src/cuda/kBWU_cnst.h"
+++ "b/C:\\amber\\amber_mint\\src\\pmemd\\src/cuda/kBWU_cnst.h"
@@ -31,9 +31,13 @@
 #endif
     PMEDouble2 constraint1 = cSim.pBwuCnst[2*startidx + tgx];
     PMEDouble2 constraint2 = cSim.pBwuCnst[2*startidx + GRID + tgx];
-    PMEDouble ax    = atmcrdx[atmI] - constraint1.y;
-    PMEDouble ay    = atmcrdy[atmI] - constraint2.x;
-    PMEDouble az    = atmcrdz[atmI] - constraint2.y;
+    // MINT32: Convert atom coordinate to real, then compute displacement from constraint reference
+    PMEDouble atmx  = (PMEDouble)atmcrdx[atmI] * cSim.mint32_inv_scale.x;
+    PMEDouble atmy  = (PMEDouble)atmcrdy[atmI] * cSim.mint32_inv_scale.y;
+    PMEDouble atmz  = (PMEDouble)atmcrdz[atmI] * cSim.mint32_inv_scale.z;
+    PMEDouble ax    = atmx - constraint1.y;
+    PMEDouble ay    = atmy - constraint2.x;
+    PMEDouble az    = atmz - constraint2.y;
     PMEDouble wx    = constraint1.x * ax;
     PMEDouble wy    = constraint1.x * ay;
     PMEDouble wz    = constraint1.x * az;
diff --git "a/C:\\amber\\amber_master\\src\\pmemd\\src/cuda/kBWU_dihe.h" "b/C:\\amber\\amber_mint\\src\\pmemd\\src/cuda/kBWU_dihe.h"
index 2bf44af..6a28505 100644
--- "a/C:\\amber\\amber_master\\src\\pmemd\\src/cuda/kBWU_dihe.h"
+++ "b/C:\\amber\\amber_mint\\src\\pmemd\\src/cuda/kBWU_dihe.h"
@@ -33,15 +33,25 @@
     unsigned int atmJ = (rawID >> 16) & 0xff;
     unsigned int atmK = (rawID >>  8) & 0xff;
     unsigned int atmL = rawID & 0xff;
-    PMEFloat xij      = atmcrdx[atmI] - atmcrdx[atmJ];
-    PMEFloat yij      = atmcrdy[atmI] - atmcrdy[atmJ];
-    PMEFloat zij      = atmcrdz[atmI] - atmcrdz[atmJ];
-    PMEFloat xkj      = atmcrdx[atmK] - atmcrdx[atmJ];
-    PMEFloat ykj      = atmcrdy[atmK] - atmcrdy[atmJ];
-    PMEFloat zkj      = atmcrdz[atmK] - atmcrdz[atmJ];
-    PMEFloat xkl      = atmcrdx[atmK] - atmcrdx[atmL];
-    PMEFloat ykl      = atmcrdy[atmK] - atmcrdy[atmL];
-    PMEFloat zkl      = atmcrdz[atmK] - atmcrdz[atmL];
+    // MINT32: Compute distance deltas as integer subtraction, then convert to real units
+    int dxij          = atmcrdx[atmI] - atmcrdx[atmJ];
+    int dyij          = atmcrdy[atmI] - atmcrdy[atmJ];
+    int dzij          = atmcrdz[atmI] - atmcrdz[atmJ];
+    int dxkj          = atmcrdx[atmK] - atmcrdx[atmJ];
+    int dykj          = atmcrdy[atmK] - atmcrdy[atmJ];
+    int dzkj          = atmcrdz[atmK] - atmcrdz[atmJ];
+    int dxkl          = atmcrdx[atmK] - atmcrdx[atmL];
+    int dykl          = atmcrdy[atmK] - atmcrdy[atmL];
+    int dzkl          = atmcrdz[atmK] - atmcrdz[atmL];
+    PMEFloat xij      = (PMEFloat)((double)dxij * cSim.mint32_inv_scale.x);
+    PMEFloat yij      = (PMEFloat)((double)dyij * cSim.mint32_inv_scale.y);
+    PMEFloat zij      = (PMEFloat)((double)dzij * cSim.mint32_inv_scale.z);
+    PMEFloat xkj      = (PMEFloat)((double)dxkj * cSim.mint32_inv_scale.x);
+    PMEFloat ykj      = (PMEFloat)((double)dykj * cSim.mint32_inv_scale.y);
+    PMEFloat zkj      = (PMEFloat)((double)dzkj * cSim.mint32_inv_scale.z);
+    PMEFloat xkl      = (PMEFloat)((double)dxkl * cSim.mint32_inv_scale.x);
+    PMEFloat ykl      = (PMEFloat)((double)dykl * cSim.mint32_inv_scale.y);
+    PMEFloat zkl      = (PMEFloat)((double)dzkl * cSim.mint32_inv_scale.z);
     PMEFloat2 dihe1   = cSim.pBwuDihe12[2*startidx + tgx];
     PMEFloat2 dihe2   = cSim.pBwuDihe12[2*startidx + GRID + tgx];
     PMEFloat  dihe3   = cSim.pBwuDihe3[startidx + tgx];
diff --git "a/C:\\amber\\amber_master\\src\\pmemd\\src/cuda/kBWU_nb14.h" "b/C:\\amber\\amber_mint\\src\\pmemd\\src/cuda/kBWU_nb14.h"
index 0a88257..ba2c17d 100644
--- "a/C:\\amber\\amber_master\\src\\pmemd\\src/cuda/kBWU_nb14.h"
+++ "b/C:\\amber\\amber_mint\\src\\pmemd\\src/cuda/kBWU_nb14.h"
@@ -46,9 +46,13 @@
 #endif
     unsigned int atmI = rawID >> 8;
     unsigned int atmJ = rawID & 0xff;
-    PMEFloat dx = atmcrdx[atmJ] - atmcrdx[atmI];
-    PMEFloat dy = atmcrdy[atmJ] - atmcrdy[atmI];
-    PMEFloat dz = atmcrdz[atmJ] - atmcrdz[atmI];
+    // MINT32: Compute distance deltas as integer subtraction, then convert to real units
+    int ddx           = atmcrdx[atmJ] - atmcrdx[atmI];
+    int ddy           = atmcrdy[atmJ] - atmcrdy[atmI];
+    int ddz           = atmcrdz[atmJ] - atmcrdz[atmI];
+    PMEFloat dx = (PMEFloat)((double)ddx * cSim.mint32_inv_scale.x);
+    PMEFloat dy = (PMEFloat)((double)ddy * cSim.mint32_inv_scale.y);
+    PMEFloat dz = (PMEFloat)((double)ddz * cSim.mint32_inv_scale.z);
     PMEFloat2 scnb  = cSim.pBwuLJnb14[startidx + tgx];
 #ifdef PHMD
     unsigned int first = indices[atmI];
diff --git "a/C:\\amber\\amber_master\\src\\pmemd\\src/cuda/kBWU_nmr2.h" "b/C:\\amber\\amber_mint\\src\\pmemd\\src/cuda/kBWU_nmr2.h"
index 7bc205f..911f5f5 100644
--- "a/C:\\amber\\amber_master\\src\\pmemd\\src/cuda/kBWU_nmr2.h"
+++ "b/C:\\amber\\amber_mint\\src\\pmemd\\src/cuda/kBWU_nmr2.h"
@@ -65,9 +65,13 @@
     // Compute displacement, force, and energy
     unsigned int atmI = rawID >> 8;
     unsigned int atmJ = rawID & 0xff;
-    PMEDouble xij     = atmcrdx[atmI] - atmcrdx[atmJ];
-    PMEDouble yij     = atmcrdy[atmI] - atmcrdy[atmJ];
-    PMEDouble zij     = atmcrdz[atmI] - atmcrdz[atmJ];
+    // MINT32: Compute distance deltas as integer subtraction, then convert to real units
+    int dxij          = atmcrdx[atmI] - atmcrdx[atmJ];
+    int dyij          = atmcrdy[atmI] - atmcrdy[atmJ];
+    int dzij          = atmcrdz[atmI] - atmcrdz[atmJ];
+    PMEDouble xij     = (PMEDouble)dxij * cSim.mint32_inv_scale.x;
+    PMEDouble yij     = (PMEDouble)dyij * cSim.mint32_inv_scale.y;
+    PMEDouble zij     = (PMEDouble)dzij * cSim.mint32_inv_scale.z;
     PMEDouble rij     = sqrt(xij*xij + yij*yij + zij*zij);
     PMEDouble df;
 #ifdef LOCAL_ENERGY
diff --git "a/C:\\amber\\amber_master\\src\\pmemd\\src/cuda/kBWU_nmr3.h" "b/C:\\amber\\amber_mint\\src\\pmemd\\src/cuda/kBWU_nmr3.h"
index de5454c..3d75897 100644
--- "a/C:\\amber\\amber_master\\src\\pmemd\\src/cuda/kBWU_nmr3.h"
+++ "b/C:\\amber\\amber_mint\\src\\pmemd\\src/cuda/kBWU_nmr3.h"
@@ -66,12 +66,19 @@
     unsigned int atmI = rawID >> 16;
     unsigned int atmJ = (rawID >> 8) & 0xff;
     unsigned int atmK = rawID & 0xff;
-    PMEDouble xij     = atmcrdx[atmI] - atmcrdx[atmJ];
-    PMEDouble yij     = atmcrdy[atmI] - atmcrdy[atmJ];
-    PMEDouble zij     = atmcrdz[atmI] - atmcrdz[atmJ];
-    PMEDouble xkj     = atmcrdx[atmK] - atmcrdx[atmJ];
-    PMEDouble ykj     = atmcrdy[atmK] - atmcrdy[atmJ];
-    PMEDouble zkj     = atmcrdz[atmK] - atmcrdz[atmJ];
+    // MINT32: Compute distance deltas as integer subtraction, then convert to real units
+    int dxij          = atmcrdx[atmI] - atmcrdx[atmJ];
+    int dyij          = atmcrdy[atmI] - atmcrdy[atmJ];
+    int dzij          = atmcrdz[atmI] - atmcrdz[atmJ];
+    int dxkj          = atmcrdx[atmK] - atmcrdx[atmJ];
+    int dykj          = atmcrdy[atmK] - atmcrdy[atmJ];
+    int dzkj          = atmcrdz[atmK] - atmcrdz[atmJ];
+    PMEDouble xij     = (PMEDouble)dxij * cSim.mint32_inv_scale.x;
+    PMEDouble yij     = (PMEDouble)dyij * cSim.mint32_inv_scale.y;
+    PMEDouble zij     = (PMEDouble)dzij * cSim.mint32_inv_scale.z;
+    PMEDouble xkj     = (PMEDouble)dxkj * cSim.mint32_inv_scale.x;
+    PMEDouble ykj     = (PMEDouble)dykj * cSim.mint32_inv_scale.y;
+    PMEDouble zkj     = (PMEDouble)dzkj * cSim.mint32_inv_scale.z;
     PMEDouble rij2    = xij*xij + yij*yij + zij*zij;
     PMEDouble rkj2    = xkj*xkj + ykj*ykj + zkj*zkj;
     PMEDouble rij     = sqrt(rij2);
diff --git "a/C:\\amber\\amber_master\\src\\pmemd\\src/cuda/kBWU_nmr4.h" "b/C:\\amber\\amber_mint\\src\\pmemd\\src/cuda/kBWU_nmr4.h"
index e6a2b81..6318e20 100644
--- "a/C:\\amber\\amber_master\\src\\pmemd\\src/cuda/kBWU_nmr4.h"
+++ "b/C:\\amber\\amber_mint\\src\\pmemd\\src/cuda/kBWU_nmr4.h"
@@ -67,15 +67,25 @@
     unsigned int atmJ = (rawID >> 16) & 0xff;
     unsigned int atmK = (rawID >> 8) & 0xff;
     unsigned int atmL = rawID & 0xff;
-    PMEDouble xij = atmcrdx[atmI] - atmcrdx[atmJ];
-    PMEDouble yij = atmcrdy[atmI] - atmcrdy[atmJ];
-    PMEDouble zij = atmcrdz[atmI] - atmcrdz[atmJ];
-    PMEDouble xkj = atmcrdx[atmK] - atmcrdx[atmJ];
-    PMEDouble ykj = atmcrdy[atmK] - atmcrdy[atmJ];
-    PMEDouble zkj = atmcrdz[atmK] - atmcrdz[atmJ];
-    PMEDouble xkl = atmcrdx[atmK] - atmcrdx[atmL];
-    PMEDouble ykl = atmcrdy[atmK] - atmcrdy[atmL];
-    PMEDouble zkl = atmcrdz[atmK] - atmcrdz[atmL];
+    // MINT32: Compute distance deltas as integer subtraction, then convert to real units
+    int dxij          = atmcrdx[atmI] - atmcrdx[atmJ];
+    int dyij          = atmcrdy[atmI] - atmcrdy[atmJ];
+    int dzij          = atmcrdz[atmI] - atmcrdz[atmJ];
+    int dxkj          = atmcrdx[atmK] - atmcrdx[atmJ];
+    int dykj          = atmcrdy[atmK] - atmcrdy[atmJ];
+    int dzkj          = atmcrdz[atmK] - atmcrdz[atmJ];
+    int dxkl          = atmcrdx[atmK] - atmcrdx[atmL];
+    int dykl          = atmcrdy[atmK] - atmcrdy[atmL];
+    int dzkl          = atmcrdz[atmK] - atmcrdz[atmL];
+    PMEDouble xij = (PMEDouble)dxij * cSim.mint32_inv_scale.x;
+    PMEDouble yij = (PMEDouble)dyij * cSim.mint32_inv_scale.y;
+    PMEDouble zij = (PMEDouble)dzij * cSim.mint32_inv_scale.z;
+    PMEDouble xkj = (PMEDouble)dxkj * cSim.mint32_inv_scale.x;
+    PMEDouble ykj = (PMEDouble)dykj * cSim.mint32_inv_scale.y;
+    PMEDouble zkj = (PMEDouble)dzkj * cSim.mint32_inv_scale.z;
+    PMEDouble xkl = (PMEDouble)dxkl * cSim.mint32_inv_scale.x;
+    PMEDouble ykl = (PMEDouble)dykl * cSim.mint32_inv_scale.y;
+    PMEDouble zkl = (PMEDouble)dzkl * cSim.mint32_inv_scale.z;
 
     // Calculate ij X jk AND kl X jk:
     PMEDouble dx = yij*zkj - zij*ykj;
diff --git "a/C:\\amber\\amber_master\\src\\pmemd\\src/cuda/kBWU_urey.h" "b/C:\\amber\\amber_mint\\src\\pmemd\\src/cuda/kBWU_urey.h"
index 4b90887..b5471a3 100644
--- "a/C:\\amber\\amber_master\\src\\pmemd\\src/cuda/kBWU_urey.h"
+++ "b/C:\\amber\\amber_mint\\src\\pmemd\\src/cuda/kBWU_urey.h"
@@ -32,9 +32,13 @@
 #endif
     unsigned int atmI = rawID >> 8;
     unsigned int atmJ = rawID & 0xff;
-    PMEDouble xij     = atmcrdx[atmI] - atmcrdx[atmJ];
-    PMEDouble yij     = atmcrdy[atmI] - atmcrdy[atmJ];
-    PMEDouble zij     = atmcrdz[atmI] - atmcrdz[atmJ];
+    // MINT32: Compute distance deltas as integer subtraction, then convert to real units
+    int dxij          = atmcrdx[atmI] - atmcrdx[atmJ];
+    int dyij          = atmcrdy[atmI] - atmcrdy[atmJ];
+    int dzij          = atmcrdz[atmI] - atmcrdz[atmJ];
+    PMEDouble xij     = (PMEDouble)((double)dxij * cSim.mint32_inv_scale.x);
+    PMEDouble yij     = (PMEDouble)((double)dyij * cSim.mint32_inv_scale.y);
+    PMEDouble zij     = (PMEDouble)((double)dzij * cSim.mint32_inv_scale.z);
     PMEDouble rij = sqrt(xij*xij + yij*yij + zij*zij);
     PMEDouble2 UBAngle = cSim.pBwuUrey[startidx + tgx];
     double da = rij - UBAngle.y;
diff --git "a/C:\\amber\\amber_master\\src\\pmemd\\src/cuda/kCCGE.h" "b/C:\\amber\\amber_mint\\src\\pmemd\\src/cuda/kCCGE.h"
index 4c3c569..bfaf942 100644
--- "a/C:\\amber\\amber_master\\src\\pmemd\\src/cuda/kCCGE.h"
+++ "b/C:\\amber\\amber_mint\\src\\pmemd\\src/cuda/kCCGE.h"
@@ -38,15 +38,16 @@
       PMEFloat fy =                           cSim.recipf[1][1]*crd_y + cSim.recipf[2][1]*crd_z;
       PMEFloat fz =                                                     cSim.recipf[2][2]*crd_z;
 #endif
-      fx = fx - round(fx);                     
+      // MINT32 requires coordinates in [-0.5, 0.5), not [0, 1)
+      fx = fx - round(fx);
       fy = fy - round(fy);
       fz = fz - round(fz);
-      if(fx < 0.0) fx+=1.0;
-      if(fy < 0.0) fy+=1.0;
-      if(fz < 0.0) fz+=1.0;
-      int gridx=fx*cSim.coarseMaxXvxl;
-      int gridy=fy*cSim.coarseMaxYvxl;
-      int gridz=fz*cSim.coarseMaxZvxl;
+      PMEFloat fxGrid = fx + (PMEFloat)0.5;
+      PMEFloat fyGrid = fy + (PMEFloat)0.5;
+      PMEFloat fzGrid = fz + (PMEFloat)0.5;
+      int gridx=fxGrid*cSim.coarseMaxXvxl;
+      int gridy=fyGrid*cSim.coarseMaxYvxl;
+      int gridz=fzGrid*cSim.coarseMaxZvxl;
       for(int x=-1*offset+gridx; x<=gridx+offset; x++)
       {
         int bkt_x= x%cSim.coarseMaxXvxl;
@@ -79,9 +80,6 @@
                 fxj = fxj - round(fxj);
                 fyj = fyj - round(fyj);
                 fzj = fzj - round(fzj);
-                fxj = (fxj < (PMEFloat)1.0 ? fxj : (PMEFloat)0.0);
-                fyj = (fyj < (PMEFloat)1.0 ? fyj : (PMEFloat)0.0);
-                fzj = (fzj < (PMEFloat)1.0 ? fzj : (PMEFloat)0.0);
                 PMEFloat xi=fx-fxj-round(fx-fxj);
                 PMEFloat yi=fy-fyj-round(fy-fyj);
                 PMEFloat zi=fz-fzj-round(fz-fzj);
@@ -164,15 +162,16 @@
       fy =                           cSim.recipf[1][1]*crd_y + cSim.recipf[2][1]*crd_z;
       fz =                                                     cSim.recipf[2][2]*crd_z;
 #endif
-      fx = fx - round(fx);                     
+      // MINT32 requires coordinates in [-0.5, 0.5), not [0, 1)
+      fx = fx - round(fx);
       fy = fy - round(fy);
       fz = fz - round(fz);
-      if(fx < 0.0) fx+=1.0;
-      if(fy < 0.0) fy+=1.0;
-      if(fz < 0.0) fz+=1.0;
-      gridx=fx*cSim.coarseMaxXvxl;
-      gridy=fy*cSim.coarseMaxYvxl;
-      gridz=fz*cSim.coarseMaxZvxl;
+      fxGrid = fx + (PMEFloat)0.5;
+      fyGrid = fy + (PMEFloat)0.5;
+      fzGrid = fz + (PMEFloat)0.5;
+      gridx=fxGrid*cSim.coarseMaxXvxl;
+      gridy=fyGrid*cSim.coarseMaxYvxl;
+      gridz=fzGrid*cSim.coarseMaxZvxl;
       // Cycle through nearby grid atoms 
       for(int x=-1*offset+gridx; x<=gridx+offset; x++)
       {
@@ -206,9 +205,6 @@
                 fxj = fxj - round(fxj);
                 fyj = fyj - round(fyj);
                 fzj = fzj - round(fzj);
-                fxj = (fxj < (PMEFloat)1.0 ? fxj : (PMEFloat)0.0);
-                fyj = (fyj < (PMEFloat)1.0 ? fyj : (PMEFloat)0.0);
-                fzj = (fzj < (PMEFloat)1.0 ? fzj : (PMEFloat)0.0);
                 PMEFloat xi=fx-fxj-round(fx-fxj);
                 PMEFloat yi=fy-fyj-round(fy-fyj);
                 PMEFloat zi=fz-fzj-round(fz-fzj);
diff --git "a/C:\\amber\\amber_master\\src\\pmemd\\src/cuda/kCQB.h" "b/C:\\amber\\amber_mint\\src\\pmemd\\src/cuda/kCQB.h"
index 57a7cc6..19fff31 100644
--- "a/C:\\amber\\amber_master\\src\\pmemd\\src/cuda/kCQB.h"
+++ "b/C:\\amber\\amber_mint\\src\\pmemd\\src/cuda/kCQB.h"
@@ -115,15 +115,19 @@
         PMEFloat charge = cSim.pAtomChargeSP[pos1];
 #endif
         PMEFloat fx = cSim.pFractX[pos1];
+        // Ensure fractional coordinate is in [0, nfft1)
+        if (fx < 0) fx += cSim.nfft1;
         crdq[              atomid] = fx;
         crdq[3*BATCHSIZE + atomid] = charge;
       }
       else if (segment == 1) {
         PMEFloat fy = cSim.pFractY[pos1];
+        if (fy < 0) fy += cSim.nfft2;
         crdq[  BATCHSIZE + atomid] = fy;
       }
       else if (segment == 2) {
         PMEFloat fz = cSim.pFractZ[pos1];
+        if (fz < 0) fz += cSim.nfft3;
         crdq[2*BATCHSIZE + atomid] = fz;
       }
     }
diff --git "a/C:\\amber\\amber_master\\src\\pmemd\\src/cuda/kCalculateCoarseGridEnergy.cu" "b/C:\\amber\\amber_mint\\src\\pmemd\\src/cuda/kCalculateCoarseGridEnergy.cu"
index 978f029..612b32f 100644
--- "a/C:\\amber\\amber_master\\src\\pmemd\\src/cuda/kCalculateCoarseGridEnergy.cu"
+++ "b/C:\\amber\\amber_mint\\src\\pmemd\\src/cuda/kCalculateCoarseGridEnergy.cu"
@@ -431,15 +431,16 @@ kMakeStericGrid_kernel()
       PMEFloat fx = cSim.recipf[0][0]*crd_x + cSim.recipf[1][0]*crd_y + cSim.recipf[2][0]*crd_z;
       PMEFloat fy =                           cSim.recipf[1][1]*crd_y + cSim.recipf[2][1]*crd_z;
       PMEFloat fz =                                                     cSim.recipf[2][2]*crd_z;
+      // MINT32 requires coordinates in [-0.5, 0.5), not [0, 1)
       fx = fx - round(fx);
       fy = fy - round(fy);
       fz = fz - round(fz);
-      if(fx < 0.0) fx+=1.0;
-      if(fy < 0.0) fy+=1.0;
-      if(fz < 0.0) fz+=1.0;
-      int vx = fx * cSim.stericMaxXVxl;
-      int vy = fy * cSim.stericMaxYVxl;
-      int vz = fz * cSim.stericMaxZVxl;
+      PMEFloat fxGrid = fx + (PMEFloat)0.5;
+      PMEFloat fyGrid = fy + (PMEFloat)0.5;
+      PMEFloat fzGrid = fz + (PMEFloat)0.5;
+      int vx = fxGrid * cSim.stericMaxXVxl;
+      int vy = fyGrid * cSim.stericMaxYVxl;
+      int vz = fzGrid * cSim.stericMaxZVxl;
       PMEFloat zradius = radius/cSim.stericGridSpacing;
       int zmin=max(0,int(floor(vz-zradius))+1);
       int zmax=min(int(vz+zradius),cSim.stericMaxZVxl);
diff --git "a/C:\\amber\\amber_master\\src\\pmemd\\src/cuda/kCalculateGBBornRadii.h" "b/C:\\amber\\amber_mint\\src\\pmemd\\src/cuda/kCalculateGBBornRadii.h"
index d97c883..14a9bae 100644
--- "a/C:\\amber\\amber_master\\src\\pmemd\\src/cuda/kCalculateGBBornRadii.h"
+++ "b/C:\\amber\\amber_mint\\src\\pmemd\\src/cuda/kCalculateGBBornRadii.h"
@@ -85,8 +85,10 @@ volatile __shared__ unsigned int sNext[GRID];
     x                = (x >> 17) << GRID_BITS;
     unsigned int tgx = threadIdx.x & (GRID - 1);
     unsigned int i   = x + tgx;
-    PMEFloat2 xyi    = cSim.pAtomXYSP[i];
-    PMEFloat zi      = cSim.pAtomZSP[i];
+    int4 coordi = cSim.pAtomCoord_M32[i];
+    PMEFloat xyi_x = (PMEFloat)coordi.x * cSim.mint32_inv_scale.x;
+    PMEFloat xyi_y = (PMEFloat)coordi.y * cSim.mint32_inv_scale.y;
+    PMEFloat zi    = (PMEFloat)coordi.z * cSim.mint32_inv_scale.z;
     PMEFloat ri      = cSim.pAtomRBorn[i];
     PMEFloat si      = cSim.pAtomS[i];
     PMEFloat si2     = si * si;
@@ -109,8 +111,8 @@ volatile __shared__ unsigned int sNext[GRID];
     // Handle diagonals uniquely at 50% efficiency, skipping i == j interactions
     // x and y are always consistent within a warp, no diverge here
     if (x == y) {
-      PMEFloat xi = xyi.x;
-      PMEFloat yi = xyi.y;
+      PMEFloat xi = xyi_x;
+      PMEFloat yi = xyi_y;
       PSATOMX(tgx) = xi;
       PSATOMY(tgx) = yi;
       PSATOMZ(tgx) = zi;
@@ -301,16 +303,18 @@ volatile __shared__ unsigned int sNext[GRID];
     }
     else {
       unsigned int j  = y + tgx;
-      PMEFloat2 xyj   = cSim.pAtomXYSP[j];
+      int4 coordj = cSim.pAtomCoord_M32[j];
+      shAtom.x = (PMEFloat)coordj.x * cSim.mint32_inv_scale.x;
+      shAtom.y = (PMEFloat)coordj.y * cSim.mint32_inv_scale.y;
+      shAtom.z = (PMEFloat)coordj.z * cSim.mint32_inv_scale.z;
       PSATOMS(tgx)    = cSim.pAtomS[j];
       PSATOMR(tgx)    = cSim.pAtomRBorn[j];
-      PSATOMZ(tgx)    = cSim.pAtomZSP[j];
-      PMEFloat xi     = xyi.x;
-      PMEFloat yi     = xyi.y;
+
+      PMEFloat xi     = xyi_x;
+      PMEFloat yi     = xyi_y;
       ri             -= cSim.offset;
       PMEFloat ri1i   = (PMEFloat)1.0 / ri;
-      PSATOMX(tgx)    = xyj.x;
-      PSATOMY(tgx)    = xyj.y;
+
       PSATOMS2(tgx)   = PSATOMS(tgx) * PSATOMS(tgx);
       PSATOMR(tgx)   -= cSim.offset;
       PSATOMR1I(tgx)  = (PMEFloat)1.0 / PSATOMR(tgx);
diff --git "a/C:\\amber\\amber_master\\src\\pmemd\\src/cuda/kCalculateGBNonbondEnergy1.h" "b/C:\\amber\\amber_mint\\src\\pmemd\\src/cuda/kCalculateGBNonbondEnergy1.h"
index 2b92e74..b3970b1 100644
--- "a/C:\\amber\\amber_master\\src\\pmemd\\src/cuda/kCalculateGBNonbondEnergy1.h"
+++ "b/C:\\amber\\amber_mint\\src\\pmemd\\src/cuda/kCalculateGBNonbondEnergy1.h"
@@ -11,6 +11,9 @@
     PMEFloat x;
     PMEFloat y;
     PMEFloat z;
+                int xi32;
+                int yi32;
+                int zi32;
     PMEFloat q;
     unsigned int LJID;
     PMEFloat r;
@@ -34,6 +37,9 @@
 #define PSATOMX(i)    shAtom.x
 #define PSATOMY(i)    shAtom.y
 #define PSATOMZ(i)    shAtom.z
+#define PSATOMXI32(i) shAtom.xi32
+#define PSATOMYI32(i) shAtom.yi32
+#define PSATOMZI32(i) shAtom.zi32
 #define PSATOMQ(i)    shAtom.q
 #define PSATOMLJID(i) shAtom.LJID
 #define PSATOMR(i)    shAtom.r
@@ -116,8 +122,13 @@
     x                = (x >> 17) << GRID_BITS;
     unsigned int tgx = threadIdx.x & (GRID - 1);
     unsigned int i   = x + tgx;
-    PMEFloat2 xyi    = cSim.pAtomXYSP[i];
-    PMEFloat zi      = cSim.pAtomZSP[i];
+        int4 coordi = cSim.pAtomCoord_M32[i];
+        int xi_i32 = coordi.x;
+        int yi_i32 = coordi.y;
+        int zi_i32 = coordi.z;
+        PMEFloat xyi_x = (PMEFloat)xi_i32 * cSim.mint32_inv_scale.x;
+        PMEFloat xyi_y = (PMEFloat)yi_i32 * cSim.mint32_inv_scale.y;
+        PMEFloat zi    = (PMEFloat)zi_i32 * cSim.mint32_inv_scale.z;
     PMEFloat2 qljid  = cSim.pAtomChargeSPLJID[i];
 #ifdef GB_GBSA3
     PMEFloat signpi  = cSim.pgbsa_sigma[i]; //pwsasa sigma
@@ -163,8 +174,8 @@
     unsigned int shIdx = sNext[tgx];
 #endif
     if (x == y) {
-      PMEFloat xi        = xyi.x;
-      PMEFloat yi        = xyi.y;
+      PMEFloat xi        = xyi_x;
+      PMEFloat yi        = xyi_y;
       PSATOMX(tgx)       = xi;
       PSATOMY(tgx)       = yi;
 #ifdef use_DPFP
@@ -173,6 +184,9 @@
       unsigned int LJIDi = __float_as_uint(qljid.y) * cSim.LJTypes;
 #endif
       PSATOMZ(tgx)       = zi;
+        PSATOMXI32(tgx)    = xi_i32;
+        PSATOMYI32(tgx)    = yi_i32;
+        PSATOMZI32(tgx)    = zi_i32;
       PSATOMQ(tgx)       = qi;
 #ifdef use_DPFP
       PSATOMLJID(tgx)    = __double_as_longlong(qljid.y);
@@ -192,6 +206,9 @@
       shAtom.x = WarpRotateLeft<GRID>(shAtom.x);
       shAtom.y = WarpRotateLeft<GRID>(shAtom.y);
       shAtom.z = WarpRotateLeft<GRID>(shAtom.z);
+        shAtom.xi32 = WarpRotateLeft<GRID>(shAtom.xi32);
+        shAtom.yi32 = WarpRotateLeft<GRID>(shAtom.yi32);
+        shAtom.zi32 = WarpRotateLeft<GRID>(shAtom.zi32);
       shAtom.q = WarpRotateLeft<GRID>(shAtom.q);
       shAtom.r = WarpRotateLeft<GRID>(shAtom.r);
       shAtom.LJID = WarpRotateLeft<GRID>(shAtom.LJID);
@@ -205,6 +222,9 @@
       shAtom.x    = __SHFL(WARP_MASK, shAtom.x, shIdx);
       shAtom.y    = __SHFL(WARP_MASK, shAtom.y, shIdx);
       shAtom.z    = __SHFL(WARP_MASK, shAtom.z, shIdx);
+        shAtom.xi32 = __SHFL(WARP_MASK, shAtom.xi32, shIdx);
+        shAtom.yi32 = __SHFL(WARP_MASK, shAtom.yi32, shIdx);
+        shAtom.zi32 = __SHFL(WARP_MASK, shAtom.zi32, shIdx);
       shAtom.q    = __SHFL(WARP_MASK, shAtom.q, shIdx);
       shAtom.r    = __SHFL(WARP_MASK, shAtom.r, shIdx);
       shAtom.LJID = __SHFL(WARP_MASK, shAtom.LJID, shIdx);
@@ -217,10 +237,13 @@
 #endif
       PMEMask mask1 = __BALLOT(WARP_MASK, j != tgx);
       while (j != tgx) {
-        PMEFloat xij  = xi - PSATOMX(j);
-        PMEFloat yij  = yi - PSATOMY(j);
-        PMEFloat zij  = zi - PSATOMZ(j);
-        PMEFloat r2   = xij*xij + yij*yij + zij*zij;
+        int xj_i32 = PSATOMXI32(j);
+        int yj_i32 = PSATOMYI32(j);
+        int zj_i32 = PSATOMZI32(j);
+        PMEFloat dx  = (PMEFloat)((double)(xi_i32 - xj_i32) * cSim.mint32_inv_scale.x);
+        PMEFloat dy  = (PMEFloat)((double)(yi_i32 - yj_i32) * cSim.mint32_inv_scale.y);
+        PMEFloat dz  = (PMEFloat)((double)(zi_i32 - zj_i32) * cSim.mint32_inv_scale.z);
+        PMEFloat r2   = dx*dx + dy*dy + dz*dz;
         PMEFloat qiqj = qi * PSATOMQ(j);
         PMEFloat v5   = rsqrt(r2);
 #ifndef GB_IGB6
@@ -547,9 +570,9 @@
                                                (PMEFloat)(0.5 / 6.0)*f6));
 #  endif
         }
-        PMEFloat dedx = de * xij;
-        PMEFloat dedy = de * yij;
-        PMEFloat dedz = de * zij;
+        PMEFloat dedx = de * dx;
+        PMEFloat dedy = de * dy;
+        PMEFloat dedz = de * dz;
 #ifdef AMBER_PLATFORM_AMD
         fx_i += dedx;
         fy_i += dedy;
@@ -573,9 +596,9 @@
 #    endif
 #  endif
         }
-        PMEFloat dedx = de * xij;
-        PMEFloat dedy = de * yij;
-        PMEFloat dedz = de * zij;
+        PMEFloat dedx = de * dx;
+        PMEFloat dedy = de * dy;
+        PMEFloat dedz = de * dz;
         fx_i += (PMEDouble)dedx;
         fy_i += (PMEDouble)dedy;
         fz_i += (PMEDouble)dedz;
@@ -585,6 +608,9 @@
         shAtom.x = WarpRotateLeft<GRID>(shAtom.x);
         shAtom.y = WarpRotateLeft<GRID>(shAtom.y);
         shAtom.z = WarpRotateLeft<GRID>(shAtom.z);
+        shAtom.xi32 = WarpRotateLeft<GRID>(shAtom.xi32);
+        shAtom.yi32 = WarpRotateLeft<GRID>(shAtom.yi32);
+        shAtom.zi32 = WarpRotateLeft<GRID>(shAtom.zi32);
         shAtom.q = WarpRotateLeft<GRID>(shAtom.q);
         shAtom.r = WarpRotateLeft<GRID>(shAtom.r);
         shAtom.LJID = WarpRotateLeft<GRID>(shAtom.LJID);
@@ -598,6 +624,9 @@
         shAtom.x    = __SHFL(mask1, shAtom.x, shIdx);
         shAtom.y    = __SHFL(mask1, shAtom.y, shIdx);
         shAtom.z    = __SHFL(mask1, shAtom.z, shIdx);
+        shAtom.xi32 = __SHFL(mask1, shAtom.xi32, shIdx);
+        shAtom.yi32 = __SHFL(mask1, shAtom.yi32, shIdx);
+        shAtom.zi32 = __SHFL(mask1, shAtom.zi32, shIdx);
         shAtom.q    = __SHFL(mask1, shAtom.q, shIdx);
         shAtom.r    = __SHFL(mask1, shAtom.r, shIdx);
         shAtom.LJID = __SHFL(mask1, shAtom.LJID, shIdx);
@@ -655,11 +684,16 @@
 #  endif
 #endif
     }
-    else {
-      unsigned int j   = y + tgx;
-      PMEFloat2 xyj    = cSim.pAtomXYSP[j];
+                else {
+                        unsigned int j   = y + tgx;
+                        int4 coordj = cSim.pAtomCoord_M32[j];
+                        shAtom.x = (PMEFloat)coordj.x * cSim.mint32_inv_scale.x;
+                        shAtom.y = (PMEFloat)coordj.y * cSim.mint32_inv_scale.y;
+                        shAtom.z = (PMEFloat)coordj.z * cSim.mint32_inv_scale.z;
+                        PSATOMXI32(tgx) = coordj.x;
+                        PSATOMYI32(tgx) = coordj.y;
+                        PSATOMZI32(tgx) = coordj.z;
       PMEFloat2 qljidj = cSim.pAtomChargeSPLJID[j];
-      PSATOMZ(tgx)     = cSim.pAtomZSP[j];
       PSATOMR(tgx)     = cSim.pReffSP[j];
       PSATOMQ(tgx)     = qljidj.x;
 #ifdef GB_GBSA3
@@ -672,16 +706,15 @@
 #else
       PSATOMLJID(tgx)  = __float_as_uint(qljidj.y);
 #endif
-      PMEFloat xi        = xyi.x;
-      PMEFloat yi        = xyi.y;
+      PMEFloat xi        = xyi_x;
+      PMEFloat yi        = xyi_y;
       PMEFloat qi        = qljid.x;
 #ifdef use_DPFP
       unsigned int LJIDi = __double_as_longlong(qljid.y) * cSim.LJTypes;
 #else
       unsigned int LJIDi = __float_as_uint(qljid.y) * cSim.LJTypes;
 #endif
-      PSATOMX(tgx)       = xyj.x;
-      PSATOMY(tgx)       = xyj.y;
+
       j = tgx;
       PMEMask mask1 = WARP_MASK;
 #ifdef AMBER_PLATFORM_AMD
@@ -689,10 +722,13 @@
 #else
       do {
 #endif
-        PMEFloat xij  = xi - PSATOMX(j);
-        PMEFloat yij  = yi - PSATOMY(j);
-        PMEFloat zij  = zi - PSATOMZ(j);
-        PMEFloat r2   = xij * xij + yij * yij + zij * zij;
+        int xj_i32 = PSATOMXI32(j);
+        int yj_i32 = PSATOMYI32(j);
+        int zj_i32 = PSATOMZI32(j);
+        PMEFloat dx  = (PMEFloat)((double)(xi_i32 - xj_i32) * cSim.mint32_inv_scale.x);
+        PMEFloat dy  = (PMEFloat)((double)(yi_i32 - yj_i32) * cSim.mint32_inv_scale.y);
+        PMEFloat dz  = (PMEFloat)((double)(zi_i32 - zj_i32) * cSim.mint32_inv_scale.z);
+        PMEFloat r2   = dx * dx + dy * dy + dz * dz;
         PMEFloat qiqj = qi * PSATOMQ(j);
         PMEFloat v5   = rsqrt(r2);
 #ifndef GB_IGB6
@@ -1022,13 +1058,13 @@
 #  endif
         }
 #ifdef AMBER_PLATFORM_AMD
-        PMEFloat dedx = de * xij;
-        PMEFloat dedy = de * yij;
-        PMEFloat dedz = de * zij;
+        PMEFloat dedx = de * dx;
+        PMEFloat dedy = de * dy;
+        PMEFloat dedz = de * dz;
 #else
-        long long int dedx = fast_llrintf(de * xij);
-        long long int dedy = fast_llrintf(de * yij);
-        long long int dedz = fast_llrintf(de * zij);
+        long long int dedx = fast_llrintf(de * dx);
+        long long int dedy = fast_llrintf(de * dy);
+        long long int dedz = fast_llrintf(de * dz);
 #endif
         fx_i += dedx;
         fy_i += dedy;
@@ -1050,9 +1086,9 @@
 #  endif
           de += ((f12 - f6) + eel) * r2inv;
         }
-        PMEForce dedx = (PMEForce)(de * xij);
-        PMEForce dedy = (PMEForce)(de * yij);
-        PMEForce dedz = (PMEForce)(de * zij);
+        PMEForce dedx = (PMEForce)(de * dx);
+        PMEForce dedy = (PMEForce)(de * dy);
+        PMEForce dedz = (PMEForce)(de * dz);
         fx_i += dedx;
         fy_i += dedy;
         fz_i += dedz;
@@ -1067,6 +1103,9 @@
         shAtom.x = WarpRotateLeft<GRID>(shAtom.x);
         shAtom.y = WarpRotateLeft<GRID>(shAtom.y);
         shAtom.z = WarpRotateLeft<GRID>(shAtom.z);
+        shAtom.xi32 = WarpRotateLeft<GRID>(shAtom.xi32);
+        shAtom.yi32 = WarpRotateLeft<GRID>(shAtom.yi32);
+        shAtom.zi32 = WarpRotateLeft<GRID>(shAtom.zi32);
         shAtom.q = WarpRotateLeft<GRID>(shAtom.q);
         shAtom.r = WarpRotateLeft<GRID>(shAtom.r);
         shAtom.LJID = WarpRotateLeft<GRID>(shAtom.LJID);
@@ -1089,6 +1128,9 @@
         shAtom.x    = __SHFL(mask1, shAtom.x, shIdx);
         shAtom.y    = __SHFL(mask1, shAtom.y, shIdx);
         shAtom.z    = __SHFL(mask1, shAtom.z, shIdx);
+        shAtom.xi32 = __SHFL(mask1, shAtom.xi32, shIdx);
+        shAtom.yi32 = __SHFL(mask1, shAtom.yi32, shIdx);
+        shAtom.zi32 = __SHFL(mask1, shAtom.zi32, shIdx);
         shAtom.q    = __SHFL(mask1, shAtom.q, shIdx);
         shAtom.r    = __SHFL(mask1, shAtom.r, shIdx);
         shAtom.LJID = __SHFL(mask1, shAtom.LJID, shIdx);
diff --git "a/C:\\amber\\amber_master\\src\\pmemd\\src/cuda/kCalculateGBNonbondEnergy2.h" "b/C:\\amber\\amber_mint\\src\\pmemd\\src/cuda/kCalculateGBNonbondEnergy2.h"
index 5af23c7..9dc76f0 100644
--- "a/C:\\amber\\amber_master\\src\\pmemd\\src/cuda/kCalculateGBNonbondEnergy2.h"
+++ "b/C:\\amber\\amber_mint\\src\\pmemd\\src/cuda/kCalculateGBNonbondEnergy2.h"
@@ -93,8 +93,10 @@
     x                = (x >> 17) << GRID_BITS;
     unsigned int tgx = threadIdx.x & (GRID - 1);
     unsigned int i   = x + tgx;
-    PMEFloat2 xyi    = cSim.pAtomXYSP[i];
-    PMEFloat zi      = cSim.pAtomZSP[i];
+    int4 coordi = cSim.pAtomCoord_M32[i];
+    PMEFloat xyi_x = (PMEFloat)coordi.x * cSim.mint32_inv_scale.x;
+    PMEFloat xyi_y = (PMEFloat)coordi.y * cSim.mint32_inv_scale.y;
+    PMEFloat zi    = (PMEFloat)coordi.z * cSim.mint32_inv_scale.z;
     PMEFloat ri      = cSim.pAtomRBorn[i];
     PMEFloat si      = cSim.pAtomS[i];
     PMEFloat temp7_i = cSim.pTemp7[i];
@@ -125,8 +127,8 @@
 
     // Handle diagonals uniquely at 50% efficiency, skipping i == j interactions
     if (x == y) {
-      PMEFloat xi  = xyi.x;
-      PMEFloat yi  = xyi.y;
+      PMEFloat xi  = xyi_x;
+      PMEFloat yi  = xyi_y;
       PSATOMX(tgx) = xi;
       PSATOMY(tgx) = yi;
       PSATOMZ(tgx) = zi;
@@ -356,18 +358,19 @@
     }
     else {
       unsigned int j   = y + tgx;
-      PMEFloat2 xyj    = cSim.pAtomXYSP[j];
-      PSATOMZ(tgx)     = cSim.pAtomZSP[j];
+      int4 coordj = cSim.pAtomCoord_M32[j];
+      shAtom.x = (PMEFloat)coordj.x * cSim.mint32_inv_scale.x;
+      shAtom.y = (PMEFloat)coordj.y * cSim.mint32_inv_scale.y;
+      shAtom.z = (PMEFloat)coordj.z * cSim.mint32_inv_scale.z;
       PSATOMR(tgx)     = cSim.pAtomRBorn[j];
       PSATOMS(tgx)     = cSim.pAtomS[j];
       PSATOMTEMP7(tgx) = cSim.pTemp7[j];
-      PMEFloat xi = xyi.x;
-      PMEFloat yi = xyi.y;
+      PMEFloat xi = xyi_x;
+      PMEFloat yi = xyi_y;
       ri             -= cSim.offset;
       PMEFloat ri1i   = (PMEFloat)1.0 / ri;
       PMEFloat si2    = si * si;
-      PSATOMX(tgx)    = xyj.x;
-      PSATOMY(tgx)    = xyj.y;
+
       PSATOMR(tgx)   -= cSim.offset;
       PSATOMR1I(tgx)  = (PMEFloat)1.0 / PSATOMR(tgx);
       j               = tgx;
diff --git "a/C:\\amber\\amber_master\\src\\pmemd\\src/cuda/kCalculatePMENonbondEnergy.cu" "b/C:\\amber\\amber_mint\\src\\pmemd\\src/cuda/kCalculatePMENonbondEnergy.cu"
index 3d695c4..0354a37 100644
--- "a/C:\\amber\\amber_master\\src\\pmemd\\src/cuda/kCalculatePMENonbondEnergy.cu"
+++ "b/C:\\amber\\amber_mint\\src\\pmemd\\src/cuda/kCalculatePMENonbondEnergy.cu"
@@ -1,4 +1,5 @@
 #include "copyright.i"
+#include "mint32_debug.h"
 
 //---------------------------------------------------------------------------------------------
 // AMBER NVIDIA CUDA GPU IMPLEMENTATION: PMEMD VERSION
@@ -10,6 +11,7 @@
 #ifndef AMBER_PLATFORM_AMD
 #include <cuda.h>
 #endif
+#include <cuda_runtime.h>
 #define GPU_CPP
 #include "gputypes.h"
 #include "gpu.h"
@@ -23,6 +25,208 @@
 // Use global instance instead of a local copy
 #include "simulationConst.h"
 CSIM_STO simulationConst cSim;
+#include <cstdlib>
+#include <cstdio>
+
+#define MINT32_SAMPLE_MAX 16
+#ifndef MINT32_PAIR_SAMPLE_DEFINED
+#define MINT32_PAIR_SAMPLE_DEFINED
+struct Mint32PairSample {
+  int block;
+  int warp;
+  int i;       // LJ term index
+  int j;       // LJ term index partner
+  int atom_i;  // original atom indices
+  int atom_j;
+  float dx, dy, dz;
+  float r;
+  float r2;
+  float qi;
+  float qj;
+  int lj_i;
+  int lj_j;
+  float termx;
+  float termy;
+  float qiqj;
+  float r2inv;
+  float coef0, coef1, coef2, coef3;
+  float ecoul;
+  float evdw;
+  float frc;
+  float swtch;
+  float dswtch;
+  int   exclusion;
+};
+#endif
+
+__device__ Mint32PairSample dMint32PairSamples[MINT32_SAMPLE_MAX];
+__device__ int dMint32PairSampleCount = 0;
+__device__ int dMint32SampleEnable = 0;
+
+extern "C" void Mint32SampleInit(int enable)
+{
+  int zero = 0;
+  cudaMemcpyToSymbol(dMint32SampleEnable, &enable, sizeof(int));
+  cudaMemcpyToSymbol(dMint32PairSampleCount, &zero, sizeof(int));
+}
+
+extern "C" void Mint32SampleDump()
+{
+  int enable = 0;
+  int count = 0;
+  cudaMemcpyFromSymbol(&enable, dMint32SampleEnable, sizeof(int));
+  cudaMemcpyFromSymbol(&count, dMint32PairSampleCount, sizeof(int));
+  char buf[512];
+  snprintf(buf, sizeof(buf), "MINT32_SAMPLE_PAIRS: enable=%d count=%d", enable, count);
+  MINT32_LOG(buf);
+  if (enable <= 0) {
+    return;
+  }
+  // If the kernel never set a count, still dump slot 0 so we can see whether
+  // anything wrote into the buffer.
+  int dumpCount = (count == 0) ? 1 : count;
+  if (dumpCount > MINT32_SAMPLE_MAX) dumpCount = MINT32_SAMPLE_MAX;
+  Mint32PairSample hostBuf[MINT32_SAMPLE_MAX];
+  cudaMemcpyFromSymbol(hostBuf, dMint32PairSamples, sizeof(Mint32PairSample) * dumpCount);
+  for (int i = 0; i < dumpCount; i++) {
+    const Mint32PairSample& s = hostBuf[i];
+    snprintf(buf, sizeof(buf),
+             " pair[%d]: block=%d warp=%d i=%d j=%d atom_i=%d atom_j=%d r=%.6f r2=%.6f "
+             "dx=%.6f dy=%.6f dz=%.6f qi=%.6f qj=%.6f lj=(%d,%d) term=(%.6f,%.6f) qiqj=%.6f r2inv=%.6f coef=(%.6f,%.6f,%.6f,%.6f) swtch=%.6f dswtch=%.6f excl=%d ecoul=%.6f evdw=%.6f frc=%.6f",
+             i, s.block, s.warp, s.i, s.j, s.atom_i, s.atom_j, s.r, s.r2,
+             s.dx, s.dy, s.dz, s.qi, s.qj, s.lj_i, s.lj_j,
+             s.termx, s.termy, s.qiqj, s.r2inv,
+             s.coef0, s.coef1, s.coef2, s.coef3,
+             s.swtch, s.dswtch, s.exclusion,
+             s.ecoul, s.evdw, s.frc);
+    MINT32_LOG(buf);
+  }
+}
+
+#ifndef MINT32_NL_CHECKSUM_DEFINED
+#define MINT32_NL_CHECKSUM_DEFINED
+struct Mint32NLChecksum {
+  unsigned long long pairCount;
+  unsigned long long pairCountNoExcl;
+  unsigned long long totalPairs;
+  unsigned long long validPairs;
+  unsigned long long recordCount;
+  unsigned long long xatomTotal;
+  double sumQiqjOverR;
+  double sumQiqjOverRNoExcl;
+  double sumR2;
+};
+#endif
+
+#ifndef MINT32_NL_CHECKSUM_ENABLED
+#define MINT32_NL_CHECKSUM_ENABLED 0
+#endif
+
+#if MINT32_NL_CHECKSUM_ENABLED
+__device__ Mint32NLChecksum dMint32NLChecksum;
+__device__ int dMint32NLChecksumEnable = 0;
+// Debug buffer written from kBNL to report a single sample of bounding-box state
+__device__ int dMint32BNLDebug[4];
+
+extern "C" void Mint32NLChecksumInit(int enable)
+{
+  Mint32NLChecksum zero = {};
+  cudaMemcpyToSymbol(dMint32NLChecksum, &zero, sizeof(Mint32NLChecksum));
+  cudaMemcpyToSymbol(dMint32NLChecksumEnable, &enable, sizeof(int));
+}
+
+extern "C" void Mint32NLChecksumDump()
+{
+  int enable = 0;
+  cudaMemcpyFromSymbol(&enable, dMint32NLChecksumEnable, sizeof(int));
+  char buf[256];
+  snprintf(buf, sizeof(buf), "MINT32_NL_CHECKSUM enable=%d", enable);
+  MINT32_LOG(buf);
+  if (enable == 0) {
+    return;
+  }
+  Mint32NLChecksum host = {};
+  cudaMemcpyFromSymbol(&host, dMint32NLChecksum, sizeof(Mint32NLChecksum));
+  // Optional: read BNL debug buffer
+  int bnlDbg[4] = {0,0,0,0};
+  cudaMemcpyFromSymbol(bnlDbg, dMint32BNLDebug, sizeof(bnlDbg));
+  snprintf(buf, sizeof(buf),
+           "MINT32_NL_CHECKSUM: pairs=%llu pairs_no_excl=%llu total_pairs=%llu valid_pairs=%llu records=%llu xatoms=%llu sum_qiqj_over_r=%.6e sum_qiqj_over_r_no_excl=%.6e sum_r2=%.6e bnl_dbg_set=%d xmax=%d span=%d cpos=%d",
+           (unsigned long long)host.pairCount,
+           (unsigned long long)host.pairCountNoExcl,
+           (unsigned long long)host.totalPairs,
+           (unsigned long long)host.validPairs,
+           (unsigned long long)host.recordCount,
+           (unsigned long long)host.xatomTotal,
+           host.sumQiqjOverR, host.sumQiqjOverRNoExcl, host.sumR2,
+           bnlDbg[0], bnlDbg[1], bnlDbg[2], bnlDbg[3]);
+  MINT32_LOG(buf);
+}
+
+__device__ __forceinline__ double Mint32AtomicAdd(double* addr, double val)
+{
+#if __CUDA_ARCH__ >= 600
+  return atomicAdd(addr, val);
+#else
+  unsigned long long int* address_as_ull =
+    reinterpret_cast<unsigned long long int*>(addr);
+  unsigned long long int old = *address_as_ull, assumed;
+  do {
+    assumed = old;
+    old = atomicCAS(address_as_ull, assumed,
+                    __double_as_longlong(__longlong_as_double(assumed) + val));
+  } while (assumed != old);
+  return __longlong_as_double(old);
+#endif
+}
+
+__device__ __forceinline__ void Mint32ChecksumAccumulate(PMEFloat qiqj, PMEFloat r2,
+                                                         unsigned int exclusion)
+{
+  if (dMint32NLChecksumEnable) {
+    double rinv = 1.0 / sqrt((double)r2);
+    atomicAdd(&dMint32NLChecksum.pairCount, 1ULL);
+    Mint32AtomicAdd(&dMint32NLChecksum.sumQiqjOverR, (double)qiqj * rinv);
+    Mint32AtomicAdd(&dMint32NLChecksum.sumR2, (double)r2);
+    if ((exclusion & 0x1u) == 0u) {
+      atomicAdd(&dMint32NLChecksum.pairCountNoExcl, 1ULL);
+      Mint32AtomicAdd(&dMint32NLChecksum.sumQiqjOverRNoExcl, (double)qiqj * rinv);
+    }
+  }
+}
+
+__device__ __forceinline__ void Mint32ChecksumAttempt(bool valid_i, bool valid_j)
+{
+  if (dMint32NLChecksumEnable) {
+    atomicAdd(&dMint32NLChecksum.totalPairs, 1ULL);
+    if (valid_i && valid_j) {
+      atomicAdd(&dMint32NLChecksum.validPairs, 1ULL);
+    }
+  }
+}
+
+__device__ __forceinline__ void Mint32ChecksumRecordEntry(int lane, int xatoms)
+{
+  if (dMint32NLChecksumEnable && lane == 0) {
+    atomicAdd(&dMint32NLChecksum.recordCount, 1ULL);
+    atomicAdd(&dMint32NLChecksum.xatomTotal, (unsigned long long)xatoms);
+  }
+}
+#else
+// Compile-time opt-out: strip checksum instrumentation from hot paths unless
+// MINT32_NL_CHECKSUM_ENABLED is set at build time.
+__device__ Mint32NLChecksum dMint32NLChecksum;
+__device__ int dMint32NLChecksumEnable = 0;
+__device__ int dMint32BNLDebug[4] = {0, 0, 0, 0};
+
+extern "C" void Mint32NLChecksumInit(int) {}
+extern "C" void Mint32NLChecksumDump() {}
+
+__device__ __forceinline__ double Mint32AtomicAdd(double*, double) { return 0.0; }
+__device__ __forceinline__ void Mint32ChecksumAccumulate(PMEFloat, PMEFloat, unsigned int) {}
+__device__ __forceinline__ void Mint32ChecksumAttempt(bool, bool) {}
+__device__ __forceinline__ void Mint32ChecksumRecordEntry(int, int) {}
+#endif
 
 #if !defined(__HIPCC_RDC__)
 
@@ -1760,6 +1964,7 @@ kCalcPMEOrthoNBMiniNrg8_kernel()
 //---------------------------------------------------------------------------------------------
 extern "C" void kCalculatePMENonbondForces(gpuContext gpu)
 {
+  MINT32_LOG("ENTRY kCalculatePMENonbondForces");
   // Set local variables to the launch bounds that we need, to make the massive
   // case switch below easier to parse.  There are subtle differences between
   // the launch bounds that can get lost in the lengthy names--this will help
@@ -1767,6 +1972,18 @@ extern "C" void kCalculatePMENonbondForces(gpuContext gpu)
   int nbBlocks = gpu->PMENonbondBlocks;
   int nrgThreads = gpu->PMENonbondEnergyThreadsPerBlock;
   int frcThreads = gpu->PMENonbondForcesThreadsPerBlock;
+  int mint32ChecksumEnable = 0;
+  const char* checksumEnv = getenv("MINT32_NL_CHECKSUM");
+  {
+    char buf[160];
+    snprintf(buf, sizeof(buf), "MINT32_NL_CHECKSUM env=%s", checksumEnv ? checksumEnv : "unset");
+    MINT32_LOG(buf);
+  }
+  if (checksumEnv && atoi(checksumEnv) > 0) {
+    mint32ChecksumEnable = atoi(checksumEnv);
+    Mint32NLChecksumInit(mint32ChecksumEnable);
+    MINT32_LOG("MINT32_NL_CHECKSUM enabled");
+  }
 
   // Decide which non-bonded direct space force kernel to use.
   if (gpu->sim.fswitch < 0) {
@@ -2177,6 +2394,10 @@ extern "C" void kCalculatePMENonbondForces(gpuContext gpu)
       }
     }
   }
+  MINT32_LOG("EXIT kCalculatePMENonbondEnergy (before LAUNCHERROR)");
+  if (mint32ChecksumEnable > 0) {
+    Mint32NLChecksumDump();
+  }
   LAUNCHERROR("kCalculatePMENonbondForces");
 }
 
@@ -2189,6 +2410,31 @@ extern "C" void kCalculatePMENonbondForces(gpuContext gpu)
 //---------------------------------------------------------------------------------------------
 extern "C" void kCalculatePMENonbondEnergy(gpuContext gpu)
 {
+  MINT32_LOG("ENTRY kCalculatePMENonbondEnergy");
+  int mint32SampleEnable = 0;
+  const char* sampleEnv = getenv("MINT32_SAMPLE_PAIRS");
+  {
+    char buf[128];
+    snprintf(buf, sizeof(buf), "MINT32_SAMPLE_PAIRS env=%s", sampleEnv ? sampleEnv : "unset");
+    MINT32_LOG(buf);
+  }
+  if (sampleEnv && atoi(sampleEnv) > 0) {
+    mint32SampleEnable = atoi(sampleEnv);
+    Mint32SampleInit(mint32SampleEnable);
+    MINT32_LOG("MINT32_SAMPLE_PAIRS enabled");
+  }
+  int mint32ChecksumEnable = 0;
+  const char* checksumEnv = getenv("MINT32_NL_CHECKSUM");
+  {
+    char buf[160];
+    snprintf(buf, sizeof(buf), "MINT32_NL_CHECKSUM env=%s", checksumEnv ? checksumEnv : "unset");
+    MINT32_LOG(buf);
+  }
+  if (checksumEnv && atoi(checksumEnv) > 0) {
+    mint32ChecksumEnable = atoi(checksumEnv);
+    Mint32NLChecksumInit(mint32ChecksumEnable);
+    MINT32_LOG("MINT32_NL_CHECKSUM enabled");
+  }
   // Set local variables to the launch bounds that we need.  Note that
   // PMENonbondForcesThreadsPerBlock is used exclusively to launch kernels
   // for force calculations (see above), but if virials are needed as part
@@ -2196,6 +2442,18 @@ extern "C" void kCalculatePMENonbondEnergy(gpuContext gpu)
   int nbBlocks = gpu->PMENonbondBlocks;
   int nrgThreads = gpu->PMENonbondEnergyThreadsPerBlock;
 
+  MINT32_LOG("After variables");
+
+  MINT32_LOG_INT("fswitch", gpu->sim.fswitch);
+  MINT32_LOG_INT("imin", gpu->imin);
+  MINT32_LOG_INT("NLAtomsPerWarp", gpu->sim.NLAtomsPerWarp);
+  MINT32_LOG_INT("ntp", gpu->sim.ntp);
+  MINT32_LOG_INT("barostat", gpu->sim.barostat);
+  MINT32_LOG_INT("iphmd", gpu->iphmd);
+  MINT32_LOG_INT("is_orthog", gpu->sim.is_orthog);
+  MINT32_LOG_INT("ti_mode", gpu->sim.ti_mode);
+  MINT32_LOG_INT("ifmbar", gpu->sim.ifmbar);
+
   // Decide which non-bonded direct space energy kernel to use.
   if (gpu->sim.fswitch < 0) {
     if (gpu->imin == 0) {
@@ -2605,6 +2863,13 @@ extern "C" void kCalculatePMENonbondEnergy(gpuContext gpu)
       }
     }
   }
+  MINT32_LOG("EXIT kCalculatePMENonbondEnergy (before LAUNCHERROR)");
+  if (mint32SampleEnable > 0) {
+    Mint32SampleDump();
+  }
+  if (mint32ChecksumEnable > 0) {
+    Mint32NLChecksumDump();
+  }
   LAUNCHERROR("kCalculatePMENonbondEnergy");
 }
 
@@ -3043,6 +3308,11 @@ kCalcIPSOrthoNBMiniNrg8_kernel()
 //---------------------------------------------------------------------------------------------
 extern "C" void kCalculatePMENonbondEnergyInitKernels(gpuContext gpu)
 {
+  const char* skipConfig = getenv("PMEMD_SKIP_SHMEM_CONFIG");
+  if (skipConfig != nullptr && skipConfig[0] == '1') {
+    fprintf(stderr, "PMEMD: skipping PMENB shared memory configuration (PMEMD_SKIP_SHMEM_CONFIG=1)\n");
+    return;
+  }
 #ifdef use_SPFP
 #  define PME_SHARED_BANK_SIZE cudaSharedMemBankSizeFourByte
 #else
diff --git "a/C:\\amber\\amber_master\\src\\pmemd\\src/cuda/kForcesUpdate.cu" "b/C:\\amber\\amber_mint\\src\\pmemd\\src/cuda/kForcesUpdate.cu"
index 78e57e2..1927982 100644
--- "a/C:\\amber\\amber_master\\src\\pmemd\\src/cuda/kForcesUpdate.cu"
+++ "b/C:\\amber\\amber_mint\\src\\pmemd\\src/cuda/kForcesUpdate.cu"
@@ -518,8 +518,11 @@ kRecenter_Molecule1_kernel()
 
   // Perform individual sums
   while (pos < cSim.atoms) {
-    PMEFloat2 xy = cSim.pAtomXYSP[pos];
-    PMEFloat z   = cSim.pAtomZSP[pos];
+    int4 coord = cSim.pAtomCoord_M32[pos];
+    PMEFloat2 xy;
+    xy.x = (PMEFloat)coord.x * cSim.mint32_inv_scale.x;
+    xy.y = (PMEFloat)coord.y * cSim.mint32_inv_scale.y;
+    PMEFloat z   = (PMEFloat)coord.z * cSim.mint32_inv_scale.z;
     xmax         = max(xy.x, xmax);
     xmin         = min(xy.x, xmin);
     ymax         = max(xy.y, ymax);
@@ -613,8 +616,27 @@ kRecenter_Molecule2_kernel()
     cSim.pAtomX[pos] = x;
     cSim.pAtomY[pos] = y;
     cSim.pAtomZ[pos] = z;
-    cSim.pAtomXYSP[pos] = xy;
-    cSim.pAtomZSP[pos]  = z;
+    
+    // MINT32 FIX: Wrap coordinates before converting to MINT32
+    // pAtomX/Y/Z may be unwrapped (large absolute values), but MINT32
+    // requires wrapped coords in [-box/2, box/2) to avoid integer overflow.
+    // Use fractional coords with rint() wrapping, then convert back.
+    PMEFloat fx = cSim.recipf[0][0]*x + cSim.recipf[1][0]*y + cSim.recipf[2][0]*z;
+    PMEFloat fy =                       cSim.recipf[1][1]*y + cSim.recipf[2][1]*z;
+    PMEFloat fz =                                             cSim.recipf[2][2]*z;
+    fx = fx - rintf(fx);  // Wrap to [-0.5, 0.5)
+    fy = fy - rintf(fy);
+    fz = fz - rintf(fz);
+    PMEFloat x_wrapped = fx * cSim.a;
+    PMEFloat y_wrapped = fy * cSim.b;
+    PMEFloat z_wrapped = fz * cSim.c;
+    
+    int4 coord;
+    coord.x = (int)rintf(x_wrapped * cSim.mint32_scale.x);
+    coord.y = (int)rintf(y_wrapped * cSim.mint32_scale.y);
+    coord.z = (int)rintf(z_wrapped * cSim.mint32_scale.z);
+    coord.w = 0;
+    cSim.pAtomCoord_M32[pos] = coord;
     pos += blockDim.x * gridDim.x;
   }
 
@@ -1496,9 +1518,16 @@ kNLClearVelocities_kernel()
 {
   unsigned int pos = blockIdx.x * blockDim.x + threadIdx.x;
   if (pos < cSim.atoms) {
+    // Keep both sorted (image) and unsorted velocity arrays in sync
     cSim.pImageVelX[pos] = (double)0.0;
     cSim.pImageVelY[pos] = (double)0.0;
     cSim.pImageVelZ[pos] = (double)0.0;
+    cSim.pVelX[pos]      = (double)0.0;
+    cSim.pVelY[pos]      = (double)0.0;
+    cSim.pVelZ[pos]      = (double)0.0;
+    cSim.pLVelX[pos]     = (double)0.0;
+    cSim.pLVelY[pos]     = (double)0.0;
+    cSim.pLVelZ[pos]     = (double)0.0;
     pos += blockDim.x * gridDim.x;
   }
 }
@@ -1596,18 +1625,32 @@ kPMERecalculateVelocities_kernel(PMEDouble dtx_inv)
   unsigned int pos = blockIdx.x*blockDim.x + threadIdx.x;
 
   if (pos < cSim.atoms) {
-    PMEDouble oldAtomX   = cSim.pOldAtomX[pos];
-    PMEDouble atomX      = cSim.pImageX[pos];
-    PMEDouble oldAtomY   = cSim.pOldAtomY[pos];
-    PMEDouble atomY      = cSim.pImageY[pos];
-    PMEDouble oldAtomZ   = cSim.pOldAtomZ[pos];
-    PMEDouble atomZ      = cSim.pImageZ[pos];
-    PMEDouble velX       = (atomX - oldAtomX) * dtx_inv;
-    PMEDouble velY       = (atomY - oldAtomY) * dtx_inv;
-    PMEDouble velZ       = (atomZ - oldAtomZ) * dtx_inv;
+    // MINT32 FIX: Use integer subtraction to handle wrapping correctly
+    int4 oldCoord = cSim.pOldAtomCoord[pos];
+    int4 coord = cSim.pAtomCoord_M32[pos];
+
+    // Integer subtraction wraps correctly across periodic boundaries
+    int dx = coord.x - oldCoord.x;
+    int dy = coord.y - oldCoord.y;
+    int dz = coord.z - oldCoord.z;
+
+    // Convert MINT32 delta to Cartesian velocity
+    PMEDouble velX = (PMEDouble)dx * cSim.mint32_inv_scale.x * dtx_inv;
+    PMEDouble velY = (PMEDouble)dy * cSim.mint32_inv_scale.y * dtx_inv;
+    PMEDouble velZ = (PMEDouble)dz * cSim.mint32_inv_scale.z * dtx_inv;
+
     cSim.pImageVelX[pos] = velX;
     cSim.pImageVelY[pos] = velY;
     cSim.pImageVelZ[pos] = velZ;
+
+    // MINT32 FIX: Also update unsorted velocity arrays to keep them synchronized
+    // This is critical for NL remapping - both arrays must stay in sync
+    unsigned int origIndex = cSim.pImageAtom[pos];
+    if (origIndex < cSim.atoms) {
+      cSim.pVelX[origIndex] = velX;
+      cSim.pVelY[origIndex] = velY;
+      cSim.pVelZ[origIndex] = velZ;
+    }
   }
 }
 
@@ -1677,6 +1720,15 @@ kMiddlePMERecalculateVelocities_kernel(PMEDouble dtx_inv)
     cSim.pImageVelX[pos] += velX;
     cSim.pImageVelY[pos] += velY;
     cSim.pImageVelZ[pos] += velZ;
+
+    // MINT32 FIX: Also update unsorted velocity arrays to keep them synchronized
+    // This is critical for NL remapping - both arrays must stay in sync
+    unsigned int origIndex = cSim.pImageAtom[pos];
+    if (origIndex < cSim.atoms) {
+      cSim.pVelX[origIndex] += velX;
+      cSim.pVelY[origIndex] += velY;
+      cSim.pVelZ[origIndex] += velZ;
+    }
   }
 }
 
@@ -1781,11 +1833,14 @@ kPMECalculateKineticEnergy_kernel(PMEFloat c_ave)
     PMEFloat svx  = vx + lvx;
     PMEFloat svy  = vy + lvy;
     PMEFloat svz  = vz + lvz;
-    eke          += mass * (svx * svx + svy * svy + svz * svz);
+    PMEFloat eke_contrib = mass * (svx * svx + svy * svy + svz * svz);
+    
+    eke          += eke_contrib;
     ekpbs        += mass * (vx * lvx + vy * lvy + vz * lvz);
     ekph         += mass * (vx * vx + vy * vy + vz * vz);
     pos          += blockDim.x * gridDim.x;
   }
+  
   eke   *= (PMEFloat)0.125 * c_ave;
   ekph  *= (PMEFloat)0.5;
   ekpbs *= (PMEFloat)0.5;
@@ -2023,6 +2078,8 @@ kPMECalculateKineticEnergyAFE_kernel(PMEFloat c_ave)
 void kCalculateKineticEnergy(gpuContext gpu, PMEFloat c_ave)
 {
   if (gpu->bNeighborList) {
+    // Snapshot velocities right before KE calculation (tag 2) for debugging
+    kVelSnapshot(2);
     kPMECalculateKineticEnergy_kernel<<<gpu->blocks, gpu->threadsPerBlock,
                                         gpu->threadsPerBlock * sizeof(KineticEnergy)>>>(c_ave);
   }
diff --git "a/C:\\amber\\amber_master\\src\\pmemd\\src/cuda/kMiddle.h" "b/C:\\amber\\amber_mint\\src\\pmemd\\src/cuda/kMiddle.h"
index 6e19f65..f5c1594 100644
--- "a/C:\\amber\\amber_master\\src\\pmemd\\src/cuda/kMiddle.h"
+++ "b/C:\\amber\\amber_mint\\src\\pmemd\\src/cuda/kMiddle.h"
@@ -117,17 +117,33 @@
         VELY(pos) = velY;
         VELZ(pos) = velZ;
 
-        ATOMX(pos) = newAtomX + velX * half_dtx;
-        ATOMY(pos) = newAtomY + velY * half_dtx;
-        ATOMZ(pos) = newAtomZ + velZ * half_dtx;
-
-#ifndef UPDATE_NEIGHBORLIST
-      PMEFloat2 xy;
-      xy.x = newAtomX;
-      xy.y = newAtomY;
-      cSim.pAtomXYSP[pos] = xy;
-      cSim.pAtomZSP[pos]  = newAtomZ;
-#endif
+        double finalX = newAtomX + velX * half_dtx;
+        double finalY = newAtomY + velY * half_dtx;
+        double finalZ = newAtomZ + velZ * half_dtx;
+        
+        // MINT32: Wrap coordinates to [-box/2, box/2) before converting to avoid overflow
+        double fx_m32 = cSim.recipf[0][0]*finalX + cSim.recipf[1][0]*finalY + cSim.recipf[2][0]*finalZ;
+        double fy_m32 =                            cSim.recipf[1][1]*finalY + cSim.recipf[2][1]*finalZ;
+        double fz_m32 =                                                       cSim.recipf[2][2]*finalZ;
+        fx_m32 = fx_m32 - rint(fx_m32);  // Wrap to [-0.5, 0.5)
+        fy_m32 = fy_m32 - rint(fy_m32);
+        fz_m32 = fz_m32 - rint(fz_m32);
+        double wrappedX = fx_m32 * cSim.a;
+        double wrappedY = fy_m32 * cSim.b;
+        double wrappedZ = fz_m32 * cSim.c;
+        
+        // Store WRAPPED coords in pImageX/Y/Z (consistent with MINT32)
+        ATOMX(pos) = wrappedX;
+        ATOMY(pos) = wrappedY;
+        ATOMZ(pos) = wrappedZ;
+
+        int4 coord;
+        coord.x = (int)rint(wrappedX * cSim.mint32_scale.x);
+        coord.y = (int)rint(wrappedY * cSim.mint32_scale.y);
+        coord.z = (int)rint(wrappedZ * cSim.mint32_scale.z);
+        coord.w = 0;
+        cSim.pAtomCoord_M32[pos] = coord;
+        // NOTE: pAtomXYSP/ZSP are populated by kMINT32ToFloat which reads from pImageX/Y/Z
 #endif
     }
 #undef MASS
diff --git "a/C:\\amber\\amber_master\\src\\pmemd\\src/cuda/kNLCINE.h" "b/C:\\amber\\amber_mint\\src\\pmemd\\src/cuda/kNLCINE.h"
index 389562f..b4599d3 100644
--- "a/C:\\amber\\amber_master\\src\\pmemd\\src/cuda/kNLCINE.h"
+++ "b/C:\\amber\\amber_mint\\src\\pmemd\\src/cuda/kNLCINE.h"
@@ -22,6 +22,10 @@
     PMEFloat x;
     PMEFloat y;
     PMEFloat z;
+    int xi32;
+    int yi32;
+    int zi32;
+    int valid;
     PMEFloat q;
     unsigned int LJID;
     unsigned int ID;
@@ -51,6 +55,10 @@
 #define PSATOMX(i) shAtom.x
 #define PSATOMY(i) shAtom.y
 #define PSATOMZ(i) shAtom.z
+#define PSATOMXI32(i) shAtom.xi32
+#define PSATOMYI32(i) shAtom.yi32
+#define PSATOMZI32(i) shAtom.zi32
+#define PSATOMVALID(i) shAtom.valid
 #define PSATOMQ(i) shAtom.q
 #define PSATOMLJID(i) shAtom.LJID
 #define PSATOMID(i) shAtom.ID
@@ -138,23 +146,31 @@
     PMEFloat yi;
     PMEFloat zi;
     PMEFloat qi;
+    int xi_i32 = 0;
+    int yi_i32 = 0;
+    int zi_i32 = 0;
+    int validAtom = 0;
     unsigned int LJIDi;
     PMEForce fx_i = (PMEForce)0;
     PMEForce fy_i = (PMEForce)0;
     PMEForce fz_i = (PMEForce)0;
     unsigned int index = psWarp->nlEntry.NL.ypos + (tgx & cSim.NLAtomsPerWarpBitsMask);
     if (index < psWarp->nlEntry.NL.ymax) {
-      PMEFloat2 xy    = cSim.pAtomXYSP[index];
+      int4 coord = cSim.pAtomCoord_M32[index];
+      xi_i32 = coord.x;
+      yi_i32 = coord.y;
+      zi_i32 = coord.z;
+      xi = (PMEFloat)xi_i32 * cSim.mint32_inv_scale.x;
+      yi = (PMEFloat)yi_i32 * cSim.mint32_inv_scale.y;
+      zi = (PMEFloat)zi_i32 * cSim.mint32_inv_scale.z;
       PMEFloat2 qljid = cSim.pAtomChargeSPLJID[index];
-      zi    = cSim.pAtomZSP[index];
-      xi    = xy.x;
-      yi    = xy.y;
       qi    = qljid.x;
 #ifdef use_DPFP
       LJIDi = __double_as_longlong(qljid.y);
 #else
       LJIDi = __float_as_uint(qljid.y);
 #endif
+      validAtom = 1;
     }
     else {
       xi    = (PMEFloat)10000.0 * index;
@@ -162,20 +178,13 @@
       zi    = (PMEFloat)10000.0 * index;
       qi    = (PMEFloat)0.0;
       LJIDi = 0;
+      xi_i32 = 0;
+      yi_i32 = 0;
+      zi_i32 = 0;
+      validAtom = 0;
     }
 
-#ifndef IPS_IS_ORTHOGONAL
-    // Transform into cartesian space
-#ifdef IPS_VIRIAL
-    xi = xi*sUcellf[0] + yi*sUcellf[1] + zi*sUcellf[2];
-    yi =                 yi*sUcellf[4] + zi*sUcellf[5];
-    zi =                                 zi*sUcellf[8];
-#else
-    xi = xi*cSim.ucellf[0][0] + yi*cSim.ucellf[0][1] + zi*cSim.ucellf[0][2];
-    yi =                        yi*cSim.ucellf[1][1] + zi*cSim.ucellf[1][2];
-    zi =                                               zi*cSim.ucellf[2][2];
-#endif
-#endif
+
     // Special-case first tile
     // Copy register data into shared memory
     if (psWarp->bHomeCell) {
@@ -188,6 +197,10 @@
       PSATOMX(tgx)     = xi;
       PSATOMY(tgx)     = yi;
       PSATOMZ(tgx)     = zi;
+      PSATOMXI32(tgx)  = xi_i32;
+      PSATOMYI32(tgx)  = yi_i32;
+      PSATOMZI32(tgx)  = zi_i32;
+      PSATOMVALID(tgx) = validAtom;
       PSATOMQ(tgx)     = qi;
       PSATOMLJID(tgx)  = LJIDi;
       LJIDi           *= cSim.LJTypes;
@@ -199,6 +212,10 @@
       shAtom.x    = __SHFL(WARP_MASK, shAtom.x, j);
       shAtom.y    = __SHFL(WARP_MASK, shAtom.y, j);
       shAtom.z    = __SHFL(WARP_MASK, shAtom.z, j);
+      shAtom.xi32 = __SHFL(WARP_MASK, shAtom.xi32, j);
+      shAtom.yi32 = __SHFL(WARP_MASK, shAtom.yi32, j);
+      shAtom.zi32 = __SHFL(WARP_MASK, shAtom.zi32, j);
+      shAtom.valid= __SHFL(WARP_MASK, shAtom.valid, j);
       shAtom.q    = __SHFL(WARP_MASK, shAtom.q, j);
       shAtom.LJID = __SHFL(WARP_MASK, shAtom.LJID, j);
       PMEMask mask1 = __BALLOT(WARP_MASK, j != tgx);
@@ -217,6 +234,8 @@
       PMEMask mask1 = __BALLOT(WARP_MASK, j != end);
       while (j != end) {
 #endif
+        // MINT32 cellOffset fix: Use float coordinates (which have cellOffset applied)
+        // Trade-off: Loses MINT32 precision for kNLCINE, but unblocks testing
         PMEFloat xij = xi - PSATOMX(j);
         PMEFloat yij = yi - PSATOMY(j);
         PMEFloat zij = zi - PSATOMZ(j);
@@ -349,16 +368,17 @@
       if (tx + tgx < psWarp->nlEntry.NL.xatoms) {
         unsigned int atom = PSATOMID(tgx) >> NLATOM_CELL_SHIFT;
 #ifndef use_DPFP
-        PMEFloat2 xy    = tex1Dfetch<float2>(cSim.texAtomXYSP, atom);
         PMEFloat2 qljid = tex1Dfetch<float2>(cSim.texAtomChargeSPLJID, atom);
-        PSATOMZ(tgx)    = tex1Dfetch<float>(cSim.texAtomZSP, atom);
 #else
-        PMEFloat2 xy    = cSim.pAtomXYSP[atom];
         PMEFloat2 qljid = cSim.pAtomChargeSPLJID[atom];
-        PSATOMZ(tgx)    = cSim.pAtomZSP[atom];
 #endif
-        PSATOMX(tgx)    = xy.x;
-        PSATOMY(tgx)    = xy.y;
+        int4 coord = cSim.pAtomCoord_M32[atom];
+        PSATOMX(tgx)    = (PMEFloat)coord.x * cSim.mint32_inv_scale.x;
+        PSATOMY(tgx)    = (PMEFloat)coord.y * cSim.mint32_inv_scale.y;
+        PSATOMZ(tgx)    = (PMEFloat)coord.z * cSim.mint32_inv_scale.z;
+        PSATOMXI32(tgx) = coord.x;
+        PSATOMYI32(tgx) = coord.y;
+        PSATOMZI32(tgx) = coord.z;
         PSATOMQ(tgx)    = qljid.x;
 #ifdef use_DPFP
         PSATOMLJID(tgx) = __double_as_longlong(qljid.y);
@@ -370,6 +390,9 @@
         PSATOMX(tgx)    = (PMEFloat)-10000.0 * tgx;
         PSATOMY(tgx)    = (PMEFloat)-10000.0 * tgx;
         PSATOMZ(tgx)    = (PMEFloat)-10000.0 * tgx;
+        PSATOMXI32(tgx) = 0;
+        PSATOMYI32(tgx) = 0;
+        PSATOMZI32(tgx) = 0;
         PSATOMQ(tgx)    = (PMEFloat)0.0;
         PSATOMLJID(tgx) = 0;
       }
@@ -387,19 +410,7 @@
       PSATOMZ(tgx) += cSim.cellOffset[cell][2];
 #endif
 
-#ifndef IPS_IS_ORTHOGONAL
-#  ifdef IPS_VIRIAL
-      PSATOMX(tgx) = sUcellf[0]*PSATOMX(tgx) + sUcellf[1]*PSATOMY(tgx) +
-                     sUcellf[2]*PSATOMZ(tgx);
-      PSATOMY(tgx) = sUcellf[4]*PSATOMY(tgx) + sUcellf[5]*PSATOMZ(tgx);
-      PSATOMZ(tgx) = sUcellf[8]*PSATOMZ(tgx);
-#  else
-      PSATOMX(tgx) = cSim.ucellf[0][0]*PSATOMX(tgx) + cSim.ucellf[0][1]*PSATOMY(tgx) +
-                     cSim.ucellf[0][2]*PSATOMZ(tgx);
-      PSATOMY(tgx) = cSim.ucellf[1][1]*PSATOMY(tgx) + cSim.ucellf[1][2]*PSATOMZ(tgx);
-      PSATOMZ(tgx) = cSim.ucellf[2][2]*PSATOMZ(tgx);
-#  endif
-#endif
+
       int j = tgx;
       int shIdx = ((tgx + 1) & cSim.NLAtomsPerWarpBitsMask) + joffset;
 
@@ -409,6 +420,7 @@
         PMEMask mask1 = WARP_MASK;
         #pragma unroll 2
         for (unsigned int ii = 0; ii < IPS_ATOMS_PER_WARP; ++ii) {
+          // MINT32 cellOffset fix: Use float coordinates (which have cellOffset applied)
           PMEFloat xij = xi - PSATOMX(j);
           PMEFloat yij = yi - PSATOMY(j);
           PMEFloat zij = zi - PSATOMZ(j);
@@ -523,6 +535,7 @@
         PMEMask mask1 = WARP_MASK;
         #pragma unroll 2
         for (unsigned int ii = 0; ii < IPS_ATOMS_PER_WARP; ++ii) {
+          // MINT32 cellOffset fix: Use float coordinates (which have cellOffset applied)
           PMEFloat xij = xi - PSATOMX(j);
           PMEFloat yij = yi - PSATOMY(j);
           PMEFloat zij = zi - PSATOMZ(j);
diff --git "a/C:\\amber\\amber_master\\src\\pmemd\\src/cuda/kNLCPNE.h" "b/C:\\amber\\amber_mint\\src\\pmemd\\src/cuda/kNLCPNE.h"
index 2fad484..a36732d 100644
--- "a/C:\\amber\\amber_master\\src\\pmemd\\src/cuda/kNLCPNE.h"
+++ "b/C:\\amber\\amber_mint\\src\\pmemd\\src/cuda/kNLCPNE.h"
@@ -24,11 +24,50 @@
 #  define VOLATILE
 #endif
 #define uint unsigned int
+#ifndef MINT32_FORCE_DOUBLE_NB32
+// Default: use double multiply for MINT32 delta -> float conversion.
+// Define MINT32_FORCE_DOUBLE_NB32=0 at compile time to force float math instead.
+#define MINT32_FORCE_DOUBLE_NB32 0
+#endif
+#ifdef MINT32_DEBUG_SAMPLE
+#ifndef MINT32_PAIR_SAMPLE_DEFINED
+#define MINT32_PAIR_SAMPLE_DEFINED
+  struct Mint32PairSample {
+    int block;
+    int warp;
+    int i;
+    int j;
+    int atom_i;
+    int atom_j;
+    float dx, dy, dz;
+    float r;
+    float r2;
+    float qi;
+    float qj;
+    int lj_i;
+    int lj_j;
+    float termx;
+    float termy;
+    float qiqj;
+    float r2inv;
+    float coef0, coef1, coef2, coef3;
+    float ecoul;
+    float evdw;
+    float frc;
+    float swtch;
+    float dswtch;
+    int   exclusion;
+  };
+#endif
+  extern __device__ int dMint32SampleEnable;
+  extern __device__ int dMint32PairSampleCount;
+  extern __device__ Mint32PairSample dMint32PairSamples[MINT32_SAMPLE_MAX];
+#endif
   struct NLAtom {
-    PMEFloat x;
-    PMEFloat y;
-    PMEFloat z;
     PMEFloat q;
+    int xi32;
+    int yi32;
+    int zi32;
 #ifdef PHMD
     PMEFloat qj_phmd;
     PMEFloat qg;
@@ -70,6 +109,9 @@
   __shared__ PMEFloat sUcellf[9];
 #endif
   __shared__ VOLATILE NLWarp sWarp[THREADS_PER_BLOCK / GRID];
+#ifdef MINT32_DEBUG_SAMPLE
+  // single-slot sampler; no shared limiter needed
+#endif
 
   // Read static data
   if (threadIdx.x == 0) {
@@ -128,15 +170,26 @@
   if (tgx == 0) {
     psWarp->pos = blockIdx.x * (THREADS_PER_BLOCK / GRID) + warp;
   }
+#ifdef MINT32_DEBUG_SAMPLE
+  // no sampler init needed for single-slot capture
+#endif
   __syncthreads();
 
   // Massive loop over all neighbor list entries
+  int loop_count = 0;
   while (psWarp->pos < sNLEntries) {
-
+    // DEBUG: Enable for hang investigation (doc 041)
+    if (blockIdx.x == 0 && threadIdx.x == 0 && loop_count < 5) {
+    }
     // Read Neighbor List entry
+    // DEBUG: Enable for hang investigation
+    if (blockIdx.x == 0 && threadIdx.x == 0 && loop_count == 0) {
+    }
     if (tgx < 4) {
       psWarp->nlEntry.array[tgx] = cSim.pNLEntry[psWarp->pos].array[tgx];
     }
+    if (blockIdx.x == 0 && threadIdx.x == 0 && loop_count == 0) {
+    }
     __SYNCWARP(WARP_MASK);
     if (tgx == 0) {
       psWarp->bHomeCell = psWarp->nlEntry.NL.ymax & NLENTRY_HOME_CELL_MASK;
@@ -144,14 +197,15 @@
     }
     __SYNCWARP(WARP_MASK);
     uint offset = psWarp->nlEntry.NL.offset;
+    Mint32ChecksumRecordEntry(tgx, psWarp->nlEntry.NL.xatoms);
 
     // Read y atoms into registers
-    PMEFloat xi;
-    PMEFloat yi;
-    PMEFloat zi;
     PMEFloat qi;
     unsigned int LJIDi;
     PMEForceAccumulator fx_i, fy_i, fz_i;
+    int xi_i32 = 0;
+    int yi_i32 = 0;
+    int zi_i32 = 0;
     fx_i = (PMEForceAccumulator)0;
     fy_i = (PMEForceAccumulator)0;
     fz_i = (PMEForceAccumulator)0;
@@ -175,27 +229,29 @@
     long long int fze_i = 0;
 #endif
     unsigned int index = psWarp->nlEntry.NL.ypos + (tgx & cSim.NLAtomsPerWarpBitsMask);
+    unsigned int atomIndex = index;
 #ifdef PHMD
       int indexi = index;
 #endif
     if (index < psWarp->nlEntry.NL.ymax) {
+      // DEBUG: Enable for hang investigation
+      if (blockIdx.x == 0 && threadIdx.x == 0 && loop_count == 0) {
+      }
+      int4 coord = cSim.pAtomCoord_M32[index];
+      if (blockIdx.x == 0 && threadIdx.x == 0 && loop_count == 0) {
+      }
+      xi_i32 = coord.x;
+      yi_i32 = coord.y;
+      zi_i32 = coord.z;
 #ifndef use_DPFP
 #  if defined(__CUDA_ARCH__) && ((__CUDA_ARCH__ == 700) || (__CUDA_ARCH__ >= 800))
-      PMEFloat2 xy = cSim.pAtomXYSP[index];
       PMEFloat2 qljid = cSim.pAtomChargeSPLJID[index];
-      zi = cSim.pAtomZSP[index];
 #  else
-      PMEFloat2 xy = tex1Dfetch<float2>(cSim.texAtomXYSP, index);
       PMEFloat2 qljid = tex1Dfetch<float2>(cSim.texAtomChargeSPLJID, index);
-      zi = tex1Dfetch<float>(cSim.texAtomZSP, index);
 #  endif
 #else
-      PMEFloat2 xy = cSim.pAtomXYSP[index];
       PMEFloat2 qljid = cSim.pAtomChargeSPLJID[index];
-      zi = cSim.pAtomZSP[index];
 #endif
-      xi = xy.x;
-      yi = xy.y;
       qi = qljid.x;
 #ifdef PHMD
       h = cSim.pImageGrplist[index] - 1;
@@ -254,32 +310,34 @@
 #endif
     }
     else {
-      xi = (PMEFloat)10000.0 * index;
-      yi = (PMEFloat)10000.0 * index;
-      zi = (PMEFloat)10000.0 * index;
       qi = (PMEFloat)0.0;
       LJIDi = 0;
+      xi_i32 = 0;
+      yi_i32 = 0;
+      zi_i32 = 0;
 #ifdef PHMD
       h = -1;
 #endif
     }
-#ifndef PME_IS_ORTHOGONAL
-    // Transform into cartesian space
-#  ifdef PME_VIRIAL
-    xi = sUcellf[0]*xi + sUcellf[1]*yi + sUcellf[2]*zi;
-    yi =                 sUcellf[4]*yi + sUcellf[5]*zi;
-    zi =                                 sUcellf[8]*zi;
-#  else
-    xi = cSim.ucellf[0][0]*xi + cSim.ucellf[0][1]*yi + cSim.ucellf[0][2]*zi;
-    yi =                        cSim.ucellf[1][1]*yi + cSim.ucellf[1][2]*zi;
-    zi =                                               cSim.ucellf[2][2]*zi;
-#  endif
-#endif
     // Special-case first tile: in the branch and loop that follows, each thread in a warp
-    // will take the perspective of one "i" atom and loop over 8, 16, or 32 "j" atoms
-    // depending on the compilation.
+  // will take the perspective of one "i" atom and loop over 8, 16, or 32 "j" atoms
+  // depending on the compilation.
+
+// Optional precision/perf toggle for MINT32 deltas.
+// Default: double multiply for maximum fidelity. Define MINT32_FORCE_DOUBLE_NB32=0
+// at compile time to force float math for throughput.
+#if MINT32_FORCE_DOUBLE_NB32
+#  define M32_DELTA_TO_FLOAT(delta, inv_scale) ((PMEFloat) ((double)delta) * inv_scale )
+#else
+#  define M32_DELTA_TO_FLOAT(delta, inv_scale) (PMEFloat)delta * (PMEFloat)inv_scale
+#endif
     if (psWarp->bHomeCell) {
+      // DEBUG: Commented out for clean test
+      // if (blockIdx.x == 0 && threadIdx.x == 0 && loop_count == 0) {
+      // }
       unsigned int exclusion = cSim.pNLAtomList[offset + (tgx & cSim.NLAtomsPerWarpBitsMask)];
+      // if (blockIdx.x == 0 && threadIdx.x == 0 && loop_count == 0) {
+      // }
       offset += cSim.NLAtomsPerWarp;
       if (PME_ATOMS_PER_WARP < GRID) {
         exclusion >>= iftlim * jgroup;
@@ -316,37 +374,59 @@
       int j = ((tgx + iftlim * jgroup + 1) & cSim.NLAtomsPerWarpBitsMask) | joffset;
       int jrec = ((PME_ATOMS_PER_WARP + tgx - (iftlim * jgroup + 1))
                   & cSim.NLAtomsPerWarpBitsMask) | joffset;
-      #pragma unroll 2
-      for (int ift = 0; ift < iftlim; ift++) {
-        PMEFloat xij = xi - __SHFL(WARP_MASK, xi, j);
-        PMEFloat yij = yi - __SHFL(WARP_MASK, yi, j);
-        PMEFloat zij = zi - __SHFL(WARP_MASK, zi, j);
+            #pragma unroll 2
+            for (int ift = 0; ift < iftlim; ift++) {
+
+        int xj_i32 = __SHFL(WARP_MASK, xi_i32, j);
+
+        int yj_i32 = __SHFL(WARP_MASK, yi_i32, j);
+        int zj_i32 = __SHFL(WARP_MASK, zi_i32, j);
+        PMEFloat xij = M32_DELTA_TO_FLOAT((xi_i32 - xj_i32), cSim.mint32_inv_scale.x);
+        PMEFloat yij = M32_DELTA_TO_FLOAT((yi_i32 - yj_i32), cSim.mint32_inv_scale.y);
+        PMEFloat zij = M32_DELTA_TO_FLOAT((zi_i32 - zj_i32), cSim.mint32_inv_scale.z);
         PMEFloat r2  = xij*xij + yij*yij + zij*zij;
-        unsigned int index = LJIDi + __SHFL(WARP_MASK, LJIDj, j);
+        unsigned int ljIndex = LJIDi + __SHFL(WARP_MASK, LJIDj, j);
         PMEFloat qiqj = qi * __SHFL(WARP_MASK, qi, j);
         PMEFloat df = (PMEFloat)0.0;
-        int inrange = ((r2 < cSim.cut2) && r2 > delta);
+        PMEFloat2 term;
+        term.x = (PMEFloat)0.0;
+        term.y = (PMEFloat)0.0;
+        PMEFloat r2inv = (PMEFloat)0.0;
+        PMEFloat swtch_val = (PMEFloat)0.0;
+        PMEFloat dswtch_val = (PMEFloat)0.0;
+        int sampleExclusion = (exclusion & 0x1);
+#if defined(use_SPFP) && !defined(PME_FSWITCH) && !defined(PME_ENERGY)
+        PMEFloat4 coef;
+        coef.x = 0.0f;
+        coef.y = 0.0f;
+        coef.z = 0.0f;
+        coef.w = 0.0f;
+#endif
+        int inrange = ((r2 < cSim.cut2) && (r2 > delta));
+#ifdef MINT32_DEBUG_SAMPLE
+#endif
 #ifdef PHMD
-        int g = __SHFL(0xFFFFFFFF, h, j);
-        int indexj = __SHFL(0xFFFFFFFF, indexi, j);
-        PMEFloat qg = __SHFL(0xFFFFFFFF, qh, j);
-        PMEFloat qxg = __SHFL(0xFFFFFFFF, qxh, j);
-        qj_phmd = __SHFL(0xFFFFFFFF, qi_phmd, j);
-        psp_grp_g = __SHFL(0xFFFFFFFF, psp_grp, j);
-        xg = __SHFL(0xFFFFFFFF, xh, j);
-        dxg = __SHFL(0xFFFFFFFF, dxh, j);
-        factg = __SHFL(0xFFFFFFFF, facth, j);
-        lgtauto = __SHFL(0xFFFFFFFF, lhtauto, j);
-        lgtitr = __SHFL(0xFFFFFFFF, lhtitr, j);
+        int g = __SHFL(WARP_MASK, h, j);
+        int indexj = __SHFL(WARP_MASK, indexi, j);
+        PMEFloat qg = __SHFL(WARP_MASK, qh, j);
+        PMEFloat qxg = __SHFL(WARP_MASK, qxh, j);
+        qj_phmd = __SHFL(WARP_MASK, qi_phmd, j);
+        psp_grp_g = __SHFL(WARP_MASK, psp_grp, j);
+        xg = __SHFL(WARP_MASK, xh, j);
+        dxg = __SHFL(WARP_MASK, dxh, j);
+        factg = __SHFL(WARP_MASK, facth, j);
+        lgtauto = __SHFL(WARP_MASK, lhtauto, j);
+        lgtitr = __SHFL(WARP_MASK, lhtitr, j);
 #endif
         if (inrange) {
+          Mint32ChecksumAccumulate(qiqj, r2, exclusion);
 #if defined(use_SPFP) && !defined(PME_FSWITCH) && !defined(PME_ENERGY)
-          PMEFloat r2inv = (PMEFloat)1.0 / r2;
+          r2inv = (PMEFloat)1.0 / r2;
           uint cidx = 2*(__float_as_uint(r2) >> 18) + (exclusion & 0x1);
 #  if defined(__CUDA_ARCH__) && ((__CUDA_ARCH__ == 700) || (__CUDA_ARCH__ >= 800))
-          PMEFloat4 coef = cSim.pErfcCoeffsTable[cidx];
+          coef = cSim.pErfcCoeffsTable[cidx];
 #  else
-          PMEFloat4 coef = tex1Dfetch<float4>(cSim.texErfcCoeffsTable, cidx);
+          coef = tex1Dfetch<float4>(cSim.texErfcCoeffsTable, cidx);
 #  endif
 #  ifdef PHMD
           PMEFloat r     = sqrt(r2);
@@ -355,16 +435,16 @@
 #else
           PMEFloat rinv  = rsqrt(r2);
           PMEFloat r     = r2 * rinv;
-          PMEFloat r2inv = rinv * rinv;
+          r2inv = rinv * rinv;
 #endif
 #ifndef use_DPFP
 #  if defined(__CUDA_ARCH__) && ((__CUDA_ARCH__ == 700) || (__CUDA_ARCH__ >= 800))
-          PMEFloat2 term = cSim.pLJTerm[index];
+          PMEFloat2 term = cSim.pLJTerm[ljIndex];
 #  else
-          PMEFloat2 term = tex1Dfetch<float2>(cSim.texLJTerm, index);
+          PMEFloat2 term = tex1Dfetch<float2>(cSim.texLJTerm, ljIndex);
 #  endif
 #else
-          PMEFloat2 term = cSim.pLJTerm[index];
+          PMEFloat2 term = cSim.pLJTerm[ljIndex];
 #endif
           PMEFloat r6inv = r2inv * r2inv * r2inv;
 #if defined(use_SPFP) && !defined(PME_FSWITCH) && !defined(PME_ENERGY)
@@ -383,6 +463,8 @@
           PMEFloat swtch = fasterfc(r);
 #  endif
           PMEFloat d_swtch_dx = cSim.negTwoEw_coeffRsqrtPI * exp(-cSim.ew_coeff2 * r2);
+          swtch_val = swtch;
+          dswtch_val = d_swtch_dx;
 #endif
 #if defined(PME_ENERGY) || defined(PHMD)
           PMEFloat fnrange = (PMEFloat)inrange;
@@ -402,10 +484,12 @@
             PMEFloat df12 = (PMEFloat)(-1) * term.x * r2inv * r12inv;
             PMEFloat df6 = (PMEFloat)(-1) * term.y * r2inv * r6inv;
 #  if defined(PME_ENERGY) || defined(PHMD)
+#    ifdef PME_ENERGY
             PMEFloat f12f = term.x * cSim.cut6invcut6minfswitch6 *
                             (r6inv - cSim.cut6inv) * (r6inv - cSim.cut6inv);
             PMEFloat f6f = term.y * cSim.cut3invcut3minfswitch3 *
                            (r3inv - cSim.cut3inv) * (r3inv - cSim.cut3inv);
+#    endif
             PMEFloat f12 = term.x*r12inv - term.x*cSim.invfswitch6cut6;
             PMEFloat f6 = term.y*r6inv - term.y*cSim.invfswitch3cut3;
 #  endif
@@ -422,7 +506,8 @@
             de = fnrange * (f12 * twelvth - f6 * sixth);
 #  endif
 #  ifdef PME_ENERGY
-            TLevdw += fnrange * (f12*twelvth - f6*sixth);
+            PMEFloat e_lj = (f12*twelvth - f6*sixth);
+            TLevdw += fnrange * e_lj;
 #  endif
             df += df6 - df12;
           }
@@ -436,7 +521,8 @@
             de = fnrange * (f12 * twelvth - f6 * sixth);
 #  endif
 #  ifdef PME_ENERGY
-            TLevdw += fnrange * (f12*twelvth - f6*sixth);
+            PMEFloat e_lj = (f12*twelvth - f6*sixth);
+            TLevdw += fnrange * e_lj;
 #  endif
           }
 #endif // PME_FSWITCH
@@ -523,6 +609,55 @@
           df = max(-10000.0f, min(df, 10000.0f));
 #endif
         } // inrange
+#ifdef MINT32_DEBUG_SAMPLE
+        // Capture a few real-space interactions when enabled (first come, first served).
+        // Limit sampling to a single interaction to avoid overhead.
+        if (dMint32SampleEnable && blockIdx.x == 0 && warp == 0 &&
+            (threadIdx.x & 31) == 0 && inrange) {
+          // Only the first successful thread records a sample.
+          if (atomicCAS(&dMint32PairSampleCount, 0, 1) == 0) {
+            Mint32PairSample s;
+            s.block = (int)blockIdx.x;
+            s.warp  = (int)(threadIdx.x >> 5);
+            s.i     = (int)ljIndex;
+            s.j     = (int)__shfl_sync(WARP_MASK, ljIndex, j);
+            s.atom_i = (int)atomIndex;
+            s.atom_j = (int)__shfl_sync(WARP_MASK, atomIndex, j);
+            s.dx    = xij;
+            s.dy    = yij;
+            s.dz    = zij;
+            s.r2    = r2;
+            s.r     = sqrtf(r2);
+            s.qi    = qi;
+            s.qj    = __shfl_sync(WARP_MASK, qi, j);
+            s.lj_i  = LJIDj;
+            s.lj_j  = __shfl_sync(WARP_MASK, LJIDj, j);
+            s.termx = term.x;
+            s.termy = term.y;
+            s.qiqj  = qiqj;
+            s.r2inv = r2inv;
+#if defined(use_SPFP) && !defined(PME_FSWITCH) && !defined(PME_ENERGY)
+            s.coef0 = coef.x;
+            s.coef1 = coef.y;
+            s.coef2 = coef.z;
+            s.coef3 = coef.w;
+#else
+            s.coef0 = 0.0f;
+            s.coef1 = 0.0f;
+            s.coef2 = 0.0f;
+            s.coef3 = 0.0f;
+#endif
+            s.ecoul = 0.0f;
+            s.evdw  = 0.0f;
+            s.frc   = df;
+            s.swtch = swtch_val;
+            s.dswtch = dswtch_val;
+            s.exclusion = sampleExclusion;
+            dMint32PairSamples[0] = s;
+            dMint32SampleEnable = 0; // disable further sampling
+          }
+        }
+#endif
         // Skip last "inactive" interactions
         if (ift * GRID + tgx >= totalInteractions) {
           df = (PMEFloat)0.0;
@@ -618,17 +753,27 @@
 
     // Handle the remainder of the line
     int tx = 0;
+    // DEBUG: Commented out for clean test
+    // if (blockIdx.x == 0 && threadIdx.x == 0 && loop_count == 0) {
+    // }
     while (tx < psWarp->nlEntry.NL.xatoms) {
+        // if (blockIdx.x == 0 && threadIdx.x == 0 && loop_count == 0) {
+        // }
 
       // Read atom ID and exclusion data
       NLAtom shAtom;
+      // DEBUG: Commented out for clean test
+      // if (blockIdx.x == 0 && threadIdx.x == 0 && loop_count == 0) {
+      // }
       shAtom.ID = cSim.pNLAtomList[offset + tgx];
       offset += GRID;
       PMEMask fullExclusion =
         ((PMEMask*)&cSim.pNLAtomList[offset])[tgx & cSim.NLAtomsPerWarpBitsMask];
       offset += cSim.NLAtomsPerWarp * sizeof(PMEMask) / sizeof(unsigned int);
       if (PME_ATOMS_PER_WARP < GRID) {
-        fullExclusion >>= joffset;
+        if ((offset & (GRID - 1)) == 0) {
+          offset += GRID - PME_ATOMS_PER_WARP;
+        }
       }
       unsigned int exclusion = (unsigned int)fullExclusion & cSim.NLAtomsPerWarpMask;
       // Clear j atom forces
@@ -689,21 +834,22 @@
 #endif
 #ifndef use_DPFP
 #  if defined(__CUDA_ARCH__) && ((__CUDA_ARCH__ == 700) || (__CUDA_ARCH__ >= 800))
-        PMEFloat2 xy = cSim.pAtomXYSP[atom];
         PMEFloat2 qljid = cSim.pAtomChargeSPLJID[atom];
-        shAtom.z = cSim.pAtomZSP[atom];
 #  else
-        PMEFloat2 xy = tex1Dfetch<float2>(cSim.texAtomXYSP, atom);
         PMEFloat2 qljid = tex1Dfetch<float2>(cSim.texAtomChargeSPLJID, atom);
-        shAtom.z = tex1Dfetch<float>(cSim.texAtomZSP, atom);
 #  endif
 #else
-        PMEFloat2 xy = cSim.pAtomXYSP[atom];
         PMEFloat2 qljid = cSim.pAtomChargeSPLJID[atom];
-        shAtom.z = cSim.pAtomZSP[atom];
 #endif
-        shAtom.x = xy.x;
-        shAtom.y = xy.y;
+        // DEBUG: Commented out for clean test
+        // if (blockIdx.x == 0 && threadIdx.x == 0 && loop_count == 0) {
+        // }
+        int4 coord = cSim.pAtomCoord_M32[atom];
+        // if (blockIdx.x == 0 && threadIdx.x == 0 && loop_count == 0) {
+        // }
+  shAtom.xi32 = coord.x;
+  shAtom.yi32 = coord.y;
+  shAtom.zi32 = coord.z;
         shAtom.q = qljid.x;
 #ifdef use_DPFP
         shAtom.LJID = __double_as_longlong(qljid.y);
@@ -712,38 +858,13 @@
 #endif
       }
       else {
-        shAtom.x = (PMEFloat)-10000.0 * tgx;
-        shAtom.y = (PMEFloat)-10000.0 * tgx;
-        shAtom.z = (PMEFloat)-10000.0 * tgx;
         shAtom.q = (PMEFloat)0.0;
         shAtom.LJID = 0;
+  shAtom.xi32 = 0;
+  shAtom.yi32 = 0;
+  shAtom.zi32 = 0;
       }
 
-      // Translate all atoms into a local coordinate system within one unit
-      // cell of the first atom read to avoid PBC handling within inner loops
-      int cell = shAtom.ID & NLATOM_CELL_TYPE_MASK;
-#if defined(PME_VIRIAL) && defined(PME_IS_ORTHOGONAL)
-      shAtom.x += sUcellf[0] * cSim.cellOffset[cell][0];
-      shAtom.y += sUcellf[4] * cSim.cellOffset[cell][1];
-      shAtom.z += sUcellf[8] * cSim.cellOffset[cell][2];
-#else
-      shAtom.x += cSim.cellOffset[cell][0];
-      shAtom.y += cSim.cellOffset[cell][1];
-      shAtom.z += cSim.cellOffset[cell][2];
-#endif
-#ifndef PME_IS_ORTHOGONAL
-#  ifdef PME_VIRIAL
-      shAtom.x = sUcellf[0]*shAtom.x + sUcellf[1]*shAtom.y + sUcellf[2]*shAtom.z;
-      shAtom.y = sUcellf[4]*shAtom.y + sUcellf[5]*shAtom.z;
-      shAtom.z = sUcellf[8]*shAtom.z;
-#  else
-      shAtom.x = cSim.ucellf[0][0]*shAtom.x + cSim.ucellf[0][1]*shAtom.y +
-                 cSim.ucellf[0][2]*shAtom.z;
-      shAtom.y = cSim.ucellf[1][1]*shAtom.y + cSim.ucellf[1][2]*shAtom.z;
-      shAtom.z = cSim.ucellf[2][2]*shAtom.z;
-#  endif
-#endif
-
 #ifdef PME_ENERGY
       PMEFloat TLeed  = (PMEFloat)0.0;
       PMEFloat TLevdw = (PMEFloat)0.0;
@@ -763,17 +884,29 @@
       // will have information to contribute to the "j atom" owned by this thread.
       int j = tgx;
       int jrec = tgx;
+      // DEBUG: Commented out for clean test
+      // if (blockIdx.x == 0 && threadIdx.x == 0 && loop_count == 0) {
+      // }
       if (__ANY(WARP_MASK, exclusion)) {
+        // if (blockIdx.x == 0 && threadIdx.x == 0 && loop_count == 0) {
+        // }
         #pragma unroll 2
         for (int it = 0; it < PME_ATOMS_PER_WARP; it++) {
-          PMEFloat xij = xi - __SHFL(WARP_MASK, shAtom.x, j);
-          PMEFloat yij = yi - __SHFL(WARP_MASK, shAtom.y, j);
-          PMEFloat zij = zi - __SHFL(WARP_MASK, shAtom.z, j);
+
+          int xj32 = __SHFL(WARP_MASK, shAtom.xi32, j);
+          int yj32 = __SHFL(WARP_MASK, shAtom.yi32, j);
+          int zj32 = __SHFL(WARP_MASK, shAtom.zi32, j);
+          PMEFloat xij = M32_DELTA_TO_FLOAT((xi_i32 - xj32), cSim.mint32_inv_scale.x);
+          PMEFloat yij = M32_DELTA_TO_FLOAT((yi_i32 - yj32), cSim.mint32_inv_scale.y);
+          PMEFloat zij = M32_DELTA_TO_FLOAT((zi_i32 - zj32), cSim.mint32_inv_scale.z);
           PMEFloat r2  = xij*xij + yij*yij + zij*zij;
           unsigned int index = LJIDi + __SHFL(WARP_MASK, shAtom.LJID, j);
+          // DEBUG: Commented out for clean test
+          // if (blockIdx.x == 0 && threadIdx.x == 0 && loop_count == 0 && it < 5) {
+          // }
           PMEFloat qiqj = qi * __SHFL(WARP_MASK, shAtom.q, j);
           PMEFloat df = (PMEFloat)0.0;
-          bool inrange = ((r2 < cSim.cut2) && r2 > delta);
+          bool inrange = ((r2 < cSim.cut2) && (r2 > delta));
 #ifdef PHMD
           int g = __SHFL(WARP_MASK, shAtom.g, j);
           PMEFloat qg = __SHFL(WARP_MASK, shAtom.qg, j);
@@ -788,9 +921,13 @@
           lgtitr = __SHFL(WARP_MASK, shAtom.lgtitr, j);
 #endif
           if (inrange) {
+            Mint32ChecksumAccumulate(qiqj, r2, exclusion);
 #if defined(use_SPFP) && !defined(PME_FSWITCH) && !defined(PME_ENERGY)
             PMEFloat r2inv = (PMEFloat)1.0 / r2;
             uint cidx = 2*(__float_as_uint(r2) >> 18) + (exclusion & 0x1);
+            // DEBUG: Commented out for clean test
+            // if (blockIdx.x == 0 && threadIdx.x == 0 && loop_count == 0) {
+            // }
 #  if defined(__CUDA_ARCH__) && ((__CUDA_ARCH__ == 700) || (__CUDA_ARCH__ >= 800))
             PMEFloat4 coef = cSim.pErfcCoeffsTable[cidx];
 #  else
@@ -846,10 +983,12 @@
               PMEFloat df12 = (PMEFloat)(-1) * term.x * r2inv * r12inv;
               PMEFloat df6 = (PMEFloat)(-1) * term.y * r2inv * r6inv;
 #  if defined(PME_ENERGY) || defined(PHMD)
+#    ifdef PME_ENERGY
               PMEFloat f12f = term.x * cSim.cut6invcut6minfswitch6 *
                               (r6inv - cSim.cut6inv)*(r6inv - cSim.cut6inv);
               PMEFloat f6f = term.y * cSim.cut3invcut3minfswitch3 *
                              (r3inv - cSim.cut3inv)*(r3inv - cSim.cut3inv);
+#    endif
               PMEFloat f12 = (term.x * r12inv) - (term.x * cSim.invfswitch6cut6);
               PMEFloat f6 = (term.y * r6inv) - (term.y * cSim.invfswitch3cut3);
 #  endif
@@ -986,12 +1125,18 @@
         // there IS at least one exclusion somewhere in the pile
       }
       else {
+        // DEBUG: Commented out for clean test
+        // if (blockIdx.x == 0 && threadIdx.x == 0 && loop_count == 0) {
+        // }
         #pragma unroll 2
         for (int it = 0; it < PME_ATOMS_PER_WARP; it++) {
-          // Read properties for the other atom
-          PMEFloat xij = xi - __SHFL(WARP_MASK, shAtom.x, j);
-          PMEFloat yij = yi - __SHFL(WARP_MASK, shAtom.y, j);
-          PMEFloat zij = zi - __SHFL(WARP_MASK, shAtom.z, j);
+
+          int xj32 = __SHFL(WARP_MASK, shAtom.xi32, j);
+          int yj32 = __SHFL(WARP_MASK, shAtom.yi32, j);
+          int zj32 = __SHFL(WARP_MASK, shAtom.zi32, j);
+          PMEFloat xij = M32_DELTA_TO_FLOAT((xi_i32 - xj32), cSim.mint32_inv_scale.x);
+          PMEFloat yij = M32_DELTA_TO_FLOAT((yi_i32 - yj32), cSim.mint32_inv_scale.y);
+          PMEFloat zij = M32_DELTA_TO_FLOAT((zi_i32 - zj32), cSim.mint32_inv_scale.z);
 #ifdef PHMD
           int g = __SHFL(WARP_MASK, shAtom.g, j);
           PMEFloat qg = __SHFL(WARP_MASK, shAtom.qg, j);
@@ -1005,13 +1150,13 @@
           lgtauto = __SHFL(WARP_MASK, shAtom.lgtauto, j);
           lgtitr = __SHFL(WARP_MASK, shAtom.lgtitr, j);
 #endif
-          // Perform the range test
           PMEFloat r2  = xij*xij + yij*yij + zij*zij;
           unsigned int index = LJIDi + __SHFL(WARP_MASK, shAtom.LJID, j);
           PMEFloat qiqj = qi * __SHFL(WARP_MASK, shAtom.q, j);
-          bool inrange = ((r2 < cSim.cut2) && r2 > delta);
+          bool inrange = ((r2 < cSim.cut2) && (r2 > delta));
           PMEFloat df = (PMEFloat)0.0;
           if (inrange) {
+            Mint32ChecksumAccumulate(qiqj, r2, exclusion);
 #if defined(use_SPFP) && !defined(PME_FSWITCH) && !defined(PME_ENERGY)
             PMEFloat r2inv = (PMEFloat)1.0 / r2;
             uint cidx = 2*(__float_as_uint(r2) >> 18);
@@ -1069,10 +1214,12 @@
             PMEFloat df12 = (PMEFloat)(-1) * term.x * r2inv * r12inv;
             PMEFloat df6 = (PMEFloat)(-1) * term.y * r2inv * r6inv;
 #  if defined(PME_ENERGY) || defined(PHMD)
+#    ifdef PME_ENERGY
             PMEFloat f12f = term.x * cSim.cut6invcut6minfswitch6 *
                             (r6inv - cSim.cut6inv)*(r6inv - cSim.cut6inv);
             PMEFloat f6f = term.y * cSim.cut3invcut3minfswitch3 *
                            (r3inv - cSim.cut3inv)*(r3inv - cSim.cut3inv);
+#    endif
             PMEFloat f12 = term.x * r12inv - term.x * cSim.invfswitch6cut6;
             PMEFloat f6 = term.y * r6inv - term.y * cSim.invfswitch3cut3;
 #  endif
@@ -1248,6 +1395,7 @@
 #endif
           j = ((j + 1) & cSim.NLAtomsPerWarpBitsMask) | joffset;
           jrec = ((jrec + PME_ATOMS_PER_WARP - 1) & cSim.NLAtomsPerWarpBitsMask) | joffset;
+
         }
         // Ends for loop for processing a pile of non-bonded
         // interactions with no exclusions to worry about
@@ -1313,6 +1461,7 @@
       // Dump j atom forces
       if (tx + tgx < psWarp->nlEntry.NL.xatoms) {
         int offset = (shAtom.ID >> NLATOM_CELL_SHIFT);
+
 #ifdef use_SPFP
         atomicAdd((unsigned long long int*)&cSim.pNBForceXAccumulator[offset],
                   llitoulli(fast_llrintf(shFx * FORCESCALEF)));
@@ -1392,6 +1541,7 @@
       psWarp->pos = atomicAdd(&cSim.pFrcBlkCounters[0], 1);
     }
     __SYNCWARP(WARP_MASK);
+    loop_count++;
   }
   // End of massive while loop iterating psWarp->pos up to sNLEntries
 
diff --git "a/C:\\amber\\amber_mint\\src\\pmemd\\src/cuda/kNLCPNE.h.saved" "b/C:\\amber\\amber_mint\\src\\pmemd\\src/cuda/kNLCPNE.h.saved"
new file mode 100644
index 0000000..db60635
--- /dev/null
+++ "b/C:\\amber\\amber_mint\\src\\pmemd\\src/cuda/kNLCPNE.h.saved"
@@ -0,0 +1,1496 @@
+#include "copyright.i"
+
+//---------------------------------------------------------------------------------------------
+// AMBER NVIDIA CUDA GPU IMPLEMENTATION: PMEMD VERSION
+//
+// July 2017, by Scott Le Grand, David S. Cerutti, Daniel J. Mermelstein, Charles Lin, and
+//               Ross C. Walker
+//---------------------------------------------------------------------------------------------
+
+//---------------------------------------------------------------------------------------------
+// This is included by kCalculatePMENonbondEnergy.cu, many times for different kernels
+// calculating forces, energies, and virials, all with different numbers of atoms per warp.
+// VDW force-switching code could use optimization.
+//
+// #defines: PME_ENERGY, PME_VIRIAL, PME_IS_ORTHOGONAL, PME_ATOMS_PER_WARP, PME_MINIMIZATION
+//
+// In this file, there is another added convention that nested pre-processor directives are
+// also indented, as there are so many of them.
+//---------------------------------------------------------------------------------------------
+{
+#if !defined(AMBER_PLATFORM_AMD)
+#  define VOLATILE volatile
+#else
+#  define VOLATILE
+#endif
+#define uint unsigned int
+#ifndef MINT32_FORCE_DOUBLE_NB32
+// Default: use double multiply for MINT32 delta -> float conversion.
+// Define MINT32_FORCE_DOUBLE_NB32=0 at compile time to force float math instead.
+#define MINT32_FORCE_DOUBLE_NB32 0
+#endif
+  struct NLAtom {
+    int xi32;
+    int yi32;
+    int zi32;
+    PMEFloat q;
+
+    int valid;
+#ifdef PHMD
+    PMEFloat qj_phmd;
+    PMEFloat qg;
+    PMEFloat qxg;
+    PMEFloat lgtauto;
+    PMEFloat igtaut;
+    PMEFloat xg;
+    PMEFloat factg;
+    PMEFloat dxg;
+    bool lgtitr;
+    int g;
+    int psp_grp_g;
+    unsigned int indexj;
+#endif
+    unsigned int LJID;
+    unsigned int ID;
+  };
+
+  struct NLWarp {
+    NLEntry nlEntry;
+    uint pos;
+    bool bHomeCell;
+  };
+
+  const PMEFloat delta = 1.0e-5;
+#if defined(PME_ENERGY) || defined(PHMD)
+  const PMEFloat sixth = (PMEFloat)0.16666666666667;
+  const PMEFloat twelvth = (PMEFloat)0.083333333333333;
+#endif
+
+#if defined(PME_VIRIAL) || defined(PME_ENERGY)
+  const int THREADS_PER_BLOCK = PMENONBONDENERGY_THREADS_PER_BLOCK;
+#else
+  const int THREADS_PER_BLOCK = PMENONBONDFORCES_THREADS_PER_BLOCK;
+#endif
+
+  __shared__ unsigned int sNLEntries;
+#if defined(PME_VIRIAL)
+  __shared__ PMEFloat sUcellf[9];
+#endif
+  __shared__ VOLATILE NLWarp sWarp[THREADS_PER_BLOCK / GRID];
+
+  // Read static data
+  if (threadIdx.x == 0) {
+    sNLEntries = *(cSim.pNLEntries);
+  }
+
+#ifdef PME_VIRIAL
+  if (threadIdx.x < 9) {
+    sUcellf[threadIdx.x] = cSim.pNTPData->ucellf[threadIdx.x];
+  }
+#endif
+#ifdef PME_ENERGY
+  PMEForceAccumulator eed, evdw;
+  eed = (PMEForceAccumulator)0;
+  evdw = (PMEForceAccumulator)0;
+#  if defined(use_DPFP) && defined(PME_MINIMIZATION)
+  long long int eede = 0;
+  long long int evdwe = 0;
+#  endif
+#endif
+
+#ifdef PME_VIRIAL
+  PMEVirialAccumulator vir_11, vir_22, vir_33;
+  vir_11 = (PMEVirialAccumulator)0;
+  vir_22 = (PMEVirialAccumulator)0;
+  vir_33 = (PMEVirialAccumulator)0;
+#  if defined(use_DPFP) && defined(PME_MINIMIZATION)
+  long long int vir_11E = 0;
+  long long int vir_22E = 0;
+  long long int vir_33E = 0;
+#  endif
+#endif
+
+
+  // Position in the warp and warp indexing into __shared__ arrays
+  uint tgx = threadIdx.x & (GRID - 1);
+  uint warp = THREADS_PER_BLOCK == GRID ? 0 : threadIdx.x / GRID;
+
+  // The number of iterations required to compute all interactions in the first tile.
+  // There are PME_ATOMS_PER_WARP / 2 * (PME_ATOMS_PER_WARP - 1) unique interactions: for every
+  // two atoms i and j we compute i-j (upper triangle of a matrix), but do not compute
+  // 1) j-i (lower triangle) since it's just a reversed i-j;
+  // 2) i-i, j-j (diagonal), i.e. self-interactions.
+  // * for 16 atoms per warp, the first tile has 120 ((1+15)/2*16) interactions to compute,
+  //   which requires 4 iterations (32-lane warps) or 2 iterations (64-lane warps);
+  // * for 32 atoms per warp, there are 496 ((1+31)/2*32) interactions,
+  //   16 iterations (32-lane warps) or 8 iterations (64-lane warps).
+  // * for 8 atoms per warp, there are only 28 ((1+7)/2*8) interactions, 1 iteration;
+  uint totalInteractions = PME_ATOMS_PER_WARP / 2 * (PME_ATOMS_PER_WARP - 1);
+  uint iftlim = (totalInteractions + GRID - 1) / GRID;
+
+  uint jgroup = tgx >> cSim.NLAtomsPerWarpBits;
+  uint joffset = tgx & ~cSim.NLAtomsPerWarpBitsMask;
+
+  VOLATILE NLWarp* psWarp = &sWarp[warp];
+  if (tgx == 0) {
+    psWarp->pos = blockIdx.x * (THREADS_PER_BLOCK / GRID) + warp;
+  }
+  __syncthreads();
+
+  // Massive loop over all neighbor list entries
+  while (psWarp->pos < sNLEntries) {
+    // Read Neighbor List entry
+    if (tgx < 4) {
+      psWarp->nlEntry.array[tgx] = cSim.pNLEntry[psWarp->pos].array[tgx];
+    }
+    __SYNCWARP(WARP_MASK);
+    if (tgx == 0) {
+      psWarp->bHomeCell = psWarp->nlEntry.NL.ymax & NLENTRY_HOME_CELL_MASK;
+      psWarp->nlEntry.NL.ymax >>= NLENTRY_YMAX_SHIFT;
+    }
+    __SYNCWARP(WARP_MASK);
+    uint offset = psWarp->nlEntry.NL.offset;
+    Mint32ChecksumRecordEntry(tgx, psWarp->nlEntry.NL.xatoms);
+
+    // Read y atoms into registers
+    PMEFloat qi;
+    unsigned int LJIDi;
+    PMEForceAccumulator fx_i, fy_i, fz_i;
+    int xi_i32 = 0;
+    int yi_i32 = 0;
+    int zi_i32 = 0;
+    int validAtom = 0;
+    fx_i = (PMEForceAccumulator)0;
+    fy_i = (PMEForceAccumulator)0;
+    fz_i = (PMEForceAccumulator)0;
+#ifdef PHMD
+    PMEFloat dudli, dudliplus;
+    PMEFloat qh, qxh, lambda, qprot, qunprot, qi_phmd, qj_phmd;
+    PMEFloat dudlj, dudljplus, factphmd;
+    PMEFloat xh, ihtaut, facth, dxh, xg, factg, dxg;
+    PMEFloat2 vstate1;
+    PMEFloat lambdah, de, dfphmd;
+    PMEFloat2 pqstate1, pqstate2;
+    PMEFloat radh = 0;
+    int h, psp_grp, psp_grp_g;
+    bool lhtauto, lgtauto;
+    bool lhtitr, lgtitr;
+    PMEFloat x2 = 1.0;
+#endif
+#if defined(use_DPFP) && defined(PME_MINIMIZATION)
+    long long int fxe_i = 0;
+    long long int fye_i = 0;
+    long long int fze_i = 0;
+#endif
+    unsigned int index = psWarp->nlEntry.NL.ypos + (tgx & cSim.NLAtomsPerWarpBitsMask);
+    unsigned int atomIndex = index;
+#ifdef PHMD
+      int indexi = index;
+#endif
+    if (index < psWarp->nlEntry.NL.ymax) {
+      // DEBUG: Enable for hang investigation
+      int4 coord = cSim.pAtomCoord_M32[index];
+      xi_i32 = coord.x;
+      yi_i32 = coord.y;
+      zi_i32 = coord.z;
+#ifndef use_DPFP
+#  if defined(__CUDA_ARCH__) && ((__CUDA_ARCH__ == 700) || (__CUDA_ARCH__ >= 800))
+      PMEFloat2 qljid = cSim.pAtomChargeSPLJID[index];
+#  else
+      PMEFloat2 qljid = tex1Dfetch<float2>(cSim.texAtomChargeSPLJID, index);
+#  endif
+#else
+      PMEFloat2 qljid = cSim.pAtomChargeSPLJID[index];
+#endif
+      qi = qljid.x;
+      validAtom = 1;
+#ifdef PHMD
+      h = cSim.pImageGrplist[index] - 1;
+      qi_phmd = cSim.pImageCharge_phmd[index];
+      qh = 0;
+      qxh = 0;
+      lhtitr = false;
+      dudli = 0;
+      dudliplus = 0;
+      if (h >= 0) {
+        pqstate1 = cSim.pImageQstate1[index];
+        pqstate2 = cSim.pImageQstate2[index];
+        psp_grp = cSim.psp_grp[h];
+        vstate1 = cSim.pvstate1[cSim.pImageIndexPHMD[index]];
+        radh = vstate1.x;
+        lambda = sin(cSim.pph_theta[h]);
+        lambda *= lambda;
+        lambdah = lambda;
+        facth = 1.0 - lambdah;
+        xh = 1.0;
+        if (psp_grp > 0) {
+          lhtauto = true;
+          x2 = sin(cSim.pph_theta[h+1]);
+          x2 *= x2;
+          radh += vstate1.y;
+          if (vstate1.x > 0) {
+            ihtaut = 1.0;
+            xh = x2;
+          }
+          else if (vstate1.y > 0) {
+            ihtaut = -1;
+            xh = 1.0 - x2;
+          }
+          if (psp_grp == 1) {
+            facth = 1.0 - lambda * xh;
+            dxh = -ihtaut * lambda;
+          }
+          else if (psp_grp == 3) {
+            facth = (1.0 - lambda) * xh;
+            dxh = ihtaut * (1.0 - lambda);
+          }
+          qunprot = lambda * (pqstate2.x - pqstate2.y);
+          qprot = (1 - lambda) * (pqstate1.x - pqstate1.y);
+          qxh = qunprot + qprot;
+        }
+        lhtitr = (radh > 0);
+        qunprot = x2 * pqstate2.x + (1.0 - x2) * pqstate2.y;
+        qprot = x2 * pqstate1.x + (1.0 - x2) * pqstate1.y;
+        qh = qunprot - qprot;
+      }
+#endif
+#ifdef use_DPFP
+      LJIDi = __double_as_longlong(qljid.y);
+#else
+      LJIDi = __float_as_uint(qljid.y);
+#endif
+    }
+    else {
+      qi = (PMEFloat)0.0;
+      LJIDi = 0;
+      xi_i32 = 0;
+      yi_i32 = 0;
+      zi_i32 = 0;
+      validAtom = 0;
+#ifdef PHMD
+      h = -1;
+#endif
+    }
+    // Special-case first tile: in the branch and loop that follows, each thread in a warp
+  // will take the perspective of one "i" atom and loop over 8, 16, or 32 "j" atoms
+  // depending on the compilation.
+
+// Optional precision/perf toggle for MINT32 deltas.
+// Default: double multiply for maximum fidelity. Define MINT32_FORCE_DOUBLE_NB32=0
+// at compile time to force float math for throughput.
+#if MINT32_FORCE_DOUBLE_NB32
+#  define M32_DELTA_TO_FLOAT(delta, inv_scale) ((PMEFloat) ((double)delta) * inv_scale )
+#else
+#  define M32_DELTA_TO_FLOAT(delta, inv_scale) (PMEFloat)delta * (PMEFloat)inv_scale
+#endif
+    if (psWarp->bHomeCell) {
+      unsigned int exclusion = cSim.pNLAtomList[offset + (tgx & cSim.NLAtomsPerWarpBitsMask)];
+      offset += cSim.NLAtomsPerWarp;
+      if (PME_ATOMS_PER_WARP < GRID) {
+        exclusion >>= iftlim * jgroup;
+      }
+      // In the first tile, atoms A, B, C, ... N are interacting with each other, not
+      // some new group of 8, 16, or 32 atoms.  shAtom, a buffer for each thread to
+      // store its j atom in addition to its i atom, is not needed in this context.
+      // However, the Lennard Jones index of each i atom gets multiplied by the total
+      // number of types to prepare for indexing into the table, so store the original
+      // value for future __shfl() reference.
+      unsigned int LJIDj = LJIDi;
+      LJIDi *= cSim.LJTypes;
+
+      // Tile accumulators for forces, energy, and virial components
+      PMEFloat TLx_i  = (PMEFloat)0.0;
+      PMEFloat TLy_i  = (PMEFloat)0.0;
+      PMEFloat TLz_i  = (PMEFloat)0.0;
+#ifdef PME_ENERGY
+      PMEFloat TLeed  = (PMEFloat)0.0;
+      PMEFloat TLevdw = (PMEFloat)0.0;
+#endif
+#ifdef PME_VIRIAL
+      PMEFloat TLvir_11 = (PMEFloat)0.0;
+      PMEFloat TLvir_22 = (PMEFloat)0.0;
+      PMEFloat TLvir_33 = (PMEFloat)0.0;
+#endif
+      // The first tile loop uniquely skips the
+      // first interaction of particle held by each thread tgx, as it would
+      // be each i atom with itself.  In subsequent tiles, the interaction
+      // of the i atom in thread tgx with the j atom in that same thread
+      // is necessary--they are different atoms.
+      // Fast-forward iftlim iterations in the final PME_ATOMS_PER_WARP lanes for the first
+      // tile computation.
+      int j = ((tgx + iftlim * jgroup + 1) & cSim.NLAtomsPerWarpBitsMask) | joffset;
+      int jrec = ((PME_ATOMS_PER_WARP + tgx - (iftlim * jgroup + 1))
+                  & cSim.NLAtomsPerWarpBitsMask) | joffset;
+            #pragma unroll 2
+            for (int ift = 0; ift < iftlim; ift++) {
+
+        // MINT32 FIX (doc 026): Use activemask to avoid divergence deadlock
+        __syncwarp(0xffffffff); // Ensure full warp convergence
+        unsigned int activeMask = __activemask();
+        int xj_i32 = __SHFL(activeMask, xi_i32, j);
+
+        int yj_i32 = __SHFL(activeMask, yi_i32, j);
+        int zj_i32 = __SHFL(activeMask, zi_i32, j);
+        PMEFloat xij = M32_DELTA_TO_FLOAT((xi_i32 - xj_i32), cSim.mint32_inv_scale.x);
+        PMEFloat yij = M32_DELTA_TO_FLOAT((yi_i32 - yj_i32), cSim.mint32_inv_scale.y);
+        PMEFloat zij = M32_DELTA_TO_FLOAT((zi_i32 - zj_i32), cSim.mint32_inv_scale.z);
+        PMEFloat r2  = xij*xij + yij*yij + zij*zij;
+        unsigned int ljIndex = LJIDi + __SHFL(activeMask, LJIDj, j);
+        PMEFloat qiqj = qi * __SHFL(activeMask, qi, j);
+        PMEFloat df = (PMEFloat)0.0;
+        PMEFloat2 term;
+        term.x = (PMEFloat)0.0;
+        term.y = (PMEFloat)0.0;
+        PMEFloat r2inv = (PMEFloat)0.0;
+        PMEFloat swtch_val = (PMEFloat)0.0;
+        PMEFloat dswtch_val = (PMEFloat)0.0;
+        int sampleExclusion = (exclusion & 0x1);
+#if defined(use_SPFP) && !defined(PME_FSWITCH) && !defined(PME_ENERGY)
+        PMEFloat4 coef;
+        coef.x = 0.0f;
+        coef.y = 0.0f;
+        coef.z = 0.0f;
+        coef.w = 0.0f;
+#endif
+        int valid_j = __SHFL(activeMask, validAtom, j);
+        Mint32ChecksumAttempt(validAtom, valid_j);
+        int inrange = (validAtom && valid_j && (r2 < cSim.cut2) && (r2 > delta));
+#ifdef MINT32_DEBUG_SAMPLE
+        // activeMask already captured above
+#endif
+#ifdef PHMD
+        int g = __SHFL(activeMask, h, j);
+        int indexj = __SHFL(activeMask, indexi, j);
+        PMEFloat qg = __SHFL(activeMask, qh, j);
+        PMEFloat qxg = __SHFL(activeMask, qxh, j);
+        qj_phmd = __SHFL(activeMask, qi_phmd, j);
+        psp_grp_g = __SHFL(activeMask, psp_grp, j);
+        xg = __SHFL(activeMask, xh, j);
+        dxg = __SHFL(activeMask, dxh, j);
+        factg = __SHFL(activeMask, facth, j);
+        lgtauto = __SHFL(activeMask, lhtauto, j);
+        lgtitr = __SHFL(activeMask, lhtitr, j);
+#endif
+        if (inrange) {
+          //Mint32ChecksumAccumulate(qiqj, r2, exclusion);
+#if defined(use_SPFP) && !defined(PME_FSWITCH) && !defined(PME_ENERGY)
+          r2inv = (PMEFloat)1.0 / r2;
+          uint cidx = 2*(__float_as_uint(r2) >> 18) + (exclusion & 0x1);
+#  if defined(__CUDA_ARCH__) && ((__CUDA_ARCH__ == 700) || (__CUDA_ARCH__ >= 800))
+          coef = cSim.pErfcCoeffsTable[cidx];
+#  else
+          coef = tex1Dfetch<float4>(cSim.texErfcCoeffsTable, cidx);
+#  endif
+#  ifdef PHMD
+          PMEFloat r     = sqrt(r2);
+          PMEFloat rinv  = (PMEFloat)1.0 / r;
+#  endif
+#else
+          PMEFloat rinv  = rsqrt(r2);
+          PMEFloat r     = r2 * rinv;
+          r2inv = rinv * rinv;
+#endif
+#ifndef use_DPFP
+#  if defined(__CUDA_ARCH__) && ((__CUDA_ARCH__ == 700) || (__CUDA_ARCH__ >= 800))
+          PMEFloat2 term = cSim.pLJTerm[ljIndex];
+#  else
+          PMEFloat2 term = tex1Dfetch<float2>(cSim.texLJTerm, ljIndex);
+#  endif
+#else
+          PMEFloat2 term = cSim.pLJTerm[ljIndex];
+#endif
+          PMEFloat r6inv = r2inv * r2inv * r2inv;
+#if defined(use_SPFP) && !defined(PME_FSWITCH) && !defined(PME_ENERGY)
+          PMEFloat d_swtch_dx = r2*coef.x + coef.y + r2inv*(coef.z + r2inv*coef.w);
+#  ifdef PHMD
+#    ifdef use_DPFP
+          PMEFloat swtch = erfc(cSim.ew_coeffSP * r);
+#    else
+          PMEFloat swtch = fasterfc(r);
+#    endif
+#  endif
+#else
+#  ifdef use_DPFP
+          PMEFloat swtch = erfc(cSim.ew_coeffSP * r);
+#  else
+          PMEFloat swtch = fasterfc(r);
+#  endif
+          PMEFloat d_swtch_dx = cSim.negTwoEw_coeffRsqrtPI * exp(-cSim.ew_coeff2 * r2);
+          swtch_val = swtch;
+          dswtch_val = d_swtch_dx;
+#endif
+#if defined(PME_ENERGY) || defined(PHMD)
+          PMEFloat fnrange = (PMEFloat)inrange;
+          // Skip last "inactive" interactions
+          if (ift * GRID + tgx >= totalInteractions) {
+            fnrange = (PMEFloat)0.0;
+          }
+#endif
+#ifdef PME_FSWITCH
+          if (!(exclusion & 0x1)) {
+            PMEFloat r3inv = rinv*r2inv;
+            PMEFloat r12inv = r6inv*r6inv;
+            PMEFloat df12f = (PMEFloat)(-1) * term.x * cSim.cut6invcut6minfswitch6 * r2inv *
+                             r6inv * (r6inv - cSim.cut6inv);
+            PMEFloat df6f = (PMEFloat)(-1) * term.y * cSim.cut3invcut3minfswitch3 * r3inv *
+                            r2inv * (r3inv - cSim.cut3inv);
+            PMEFloat df12 = (PMEFloat)(-1) * term.x * r2inv * r12inv;
+            PMEFloat df6 = (PMEFloat)(-1) * term.y * r2inv * r6inv;
+#  if defined(PME_ENERGY) || defined(PHMD)
+            PMEFloat f12f = term.x * cSim.cut6invcut6minfswitch6 *
+                            (r6inv - cSim.cut6inv) * (r6inv - cSim.cut6inv);
+            PMEFloat f6f = term.y * cSim.cut3invcut3minfswitch3 *
+                           (r3inv - cSim.cut3inv) * (r3inv - cSim.cut3inv);
+            PMEFloat f12 = term.x*r12inv - term.x*cSim.invfswitch6cut6;
+            PMEFloat f6 = term.y*r6inv - term.y*cSim.invfswitch3cut3;
+#  endif
+            if (r2 > cSim.fswitch2) {
+              df12 = df12f;
+              df6 = df6f;
+#  ifdef PME_ENERGY
+              f12 = f12f;
+              f6 = f6f;
+#  endif
+            }
+#  ifdef PHMD
+            dfphmd = df6 - df12;
+            de = fnrange * (f12 * twelvth - f6 * sixth);
+#  endif
+#  ifdef PME_ENERGY
+            PMEFloat e_lj = (f12*twelvth - f6*sixth);
+            TLevdw += fnrange * e_lj;
+#  endif
+            df += df6 - df12;
+          }
+#else  // PME_FSWITCH
+          if (!(exclusion & 0x1)) {
+            PMEFloat f6 = term.y * r6inv;
+            PMEFloat f12 = term.x * r6inv * r6inv;
+            df += (f12 - f6) * r2inv;
+#  ifdef PHMD
+            dfphmd = (f12 - f6) * r2inv;
+            de = fnrange * (f12 * twelvth - f6 * sixth);
+#  endif
+#  ifdef PME_ENERGY
+            PMEFloat e_lj = (f12*twelvth - f6*sixth);
+            TLevdw += fnrange * e_lj;
+#  endif
+          }
+#endif // PME_FSWITCH
+#if defined(use_DPFP) || defined(PME_FSWITCH) || defined(PME_ENERGY) || defined(PHMD)
+          else {
+            swtch -= (PMEFloat)1.0;
+          }
+#endif
+          // This ends a branch for "not an exclusion"--the non-bonded interaction is
+          // to be counted.  0x1 is simply 1 in hexadecimal.
+
+#ifdef PHMD
+          //First
+          dudlj = 0;
+          dudljplus = 0;
+          factphmd = 1.0;
+          if (!(exclusion & 0x1)) {
+            if (lhtitr && lgtitr) {
+              if (h != g) {
+                factphmd = facth * factg;
+                dudli -= xh * factg * de;
+                dudlj -= xg * facth * de;
+                if (lhtauto) {
+                  dudliplus += dxh * factg * de;
+                }
+                if (lgtauto) {
+                  dudljplus += dxg * facth * de;
+                }
+              }
+              else if (psp_grp == 1) {
+                factphmd = 1.0 - lambdah;
+                dudli -= de;
+              }
+            }
+            else if (lhtitr) {
+              factphmd = facth;
+              dudli -= xh * de;
+              if (lhtauto) {
+                dudliplus += dxh * de;
+              }
+            }
+            else if (lgtitr) {
+              factphmd = factg;
+              dudlj -= xg * de;
+              if (lgtauto) {
+                dudljplus += dxg * de;
+              }
+            }
+            df += (factphmd - 1.0) * dfphmd;
+#  ifdef PME_ENERGY
+            TLevdw += (factphmd - 1.0) * de;
+#   endif
+          }
+          if (h >= 0) {
+            dudli += qj_phmd * qh * swtch * rinv * fnrange;
+            if (psp_grp > 0) {
+              dudliplus += qj_phmd * qxh * swtch * rinv * fnrange;
+            }
+          }
+          if (g >= 0) {
+            dudlj += qi_phmd * qg * swtch * rinv * fnrange;
+            if (psp_grp_g > 0) {
+              dudljplus += qi_phmd * qxg * swtch * rinv * fnrange;
+          }
+          atomicAdd((unsigned long long int*)&cSim.pdph_accumulator[indexj],
+                    llitoulli(fast_llrintf(dudlj * FORCESCALEF)));
+          atomicAdd((unsigned long long int*)&cSim.pdph_plus_accumulator[indexj],
+                    llitoulli(fast_llrintf(dudljplus * FORCESCALEF)));
+        }
+#endif
+#ifdef PME_ENERGY
+          PMEFloat b0 = qiqj * swtch * rinv;
+          PMEFloat b1 = b0 - qiqj * d_swtch_dx;
+          df += b1 * r2inv;
+          TLeed += fnrange * b0;
+#else  // PME_ENERGY
+#  if defined(use_SPFP) && !defined(PME_FSWITCH)
+          df += qiqj * d_swtch_dx;
+#  else
+          df += qiqj * (swtch * rinv - d_swtch_dx) * r2inv;
+#  endif
+#endif // PME_ENERGY
+#if !defined(use_DPFP) && defined(PME_MINIMIZATION)
+          df = max(-10000.0f, min(df, 10000.0f));
+#endif
+        } // inrange
+        // Skip last "inactive" interactions
+        if (ift * GRID + tgx >= totalInteractions) {
+          df = (PMEFloat)0.0;
+        }
+        PMEFloat dfdx = df * xij;
+        PMEFloat dfdy = df * yij;
+        PMEFloat dfdz = df * zij;
+
+        // Accumulate into registers for i atoms only, but accumulate
+        // both the action and the equal and opposite reaction
+        TLx_i += dfdx;
+        TLy_i += dfdy;
+        TLz_i += dfdz;
+        TLx_i -= __SHFL(activeMask, dfdx, jrec);
+        TLy_i -= __SHFL(activeMask, dfdy, jrec);
+        TLz_i -= __SHFL(activeMask, dfdz, jrec);
+#ifdef PME_VIRIAL
+        TLvir_11 -= xij * dfdx;
+        TLvir_22 -= yij * dfdy;
+        TLvir_33 -= zij * dfdz;
+#endif
+
+        // Shift bits one to the right in the exclusion tracker to move on to the next atom.
+        exclusion >>= 1;
+        j = ((j + 1) & cSim.NLAtomsPerWarpBitsMask) | joffset;
+        jrec = ((jrec + PME_ATOMS_PER_WARP - 1) & cSim.NLAtomsPerWarpBitsMask) | joffset;
+      }
+
+      // Commit tile accumulators
+#ifdef use_SPFP
+      fx_i += fast_llrintf(TLx_i * FORCESCALEF);
+      fy_i += fast_llrintf(TLy_i * FORCESCALEF);
+      fz_i += fast_llrintf(TLz_i * FORCESCALEF);
+#else
+#  ifdef PME_MINIMIZATION
+      PMEFloat i;
+      TLx_i = modf(TLx_i, &i);
+      fxe_i += llrint(i);
+      TLy_i = modf(TLy_i, &i);
+      fye_i += llrint(i);
+      TLz_i = modf(TLz_i, &i);
+      fze_i += llrint(i);
+#  endif
+      fx_i += llrint(TLx_i * FORCESCALE);
+      fy_i += llrint(TLy_i * FORCESCALE);
+      fz_i += llrint(TLz_i * FORCESCALE);
+#endif
+
+#ifdef PME_ENERGY
+      // The factor of 1/2 was folded into the Lennard-Jones calculation
+      // above, but not the electrostatic calculation.
+#  ifdef use_SPFP
+      eed  += fast_llrintf(TLeed * ENERGYSCALEF);
+      evdw += fast_llrintf(TLevdw * ENERGYSCALEF);
+#  else
+#    ifdef PME_MINIMIZATION
+      TLevdw = modf(TLevdw, &i);
+      evdwe += llrint(i);
+      TLeed = modf(TLeed, &i);
+      eede += llrint(i);
+#    endif
+      evdw += llrint(TLevdw * ENERGYSCALE);
+      eed += llrint(TLeed * ENERGYSCALE);
+#  endif
+#endif
+
+#ifdef PME_VIRIAL
+#  ifdef use_SPFP
+      vir_11 += fast_llrintf(TLvir_11 * FORCESCALEF);
+      vir_22 += fast_llrintf(TLvir_22 * FORCESCALEF);
+      vir_33 += fast_llrintf(TLvir_33 * FORCESCALEF);
+#  else
+#    if defined(PME_MINIMIZATION)
+      TLvir_11 = modf(TLvir_11, &i);
+      vir_11E += llrint(i);
+      TLvir_22 = modf(TLvir_22, &i);
+      vir_22E += llrint(i);
+      TLvir_33 = modf(TLvir_33, &i);
+      vir_33E += llrint(i);
+#    endif
+      vir_11 += llrint(TLvir_11 * FORCESCALE);
+      vir_22 += llrint(TLvir_22 * FORCESCALE);
+      vir_33 += llrint(TLvir_33 * FORCESCALE);
+#  endif
+#endif
+    }//if (psWarp->bHomeCell)
+    else {
+      LJIDi *= cSim.LJTypes;
+    }
+    // This ends the branch for special-casing the first tile to have
+    // 32 atoms interact with each other.  LJIDi, the LJ index for the
+    // ith atom, has to be set for future operations, no matter what.
+
+    // Handle the remainder of the line
+    int tx = 0;
+    while (tx < psWarp->nlEntry.NL.xatoms) {
+
+      // Read atom ID and exclusion data
+      NLAtom shAtom;
+      shAtom.ID = cSim.pNLAtomList[offset + tgx];
+      offset += GRID;
+      PMEMask fullExclusion =
+        ((PMEMask*)&cSim.pNLAtomList[offset])[tgx & cSim.NLAtomsPerWarpBitsMask];
+      offset += cSim.NLAtomsPerWarp * sizeof(PMEMask) / sizeof(unsigned int);
+      if (PME_ATOMS_PER_WARP < GRID) {
+        if ((offset & (GRID - 1)) == 0) {
+          offset += GRID - PME_ATOMS_PER_WARP;
+        }
+      }
+      unsigned int exclusion = (unsigned int)fullExclusion & cSim.NLAtomsPerWarpMask;
+      // Clear j atom forces
+      PMEFloat shFx = (PMEFloat)0.0;
+      PMEFloat shFy = (PMEFloat)0.0;
+      PMEFloat shFz = (PMEFloat)0.0;
+
+      // Read shared memory data
+      if (tx + tgx < psWarp->nlEntry.NL.xatoms) {
+        unsigned int atom = shAtom.ID >> NLATOM_CELL_SHIFT;
+#ifdef PHMD
+        shAtom.indexj = atom;
+        shAtom.g = cSim.pImageGrplist[atom] - 1;
+        shAtom.qg = 0;
+        shAtom.qxg = 0;
+        shAtom.lgtitr = false;
+        shAtom.qj_phmd = cSim.pImageCharge_phmd[atom];
+        if (shAtom.g >= 0) {
+          pqstate1 = cSim.pImageQstate1[atom];
+          pqstate2 = cSim.pImageQstate2[atom];
+          shAtom.psp_grp_g = cSim.psp_grp[shAtom.g];
+          vstate1 = cSim.pvstate1[cSim.pImageIndexPHMD[atom]];
+          radh = vstate1.x;
+          lambda = sin(cSim.pph_theta[shAtom.g]);
+          lambda *= lambda;
+          shAtom.factg = 1.0 - lambda;
+          shAtom.xg = 1.0;
+          if (shAtom.psp_grp_g > 0) {
+            shAtom.lgtauto = true;
+            x2 = sin(cSim.pph_theta[shAtom.g+1]);
+            x2 *= x2;
+            radh += vstate1.y;
+            if (vstate1.x > 0) {
+              shAtom.igtaut = 1.0;
+              shAtom.xg = x2;
+            }
+            else if (vstate1.y > 0) {
+              shAtom.igtaut = -1.0;
+              shAtom.xg = 1.0 - x2;
+            }
+            if (shAtom.psp_grp_g == 1) {
+              shAtom.factg = 1.0 - lambda * shAtom.xg;
+              shAtom.dxg = -shAtom.igtaut * lambda;
+            }
+            else if (shAtom.psp_grp_g == 3) {
+              shAtom.factg = (1.0 - lambda) * shAtom.xg;
+              shAtom.dxg = shAtom.igtaut * (1.0 - lambda);
+            }
+            qunprot = lambda * (pqstate2.x - pqstate2.y);
+            qprot = (1 - lambda) * (pqstate1.x - pqstate1.y);
+            shAtom.qxg = qunprot + qprot;
+          }
+          shAtom.lgtitr = (radh > 0);
+          qunprot = x2 * pqstate2.x + (1.0 - x2) * pqstate2.y;
+          qprot = x2 * pqstate1.x + (1.0 - x2) * pqstate1.y;
+          shAtom.qg = qunprot - qprot;
+        }
+#endif
+#ifndef use_DPFP
+#  if defined(__CUDA_ARCH__) && ((__CUDA_ARCH__ == 700) || (__CUDA_ARCH__ >= 800))
+        PMEFloat2 qljid = cSim.pAtomChargeSPLJID[atom];
+#  else
+        PMEFloat2 qljid = tex1Dfetch<float2>(cSim.texAtomChargeSPLJID, atom);
+#  endif
+#else
+        PMEFloat2 qljid = cSim.pAtomChargeSPLJID[atom];
+#endif
+        int4 coord = cSim.pAtomCoord_M32[atom];
+  shAtom.xi32 = coord.x;
+  shAtom.yi32 = coord.y;
+  shAtom.zi32 = coord.z;
+        shAtom.q = qljid.x;
+#ifdef use_DPFP
+        shAtom.LJID = __double_as_longlong(qljid.y);
+#else
+        shAtom.LJID = __float_as_uint(qljid.y);
+#endif
+  shAtom.valid = 1;
+      }
+      else {
+        shAtom.q = (PMEFloat)0.0;
+        shAtom.LJID = 0;
+  shAtom.xi32 = 0;
+  shAtom.yi32 = 0;
+  shAtom.zi32 = 0;
+  shAtom.valid = 0;
+      }
+
+      // Translate all atoms into a local coordinate system within one unit
+      // cell of the first atom read to avoid PBC handling within inner loops
+      int cell = shAtom.ID & NLATOM_CELL_TYPE_MASK;
+
+
+#ifdef PME_ENERGY
+      PMEFloat TLeed  = (PMEFloat)0.0;
+      PMEFloat TLevdw = (PMEFloat)0.0;
+#endif
+#ifdef PME_VIRIAL
+      PMEFloat TLvir_11 = (PMEFloat)0.0;
+      PMEFloat TLvir_22 = (PMEFloat)0.0;
+      PMEFloat TLvir_33 = (PMEFloat)0.0;
+#endif
+      // Initialize tile-specific accumulators
+      PMEFloat TLx_i  = (PMEFloat)0.0;
+      PMEFloat TLy_i  = (PMEFloat)0.0;
+      PMEFloat TLz_i  = (PMEFloat)0.0;
+
+      // Initialize iterators: j indicates that this thread should seek out the
+      // "j atom" atom owned by thread j, whereas jrec indicates that thread jrec
+      // will have information to contribute to the "j atom" owned by this thread.
+      int j = tgx;
+      int jrec = tgx;
+      // MINT32 FIX: Use activemask for exclusion handling
+      __syncwarp(0xffffffff); // Ensure full warp convergence
+      unsigned int exclMask = __activemask();
+      if (__ANY(exclMask, exclusion)) {
+        #pragma unroll 2
+        for (int it = 0; it < PME_ATOMS_PER_WARP; it++) {
+
+          int valid_j = __SHFL(exclMask, shAtom.valid, j);
+          PMEFloat xij = (PMEFloat)0.0;
+          PMEFloat yij = (PMEFloat)0.0;
+          PMEFloat zij = (PMEFloat)0.0;
+          int xj32 = __SHFL(exclMask, shAtom.xi32, j);
+          int yj32 = __SHFL(exclMask, shAtom.yi32, j);
+          int zj32 = __SHFL(exclMask, shAtom.zi32, j);
+          if (validAtom && valid_j) {
+            xij = M32_DELTA_TO_FLOAT((xi_i32 - xj32), cSim.mint32_inv_scale.x);
+            yij = M32_DELTA_TO_FLOAT((yi_i32 - yj32), cSim.mint32_inv_scale.y);
+            zij = M32_DELTA_TO_FLOAT((zi_i32 - zj32), cSim.mint32_inv_scale.z);
+          }
+            PMEFloat r2  = xij*xij + yij*yij + zij*zij;
+          unsigned int index = LJIDi + __SHFL(exclMask, shAtom.LJID, j);
+          PMEFloat qiqj = qi * __SHFL(exclMask, shAtom.q, j);
+          PMEFloat df = (PMEFloat)0.0;
+          bool inrange = (validAtom && valid_j && (r2 < cSim.cut2) && (r2 > delta));
+          //Mint32ChecksumAttempt(validAtom, valid_j);
+#ifdef PHMD
+          int g = __SHFL(exclMask, shAtom.g, j);
+          PMEFloat qg = __SHFL(exclMask, shAtom.qg, j);
+          PMEFloat qxg = __SHFL(exclMask, shAtom.qxg, j);
+          int indexj = __SHFL(exclMask, shAtom.indexj, j);
+          qj_phmd = __SHFL(exclMask, shAtom.qj_phmd, j);
+          psp_grp_g = __SHFL(exclMask, shAtom.psp_grp_g, j);
+          xg = __SHFL(exclMask, shAtom.xg, j);
+          dxg = __SHFL(exclMask, shAtom.dxg, j);
+          factg = __SHFL(exclMask, shAtom.factg, j);
+          lgtauto = __SHFL(exclMask, shAtom.lgtauto, j);
+          lgtitr = __SHFL(exclMask, shAtom.lgtitr, j);
+#endif
+          if (inrange) {
+            //Mint32ChecksumAccumulate(qiqj, r2, exclusion);
+#if defined(use_SPFP) && !defined(PME_FSWITCH) && !defined(PME_ENERGY)
+            PMEFloat r2inv = (PMEFloat)1.0 / r2;
+            uint cidx = 2*(__float_as_uint(r2) >> 18) + (exclusion & 0x1);
+#  if defined(__CUDA_ARCH__) && ((__CUDA_ARCH__ == 700) || (__CUDA_ARCH__ >= 800))
+            PMEFloat4 coef = cSim.pErfcCoeffsTable[cidx];
+#  else
+            PMEFloat4 coef = tex1Dfetch<float4>(cSim.texErfcCoeffsTable, cidx);
+#  endif
+#  ifdef PHMD
+          PMEFloat r     = sqrt(r2);
+          PMEFloat rinv  = (PMEFloat)1.0 / r;
+#  endif
+#else
+            PMEFloat rinv      = rsqrt(r2);
+            PMEFloat r         = r2 * rinv;
+            PMEFloat r2inv     = rinv * rinv;
+#endif
+#ifndef use_DPFP
+#  if defined(__CUDA_ARCH__) && ((__CUDA_ARCH__ == 700) || (__CUDA_ARCH__ >= 800))
+            PMEFloat2 term = cSim.pLJTerm[index];
+#  else
+            PMEFloat2 term = tex1Dfetch<float2>(cSim.texLJTerm, index);
+#  endif
+#else
+            PMEFloat2 term = cSim.pLJTerm[index];
+#endif
+            PMEFloat r6inv     = r2inv * r2inv * r2inv;
+#if defined(use_SPFP) && !defined(PME_FSWITCH) && !defined(PME_ENERGY)
+            PMEFloat d_swtch_dx = r2*coef.x + coef.y + r2inv*(coef.z + r2inv*coef.w);
+#  ifdef PHMD
+#    ifdef use_DPFP
+            PMEFloat swtch = erfc(cSim.ew_coeffSP * r);
+#    else
+            PMEFloat swtch = fasterfc(r);
+#    endif
+#  endif
+#else
+#  ifdef use_DPFP
+            PMEFloat swtch = erfc(cSim.ew_coeffSP * r);
+#  else
+            PMEFloat swtch = fasterfc(r);
+#  endif
+            PMEFloat d_swtch_dx = cSim.negTwoEw_coeffRsqrtPI * exp(-cSim.ew_coeff2 * r2);
+#endif
+#if defined(PME_ENERGY) || defined(PHMD)
+            PMEFloat fnrange = (PMEFloat)inrange;
+#endif
+#ifdef PME_FSWITCH
+            if (!(exclusion & 0x1)) {
+              PMEFloat r3inv = rinv*r2inv;
+              PMEFloat r12inv = r6inv*r6inv;
+              PMEFloat df12f = (PMEFloat)(-1) * term.x * cSim.cut6invcut6minfswitch6 *
+                               r2inv * r6inv * (r6inv - cSim.cut6inv);
+              PMEFloat df6f = (PMEFloat)(-1) * term.y * cSim.cut3invcut3minfswitch3 *
+                              r3inv * r2inv * (r3inv - cSim.cut3inv);
+              PMEFloat df12 = (PMEFloat)(-1) * term.x * r2inv * r12inv;
+              PMEFloat df6 = (PMEFloat)(-1) * term.y * r2inv * r6inv;
+#  if defined(PME_ENERGY) || defined(PHMD)
+              PMEFloat f12f = term.x * cSim.cut6invcut6minfswitch6 *
+                              (r6inv - cSim.cut6inv)*(r6inv - cSim.cut6inv);
+              PMEFloat f6f = term.y * cSim.cut3invcut3minfswitch3 *
+                             (r3inv - cSim.cut3inv)*(r3inv - cSim.cut3inv);
+              PMEFloat f12 = (term.x * r12inv) - (term.x * cSim.invfswitch6cut6);
+              PMEFloat f6 = (term.y * r6inv) - (term.y * cSim.invfswitch3cut3);
+#  endif
+              if (r2 > cSim.fswitch2) {
+                df12 = df12f;
+                df6 = df6f;
+#  ifdef PME_ENERGY
+                f12 = f12f;
+                f6 = f6f;
+#  endif
+              }
+              df += df6 - df12;
+#ifdef PHMD
+              dfphmd = df6 - df12;
+              de = fnrange * (f12 * twelvth - f6 * sixth);
+#endif
+#  ifdef PME_ENERGY
+              TLevdw += fnrange * (f12*twelvth - f6*sixth);
+#  endif
+            }
+#else  // PME_FSWITCH
+            if (!(exclusion & 0x1)) {
+              PMEFloat f6 = term.y * r6inv;
+              PMEFloat f12 = term.x * r6inv * r6inv;
+              df += (f12 - f6) * r2inv;
+#ifdef PHMD
+              dfphmd = (f12 - f6) * r2inv;
+              de = fnrange * (f12 * twelvth - f6 * sixth);
+#endif
+#  ifdef PME_ENERGY
+              TLevdw += fnrange * (f12*twelvth - f6*sixth);
+#  endif
+            }
+#endif // PME_FSWITCH
+#if defined(use_DPFP) || defined(PME_FSWITCH) || defined(PME_ENERGY) || defined(PHMD)
+            else {
+              swtch -= (PMEFloat)1.0;
+            }
+#endif
+#ifdef PHMD
+            //Second
+            dudlj = 0;
+            dudljplus = 0;
+            factphmd = 1.0;
+            if (!(exclusion & 0x1)) {
+              if (lhtitr && lgtitr) {
+                if (h != g) {
+                  factphmd = facth * factg;
+                  dudli -= xh * factg * de;
+                  dudlj -= xg * facth * de;
+                  if (lhtauto) {
+                    dudliplus += dxh * factg * de;
+                  }
+                  if (lgtauto) {
+                    dudljplus += dxg * facth * de;
+                  }
+                }
+                else if (psp_grp == 1) {
+                  factphmd = 1.0 - lambdah;
+                  dudli -= de;
+                }
+              }
+              else if (lhtitr) {
+                factphmd = facth;
+                dudli -= xh * de;
+                if (lhtauto) {
+                  dudliplus += dxh * de;
+                }
+              }
+              else if (lgtitr) {
+                factphmd = factg;
+                dudlj -= xg * de;
+                if (lgtauto) {
+                  dudljplus += dxg * de;
+                }
+              }
+              df += (factphmd - 1.0) * dfphmd;
+#  ifdef PME_ENERGY
+              TLevdw += (factphmd - 1.0) * de;
+#   endif
+          }
+          if (h >= 0) {
+            dudli += qj_phmd * qh * swtch * rinv * fnrange;
+            if (psp_grp > 0) {
+              dudliplus += qj_phmd * qxh * swtch * rinv * fnrange;
+            }
+          }
+          if (g >= 0) {
+            dudlj += qi_phmd * qg * swtch * rinv * fnrange;
+            if (psp_grp_g > 0) {
+              dudljplus += fnrange * qi_phmd * qxg * swtch * rinv;
+            }
+            atomicAdd((unsigned long long int*)&cSim.pdph_accumulator[indexj],
+                      llitoulli(fast_llrintf(dudlj * FORCESCALEF)));
+            atomicAdd((unsigned long long int*)&cSim.pdph_plus_accumulator[indexj],
+                      llitoulli(fast_llrintf(dudljplus * FORCESCALEF)));
+          }
+#endif
+#ifdef PME_ENERGY
+            PMEFloat b0 = qiqj * swtch * rinv;
+            PMEFloat b1 = b0 - qiqj * d_swtch_dx;
+            df += b1 * r2inv;
+            TLeed += fnrange * b0;
+#else  // PME_ENERGY
+#  if defined(use_SPFP) && !defined(PME_FSWITCH)
+            df += qiqj * d_swtch_dx;
+#  else
+            df += qiqj*(swtch*rinv - d_swtch_dx)*r2inv;
+#  endif
+#endif // PME_ENERGY
+#if !defined(use_DPFP) && defined(PME_MINIMIZATION)
+            df = max(-10000.0f, min(df, 10000.0f));
+#endif
+          }
+          PMEFloat dfdx = df * xij;
+          PMEFloat dfdy = df * yij;
+          PMEFloat dfdz = df * zij;
+          TLx_i += dfdx;
+          TLy_i += dfdy;
+          TLz_i += dfdz;
+          shFx -= __SHFL(exclMask, dfdx, jrec);
+          shFy -= __SHFL(exclMask, dfdy, jrec);
+          shFz -= __SHFL(exclMask, dfdz, jrec);
+#ifdef PME_VIRIAL
+          TLvir_11 -= xij * dfdx;
+          TLvir_22 -= yij * dfdy;
+          TLvir_33 -= zij * dfdz;
+#endif
+          exclusion >>= 1;
+          j = ((j + 1) & cSim.NLAtomsPerWarpBitsMask) | joffset;
+          jrec = ((jrec + PME_ATOMS_PER_WARP - 1) & cSim.NLAtomsPerWarpBitsMask) | joffset;
+        }
+        // End for loop covering non-bonded computations when
+        // there IS at least one exclusion somewhere in the pile
+      }
+      else {
+        #pragma unroll 2
+        for (int it = 0; it < PME_ATOMS_PER_WARP; it++) {
+
+          int valid_j = __SHFL(exclMask, shAtom.valid, j);
+          PMEFloat xij = (PMEFloat)0.0;
+          PMEFloat yij = (PMEFloat)0.0;
+          PMEFloat zij = (PMEFloat)0.0;
+          int xj32 = __SHFL(exclMask, shAtom.xi32, j);
+          int yj32 = __SHFL(exclMask, shAtom.yi32, j);
+          int zj32 = __SHFL(exclMask, shAtom.zi32, j);
+          if (validAtom && valid_j) {
+            xij = M32_DELTA_TO_FLOAT((xi_i32 - xj32), cSim.mint32_inv_scale.x);
+            yij = M32_DELTA_TO_FLOAT((yi_i32 - yj32), cSim.mint32_inv_scale.y);
+            zij = M32_DELTA_TO_FLOAT((zi_i32 - zj32), cSim.mint32_inv_scale.z);
+          }
+#ifdef PHMD
+          int g = __SHFL(exclMask, shAtom.g, j);
+          PMEFloat qg = __SHFL(exclMask, shAtom.qg, j);
+          PMEFloat qxg = __SHFL(exclMask, shAtom.qxg, j);
+          int indexj = __SHFL(exclMask, shAtom.indexj, j);
+          qj_phmd = __SHFL(exclMask, shAtom.qj_phmd, j);
+          psp_grp_g = __SHFL(exclMask, shAtom.psp_grp_g, j);
+          xg = __SHFL(exclMask, shAtom.xg, j);
+          dxg = __SHFL(exclMask, shAtom.dxg, j);
+          factg = __SHFL(exclMask, shAtom.factg, j);
+          lgtauto = __SHFL(exclMask, shAtom.lgtauto, j);
+          lgtitr = __SHFL(exclMask, shAtom.lgtitr, j);
+#endif
+          // Perform the range test
+          PMEFloat r2  = xij*xij + yij*yij + zij*zij;
+          unsigned int index = LJIDi + __SHFL(exclMask, shAtom.LJID, j);
+          PMEFloat qiqj = qi * __SHFL(exclMask, shAtom.q, j);
+          bool inrange = (validAtom && valid_j && (r2 < cSim.cut2) && (r2 > delta));
+          Mint32ChecksumAttempt(validAtom, valid_j);
+          PMEFloat df = (PMEFloat)0.0;
+          if (inrange) {
+            //Mint32ChecksumAccumulate(qiqj, r2, exclusion);
+#if defined(use_SPFP) && !defined(PME_FSWITCH) && !defined(PME_ENERGY)
+            PMEFloat r2inv = (PMEFloat)1.0 / r2;
+            uint cidx = 2*(__float_as_uint(r2) >> 18);
+#  if defined(__CUDA_ARCH__) && ((__CUDA_ARCH__ == 700) || (__CUDA_ARCH__ >= 800))
+            PMEFloat4 coef = cSim.pErfcCoeffsTable[cidx];
+#  else
+            PMEFloat4 coef = tex1Dfetch<float4>(cSim.texErfcCoeffsTable, cidx);
+#  endif
+#  ifdef PHMD
+          PMEFloat r     = sqrt(r2);
+          PMEFloat rinv  = (PMEFloat)1.0 / r;
+#  endif
+#else
+            PMEFloat rinv      = rsqrt(r2);
+            PMEFloat r         = r2 * rinv;
+            PMEFloat r2inv     = rinv * rinv;
+#endif
+#ifndef use_DPFP
+#  if defined(__CUDA_ARCH__) && ((__CUDA_ARCH__ == 700) || (__CUDA_ARCH__ >= 800))
+            PMEFloat2 term = cSim.pLJTerm[index];
+#  else
+            PMEFloat2 term = tex1Dfetch<float2>(cSim.texLJTerm, index);
+#  endif
+#else
+            PMEFloat2 term = cSim.pLJTerm[index];
+#endif
+            PMEFloat r6inv     = r2inv * r2inv * r2inv;
+#if defined(use_SPFP) && !defined(PME_FSWITCH) && !defined(PME_ENERGY)
+            PMEFloat d_swtch_dx = r2*coef.x + coef.y + r2inv*(coef.z + r2inv*coef.w);
+#  ifdef PHMD
+#    ifdef use_DPFP
+            PMEFloat swtch = erfc(cSim.ew_coeffSP * r) * rinv;
+#    else
+            PMEFloat swtch = fasterfc(r) * rinv;
+#    endif
+#  endif
+#else
+#  ifdef use_DPFP
+            PMEFloat swtch = erfc(cSim.ew_coeffSP * r) * rinv;
+#  else
+            PMEFloat swtch = fasterfc(r) * rinv;
+#  endif
+            PMEFloat d_swtch_dx = cSim.negTwoEw_coeffRsqrtPI * exp(-cSim.ew_coeff2 * r2);
+#endif
+#if defined(PME_ENERGY) || defined(PHMD)
+            PMEFloat fnrange = (PMEFloat)inrange;
+#endif
+#ifdef PME_FSWITCH
+            PMEFloat r3inv = rinv*r2inv;
+            PMEFloat r12inv = r6inv*r6inv;
+            PMEFloat df12f = (PMEFloat)(-1) * term.x * cSim.cut6invcut6minfswitch6 *
+                             r2inv * r6inv * (r6inv - cSim.cut6inv);
+            PMEFloat df6f = (PMEFloat)(-1) * term.y * cSim.cut3invcut3minfswitch3 *
+                            r3inv * r2inv * (r3inv - cSim.cut3inv);
+            PMEFloat df12 = (PMEFloat)(-1) * term.x * r2inv * r12inv;
+            PMEFloat df6 = (PMEFloat)(-1) * term.y * r2inv * r6inv;
+#  if defined(PME_ENERGY) || defined(PHMD)
+            PMEFloat f12f = term.x * cSim.cut6invcut6minfswitch6 *
+                            (r6inv - cSim.cut6inv)*(r6inv - cSim.cut6inv);
+            PMEFloat f6f = term.y * cSim.cut3invcut3minfswitch3 *
+                           (r3inv - cSim.cut3inv)*(r3inv - cSim.cut3inv);
+            PMEFloat f12 = term.x * r12inv - term.x * cSim.invfswitch6cut6;
+            PMEFloat f6 = term.y * r6inv - term.y * cSim.invfswitch3cut3;
+#  endif
+            if (r2 > cSim.fswitch2) {
+              df12 = df12f;
+              df6 = df6f;
+#  ifdef PME_ENERGY
+              f12 = f12f;
+              f6 = f6f;
+#  endif
+            }
+            df += df6 - df12;
+#  ifdef PHMD
+            //Third
+          dudlj = 0;
+          dudljplus = 0;
+          de = fnrange * (f12 * twelvth - f6 * sixth);
+          factphmd = 1.0;
+          if (lhtitr && lgtitr) {
+            if (h != g) {
+              factphmd = facth * factg;
+              dudli -= xh * factg * de;
+              dudlj -= xg * facth * de;
+              if (lhtauto) {
+                dudliplus += dxh * factg * de;
+              }
+              if (lgtauto) {
+                dudljplus += dxg * facth * de;
+              }
+            }
+            else if (psp_grp == 1) {
+              factphmd = 1.0 - lambdah;
+              dudli -= de;
+            }
+          }
+          else if (lhtitr) {
+            factphmd = facth;
+            dudli -= xh * de;
+            if (lhtauto) {
+              dudliplus += dxh * de;
+            }
+          }
+          else if (lgtitr) {
+            factphmd = factg;
+            dudlj -= xg * de;
+            if (lgtauto) {
+              dudljplus += dxg * de;
+            }
+          }
+          df += (factphmd - 1.0) * (df6 - df12);
+#  ifdef PME_ENERGY
+          TLevdw += (factphmd - 1.0) * de;
+#   endif
+          if (h >= 0) {
+            dudli += qj_phmd * qh * swtch;
+            if (psp_grp > 0) {
+              dudliplus += qj_phmd * qxh * swtch;
+            }
+          }
+          if (g >= 0) {
+            dudlj += qi_phmd * qg * swtch;
+            if (psp_grp_g > 0) {
+              dudljplus += qi_phmd * qxg * swtch;
+            }
+            atomicAdd((unsigned long long int*)&cSim.pdph_accumulator[indexj],
+                      llitoulli(fast_llrintf(dudlj * FORCESCALEF)));
+            atomicAdd((unsigned long long int*)&cSim.pdph_plus_accumulator[indexj],
+                      llitoulli(fast_llrintf(dudljplus * FORCESCALEF)));
+          }
+#  endif
+#  ifdef PME_ENERGY
+            TLevdw += fnrange * (f12*twelvth - f6*sixth);
+            PMEFloat b0 = qiqj * swtch;
+            PMEFloat b1 = b0 - qiqj * d_swtch_dx;
+            df += b1 * r2inv;
+            TLeed += fnrange * b0;
+#  else  // PME_ENERGY
+            df += qiqj * (swtch - d_swtch_dx) * r2inv;
+#  endif // PME_ENERGY
+#else // PME_FSWITCH
+            PMEFloat f6 = term.y * r6inv;
+            PMEFloat f12 = term.x * r6inv * r6inv;
+            df += (f12-f6) * r2inv;
+#ifdef PHMD
+            //Fourth
+            dudlj = 0;
+            dudljplus = 0;
+            de = fnrange * (f12 * twelvth - f6 * sixth);
+            factphmd = 1.0;
+            if (lhtitr && lgtitr) {
+              if (h != g) {
+                factphmd = facth * factg;
+                dudli -= xh * factg * de;
+                dudlj -= xg * facth * de;
+                if (lhtauto) {
+                  dudliplus += dxh * factg * de;
+                }
+                if (lgtauto) {
+                  dudljplus += dxg * facth * de;
+                }
+              }
+              else if (psp_grp == 1) {
+                factphmd = 1.0 - lambdah;
+                dudli -= de;
+              }
+            }
+              else if (lhtitr) {
+                factphmd = facth;
+                dudli -= xh * de;
+                if (lhtauto) {
+                  dudliplus += dxh * de;
+                }
+              }
+              else if (lgtitr) {
+                factphmd = factg;
+                dudlj -= xg * de;
+                if (lgtauto) {
+                  dudljplus += dxg * de;
+                }
+              }
+              df += (factphmd - 1.0) * (f12 - f6) * r2inv;
+#  ifdef PME_ENERGY
+              TLevdw += (factphmd - 1.0) * de;
+#   endif
+            if (h >= 0) {
+              dudli += fnrange * qj_phmd * qh * swtch;
+              if (psp_grp > 0) {
+                dudliplus += fnrange * qj_phmd * qxh * swtch;
+              }
+            }
+            if (g >= 0) {
+              dudlj += fnrange * qi_phmd * qg * swtch;
+              if (psp_grp_g > 0) {
+                dudljplus += fnrange * qi_phmd * qxg * swtch;
+              }
+              atomicAdd((unsigned long long int*)&cSim.pdph_accumulator[indexj],
+                        llitoulli(fast_llrintf(dudlj * FORCESCALEF)));
+              atomicAdd((unsigned long long int*)&cSim.pdph_plus_accumulator[indexj],
+                        llitoulli(fast_llrintf(dudljplus * FORCESCALEF)));
+            }
+#endif
+#  ifdef PME_ENERGY
+            TLevdw += fnrange * (f12*twelvth - f6*sixth);
+            PMEFloat b0 = qiqj * swtch;
+            PMEFloat b1 = b0 - qiqj * d_swtch_dx;
+            df += b1 * r2inv;
+            TLeed += fnrange * b0;
+#  else  // PME_ENERGY
+#    if defined(use_SPFP) && !defined(PME_FSWITCH)
+            df += qiqj * d_swtch_dx;
+#    else
+            df += qiqj * (swtch - d_swtch_dx) * r2inv;
+#    endif
+#  endif // PME_ENERGY
+#endif //PME_FSWITCH
+#if !defined(use_DPFP) && defined(PME_MINIMIZATION)
+            df = max(-10000.0f, min(df, 10000.0f));
+#endif
+          }
+          PMEFloat dfdx = df * xij;
+          PMEFloat dfdy = df * yij;
+          PMEFloat dfdz = df * zij;
+          TLx_i += dfdx;
+          TLy_i += dfdy;
+          TLz_i += dfdz;
+          shFx -= __SHFL(exclMask, dfdx, jrec);
+          shFy -= __SHFL(exclMask, dfdy, jrec);
+          shFz -= __SHFL(exclMask, dfdz, jrec);
+#ifdef PME_VIRIAL
+          TLvir_11 -= xij * dfdx;
+          TLvir_22 -= yij * dfdy;
+          TLvir_33 -= zij * dfdz;
+#endif
+          j = ((j + 1) & cSim.NLAtomsPerWarpBitsMask) | joffset;
+          jrec = ((jrec + PME_ATOMS_PER_WARP - 1) & cSim.NLAtomsPerWarpBitsMask) | joffset;
+
+        }
+        // Ends for loop for processing a pile of non-bonded
+        // interactions with no exclusions to worry about
+      }
+      // End branch dealing with the presence of exclusions
+      // in the pile of non-bonded interactions.
+
+#ifdef use_SPFP
+      // Commit tile accumulators to registers
+      fx_i += fast_llrintf(TLx_i * FORCESCALEF);
+      fy_i += fast_llrintf(TLy_i * FORCESCALEF);
+      fz_i += fast_llrintf(TLz_i * FORCESCALEF);
+#else
+#  ifdef PME_MINIMIZATION
+      PMEFloat i;
+      TLx_i = modf(TLx_i, &i);
+      fxe_i += llrint(i);
+      TLy_i = modf(TLy_i, &i);
+      fye_i += llrint(i);
+      TLz_i = modf(TLz_i, &i);
+      fze_i += llrint(i);
+#  endif
+      fx_i += llrint(TLx_i * FORCESCALE);
+      fy_i += llrint(TLy_i * FORCESCALE);
+      fz_i += llrint(TLz_i * FORCESCALE);
+#endif
+
+#ifdef PME_ENERGY
+#  ifdef use_SPFP
+      eed  += fast_llrintf(TLeed * ENERGYSCALEF);
+      evdw += fast_llrintf(TLevdw * ENERGYSCALEF);
+#  else
+#    ifdef PME_MINIMIZATION
+      TLevdw = modf(TLevdw, &i);
+      evdwe += llrint(i);
+      TLeed = modf(TLeed, &i);
+      eede += llrint(i);
+#    endif
+      eed  += llrint(TLeed * ENERGYSCALE);
+      evdw += llrint(TLevdw * ENERGYSCALE);
+#  endif
+#endif
+
+#ifdef PME_VIRIAL
+#  ifdef use_SPFP
+      vir_11 += fast_llrintf(TLvir_11 * FORCESCALEF);
+      vir_22 += fast_llrintf(TLvir_22 * FORCESCALEF);
+      vir_33 += fast_llrintf(TLvir_33 * FORCESCALEF);
+#  else
+#    if defined(PME_MINIMIZATION)
+      TLvir_11 = modf(TLvir_11, &i);
+      vir_11E += llrint(i);
+      TLvir_22 = modf(TLvir_22, &i);
+      vir_22E += llrint(i);
+      TLvir_33 = modf(TLvir_33, &i);
+      vir_33E += llrint(i);
+#    endif
+      vir_11 += llrint(TLvir_11 * FORCESCALE);
+      vir_22 += llrint(TLvir_22 * FORCESCALE);
+      vir_33 += llrint(TLvir_33 * FORCESCALE);
+#  endif
+#endif
+      // Dump j atom forces
+      if (tx + tgx < psWarp->nlEntry.NL.xatoms) {
+        int offset = (shAtom.ID >> NLATOM_CELL_SHIFT);
+
+#ifdef use_SPFP
+        atomicAdd((unsigned long long int*)&cSim.pNBForceXAccumulator[offset],
+                  llitoulli(fast_llrintf(shFx * FORCESCALEF)));
+        atomicAdd((unsigned long long int*)&cSim.pNBForceYAccumulator[offset],
+                  llitoulli(fast_llrintf(shFy * FORCESCALEF)));
+        atomicAdd((unsigned long long int*)&cSim.pNBForceZAccumulator[offset],
+                  llitoulli(fast_llrintf(shFz * FORCESCALEF)));
+#else // use_DPFP
+#  ifdef PME_MINIMIZATION
+        PMEFloat i;
+        shFx = modf(shFx, &i);
+        atomicAdd((unsigned long long int*)&cSim.pIntForceXAccumulator[offset],
+                  llitoulli(llrint(i)));
+        shFy = modf(shFy, &i);
+        atomicAdd((unsigned long long int*)&cSim.pIntForceYAccumulator[offset],
+                  llitoulli(llrint(i)));
+        shFz = modf(shFz, &i);
+        atomicAdd((unsigned long long int*)&cSim.pIntForceZAccumulator[offset],
+                  llitoulli(llrint(i)));
+#  endif
+        atomicAdd((unsigned long long int*)&cSim.pNBForceXAccumulator[offset],
+                  llitoulli(llrint(shFx * FORCESCALE)));
+        atomicAdd((unsigned long long int*)&cSim.pNBForceYAccumulator[offset],
+                  llitoulli(llrint(shFy * FORCESCALE)));
+        atomicAdd((unsigned long long int*)&cSim.pNBForceZAccumulator[offset],
+                  llitoulli(llrint(shFz * FORCESCALE)));
+#endif
+      }
+      // End contingency for dumping j atom forces to global
+
+      // Advance to next x tile
+      tx += GRID;
+    }
+
+    // Reduce register forces
+    // MINT32 FIX: Use activemask for reduction
+    unsigned int reduceMask = __activemask();
+    for (unsigned int rlev = GRID >> 1; rlev >= PME_ATOMS_PER_WARP; rlev /= 2) {
+      fx_i += __SHFL(reduceMask, fx_i, tgx + rlev);
+      fy_i += __SHFL(reduceMask, fy_i, tgx + rlev);
+      fz_i += __SHFL(reduceMask, fz_i, tgx + rlev);
+#if defined(use_DPFP) && defined(PME_MINIMIZATION)
+      fxe_i += __SHFL(reduceMask, fxe_i, tgx + rlev);
+      fye_i += __SHFL(reduceMask, fye_i, tgx + rlev);
+      fze_i += __SHFL(reduceMask, fze_i, tgx + rlev);
+#endif
+    }
+
+    // Dump register forces
+    if (psWarp->nlEntry.NL.ypos + tgx < psWarp->nlEntry.NL.ymax) {
+      int offset = psWarp->nlEntry.NL.ypos + tgx;
+      atomicAdd((unsigned long long int*)&cSim.pNBForceXAccumulator[offset],
+                llitoulli(fx_i));
+      atomicAdd((unsigned long long int*)&cSim.pNBForceYAccumulator[offset],
+                llitoulli(fy_i));
+      atomicAdd((unsigned long long int*)&cSim.pNBForceZAccumulator[offset],
+                llitoulli(fz_i));
+#if defined(use_DPFP) && defined(PME_MINIMIZATION)
+      atomicAdd((unsigned long long int*)&cSim.pIntForceXAccumulator[offset],
+                llitoulli(fxe_i));
+      atomicAdd((unsigned long long int*)&cSim.pIntForceYAccumulator[offset],
+                llitoulli(fye_i));
+      atomicAdd((unsigned long long int*)&cSim.pIntForceZAccumulator[offset],
+                llitoulli(fze_i));
+#endif
+    }
+    // End of contingency for dumping register forces.
+#ifdef PHMD
+      if (h >= 0) {
+        atomicAdd((unsigned long long int*)&cSim.pdph_accumulator[indexi],
+                   llitoulli(fast_llrintf(dudli * FORCESCALEF)));
+        atomicAdd((unsigned long long int*)&cSim.pdph_plus_accumulator[indexi],
+                   llitoulli(fast_llrintf(dudliplus * FORCESCALEF)));
+      }
+#endif
+
+    // Get next Neighbor List entry
+    if (tgx == 0) {
+      psWarp->pos = atomicAdd(&cSim.pFrcBlkCounters[0], 1);
+    }
+    __SYNCWARP(WARP_MASK);
+  }
+  // End of massive while loop iterating psWarp->pos up to sNLEntries
+
+  // Reduce virials and/or energies
+#if defined(PME_VIRIAL) || defined(PME_ENERGY)
+  // MINT32 FIX: Use activemask for final reduction
+  unsigned int finalMask = __activemask();
+  for (unsigned int rlev = GRID >> 1; rlev > 0; rlev /= 2) {
+#  ifdef PME_ENERGY
+#    if defined(use_DPFP) && defined(PME_MINIMIZATION)
+    eede += __SHFL(finalMask, eede, tgx + rlev);
+    evdwe += __SHFL(finalMask, evdwe, tgx + rlev);
+#    endif
+    eed += __SHFL(finalMask, eed, tgx + rlev);
+    evdw += __SHFL(finalMask, evdw, tgx + rlev);
+#  endif
+
+#  ifdef PME_VIRIAL
+#    if defined(use_DPFP) && defined(PME_MINIMIZATION)
+    vir_11E += __SHFL(finalMask, vir_11E, tgx + rlev);
+    vir_22E += __SHFL(finalMask, vir_22E, tgx + rlev);
+    vir_33E += __SHFL(finalMask, vir_33E, tgx + rlev);
+#    endif
+    vir_11 += __SHFL(finalMask, vir_11, tgx + rlev);
+    vir_22 += __SHFL(finalMask, vir_22, tgx + rlev);
+    vir_33 += __SHFL(finalMask, vir_33, tgx + rlev);
+#  endif
+  }
+#endif
+
+#ifdef PME_VIRIAL
+  if (tgx == 0) {
+#  if defined(use_DPFP) && defined(PME_MINIMIZATION)
+    atomicAdd(cSim.pVirial_11E, llitoulli(vir_11E));
+    atomicAdd(cSim.pVirial_22E, llitoulli(vir_22E));
+    atomicAdd(cSim.pVirial_33E, llitoulli(vir_33E));
+#  endif
+    atomicAdd(cSim.pVirial_11, llitoulli(vir_11));
+    atomicAdd(cSim.pVirial_22, llitoulli(vir_22));
+    atomicAdd(cSim.pVirial_33, llitoulli(vir_33));
+  }
+#endif
+
+#ifdef PME_ENERGY
+  if (tgx == 0) {
+    atomicAdd(cSim.pEVDW, llitoulli(evdw));
+    atomicAdd(cSim.pEED, llitoulli(eed));
+#  if defined(use_DPFP) && defined(PME_MINIMIZATION)
+    atomicAdd(cSim.pEVDWE, llitoulli(evdwe));
+    atomicAdd(cSim.pEEDE, llitoulli(eede));
+#  endif
+  }
+#endif // PME_ENERGY
+#undef VOLATILE
+}
diff --git "a/C:\\amber\\amber_master\\src\\pmemd\\src/cuda/kNLCPNE_AFE.h" "b/C:\\amber\\amber_mint\\src\\pmemd\\src/cuda/kNLCPNE_AFE.h"
index f54649a..f8713d7 100644
--- "a/C:\\amber\\amber_master\\src\\pmemd\\src/cuda/kNLCPNE_AFE.h"
+++ "b/C:\\amber\\amber_mint\\src\\pmemd\\src/cuda/kNLCPNE_AFE.h"
@@ -21,6 +21,10 @@
     PMEFloat x;
     PMEFloat y;
     PMEFloat z;
+    int xi32;
+    int yi32;
+    int zi32;
+    int valid;
     PMEFloat q;
     unsigned int LJID;
     unsigned int ID;
@@ -59,6 +63,10 @@
 #define PSATOMX(i) shAtom.x
 #define PSATOMY(i) shAtom.y
 #define PSATOMZ(i) shAtom.z
+#define PSATOMXI32(i) shAtom.xi32
+#define PSATOMYI32(i) shAtom.yi32
+#define PSATOMZI32(i) shAtom.zi32
+#define PSATOMVALID(i) shAtom.valid
 #define PSATOMQ(i) shAtom.q
 #define PSATOMLJID(i) shAtom.LJID
 #define PSATOMID(i) shAtom.ID
@@ -196,6 +204,10 @@
     PMEFloat yi;
     PMEFloat zi;
     PMEFloat qi;
+    int xi_i32 = 0;
+    int yi_i32 = 0;
+    int zi_i32 = 0;
+    int validAtom = 0;
 #ifdef PME_SCTI
     unsigned int TIi;
 #endif
@@ -205,20 +217,25 @@
     PMEForce fz_i = (PMEForce)0;
     unsigned int index = psWarp->nlEntry.NL.ypos + (tgx & cSim.NLAtomsPerWarpBitsMask);
     if (index < psWarp->nlEntry.NL.ymax) {
-      PMEFloat2 xy    = cSim.pAtomXYSP[index];
+      int4 coord = cSim.pAtomCoord_M32[index];
+      xi_i32 = coord.x;
+      yi_i32 = coord.y;
+      zi_i32 = coord.z;
+      xi = (PMEFloat)xi_i32 * cSim.mint32_inv_scale.x;
+      yi = (PMEFloat)yi_i32 * cSim.mint32_inv_scale.y;
+      zi = (PMEFloat)zi_i32 * cSim.mint32_inv_scale.z;
       PMEFloat2 qljid = cSim.pAtomChargeSPLJID[index];
-      zi = cSim.pAtomZSP[index];
 #ifdef PME_SCTI
       TIi = cSim.pImageTIRegion[index];
 #endif
-      xi = xy.x;
-      yi = xy.y;
+
       qi = qljid.x;
 #ifdef use_DPFP
       LJIDi = __double_as_longlong(qljid.y);
 #else
       LJIDi = __float_as_uint(qljid.y);
 #endif
+      validAtom = 1;
     }
     else {
       xi = (PMEFloat)10000.0 * index;
@@ -229,20 +246,13 @@
 #ifdef PME_SCTI
       TIi = 0;
 #endif
+      xi_i32 = 0;
+      yi_i32 = 0;
+      zi_i32 = 0;
+      validAtom = 0;
     }
 
-#ifndef PME_IS_ORTHOGONAL
-    // Transform into cartesian space
-#  ifdef PME_VIRIAL
-    xi     = xi*sUcellf[0] + yi*sUcellf[1] + zi*sUcellf[2];
-    yi     =                 yi*sUcellf[4] + zi*sUcellf[5];
-    zi     =                                 zi*sUcellf[8];
-#  else
-    xi     = xi*cSim.ucellf[0][0] + yi*cSim.ucellf[0][1] + zi*cSim.ucellf[0][2];
-    yi     =                        yi*cSim.ucellf[1][1] + zi*cSim.ucellf[1][2];
-    zi     =                                               zi*cSim.ucellf[2][2];
-#  endif
-#endif
+
     // Special-case first tile
     // Copy register data into shared memory
     if (psWarp->bHomeCell) {
@@ -259,6 +269,10 @@
       PSATOMX(tgx)    = xi;
       PSATOMY(tgx)    = yi;
       PSATOMZ(tgx)    = zi;
+      PSATOMXI32(tgx) = xi_i32;
+      PSATOMYI32(tgx) = yi_i32;
+      PSATOMZI32(tgx) = zi_i32;
+      PSATOMVALID(tgx)= validAtom;
       PSATOMQ(tgx)    = qi;
       PSATOMLJID(tgx) = LJIDi;
 #ifdef PME_SCTI
@@ -277,6 +291,10 @@
       shAtom.x    = __SHFL(WARP_MASK, shAtom.x, j);
       shAtom.y    = __SHFL(WARP_MASK, shAtom.y, j);
       shAtom.z    = __SHFL(WARP_MASK, shAtom.z, j);
+      shAtom.xi32 = __SHFL(WARP_MASK, shAtom.xi32, j);
+      shAtom.yi32 = __SHFL(WARP_MASK, shAtom.yi32, j);
+      shAtom.zi32 = __SHFL(WARP_MASK, shAtom.zi32, j);
+      shAtom.valid= __SHFL(WARP_MASK, shAtom.valid, j);
       shAtom.q    = __SHFL(WARP_MASK, shAtom.q, j);
       shAtom.LJID = __SHFL(WARP_MASK, shAtom.LJID, j);
 #ifdef PME_SCTI
@@ -293,11 +311,21 @@
         PMEMask mask1 = __BALLOT(WARP_MASK, j != end);
         while (j != end) {
 #endif
-          PMEFloat xij = xi - PSATOMX(j);
-          PMEFloat yij = yi - PSATOMY(j);
-          PMEFloat zij = zi - PSATOMZ(j);
-          PMEFloat r2  = xij * xij + yij * yij + zij * zij;
-          if (r2 < cSim.cut2) {
+          PMEFloat xij = (PMEFloat)0.0;
+          PMEFloat yij = (PMEFloat)0.0;
+          PMEFloat zij = (PMEFloat)0.0;
+          bool valid_pair = (validAtom && shAtom.valid);
+          PMEFloat r2 = (PMEFloat)0.0;
+          if (valid_pair) {
+            PMEFloat xij_frac = (PMEFloat)((double)(xi_i32 - shAtom.xi32) * cSim.mint32_inv_scale.x);
+            PMEFloat yij_frac = (PMEFloat)((double)(yi_i32 - shAtom.yi32) * cSim.mint32_inv_scale.y);
+            PMEFloat zij_frac = (PMEFloat)((double)(zi_i32 - shAtom.zi32) * cSim.mint32_inv_scale.z);
+            xij = xij_frac;
+            yij = yij_frac;
+            zij = zij_frac;
+            r2 = xij * xij + yij * yij + zij * zij;
+          }
+          if (valid_pair && (r2 < cSim.cut2)) {
             PMEFloat rinv      = rsqrt(r2);
             PMEFloat r         = r2 * rinv;
             PMEFloat r2inv     = rinv * rinv;
@@ -417,6 +445,10 @@
           shAtom.x    = __SHFL(mask1, shAtom.x, shIdx);
           shAtom.y    = __SHFL(mask1, shAtom.y, shIdx);
           shAtom.z    = __SHFL(mask1, shAtom.z, shIdx);
+          shAtom.xi32 = __SHFL(mask1, shAtom.xi32, shIdx);
+          shAtom.yi32 = __SHFL(mask1, shAtom.yi32, shIdx);
+          shAtom.zi32 = __SHFL(mask1, shAtom.zi32, shIdx);
+          shAtom.valid= __SHFL(mask1, shAtom.valid, shIdx);
           shAtom.q    = __SHFL(mask1, shAtom.q, shIdx);
           shAtom.LJID = __SHFL(mask1, shAtom.LJID, shIdx);
           j = sNext[j];
@@ -446,13 +478,21 @@
         while (j != end) {
 #endif
           unsigned int TIj = PSATOMTI(j);
+          int valid_j = shAtom.valid;
+          bool valid_pair = (validAtom && valid_j);
 
           // Exclude all interactions across TI regions.  TIi can be 101, 011, 100,
           // 010, where anytime you're at 6, you're cross region (111 or 110)
-          if ((TIi | TIj) < 6) {
-            PMEFloat xij = xi - PSATOMX(j);
-            PMEFloat yij = yi - PSATOMY(j);
-            PMEFloat zij = zi - PSATOMZ(j);
+          if (valid_pair && ((TIi | TIj) < 6)) {
+            PMEFloat xij = (PMEFloat)0.0;
+            PMEFloat yij = (PMEFloat)0.0;
+            PMEFloat zij = (PMEFloat)0.0;
+            PMEFloat xij_frac = (PMEFloat)((double)(xi_i32 - shAtom.xi32) * cSim.mint32_inv_scale.x);
+            PMEFloat yij_frac = (PMEFloat)((double)(yi_i32 - shAtom.yi32) * cSim.mint32_inv_scale.y);
+            PMEFloat zij_frac = (PMEFloat)((double)(zi_i32 - shAtom.zi32) * cSim.mint32_inv_scale.z);
+            xij = xij_frac;
+            yij = yij_frac;
+            zij = zij_frac;
             PMEFloat r2  = xij*xij + yij*yij + zij*zij;
             if (r2 < cSim.cut2) {
               PMEFloat rinv       = rsqrt(r2);
@@ -892,6 +932,10 @@
           shAtom.x    = __SHFL(mask1, shAtom.x, shIdx);
           shAtom.y    = __SHFL(mask1, shAtom.y, shIdx);
           shAtom.z    = __SHFL(mask1, shAtom.z, shIdx);
+          shAtom.xi32 = __SHFL(mask1, shAtom.xi32, shIdx);
+          shAtom.yi32 = __SHFL(mask1, shAtom.yi32, shIdx);
+          shAtom.zi32 = __SHFL(mask1, shAtom.zi32, shIdx);
+          shAtom.valid= __SHFL(mask1, shAtom.valid, shIdx);
           shAtom.q    = __SHFL(mask1, shAtom.q, shIdx);
           shAtom.LJID = __SHFL(mask1, shAtom.LJID, shIdx);
           shAtom.TI   = __SHFL(mask1, shAtom.TI, shIdx);
@@ -946,25 +990,27 @@
       if (tx + tgx < psWarp->nlEntry.NL.xatoms) {
         unsigned int atom = PSATOMID(tgx) >> NLATOM_CELL_SHIFT;
 #ifndef use_DPFP
-        PMEFloat2 xy    = tex1Dfetch<float2>(cSim.texAtomXYSP, atom);
         PMEFloat2 qljid = tex1Dfetch<float2>(cSim.texAtomChargeSPLJID, atom);
-        PSATOMZ(tgx)    = tex1Dfetch<float>(cSim.texAtomZSP, atom);
 #else
-        PMEFloat2 xy    = cSim.pAtomXYSP[atom];
         PMEFloat2 qljid = cSim.pAtomChargeSPLJID[atom];
-        PSATOMZ(tgx)    = cSim.pAtomZSP[atom];
 #endif
+        int4 coord = cSim.pAtomCoord_M32[atom];
+        PSATOMXI32(tgx) = coord.x;
+        PSATOMYI32(tgx) = coord.y;
+        PSATOMZI32(tgx) = coord.z;
+        PSATOMX(tgx)    = (PMEFloat)PSATOMXI32(tgx) * cSim.mint32_inv_scale.x;
+        PSATOMY(tgx)    = (PMEFloat)PSATOMYI32(tgx) * cSim.mint32_inv_scale.y;
+        PSATOMZ(tgx)    = (PMEFloat)PSATOMZI32(tgx) * cSim.mint32_inv_scale.z;
+        PSATOMQ(tgx)    = qljid.x;
 #ifdef PME_SCTI
         PSATOMTI(tgx)   = cSim.pImageTIRegion[atom];
 #endif
-        PSATOMX(tgx)    = xy.x;
-        PSATOMY(tgx)    = xy.y;
-        PSATOMQ(tgx)    = qljid.x;
 #ifdef use_DPFP
         PSATOMLJID(tgx) = __double_as_longlong(qljid.y);
 #else
         PSATOMLJID(tgx) = __float_as_uint(qljid.y);
 #endif
+        PSATOMVALID(tgx)= 1;
       }
       else {
         PSATOMX(tgx)    = (PMEFloat)-10000.0 * tgx;
@@ -975,6 +1021,10 @@
 #ifdef PME_SCTI
         PSATOMTI(tgx)   = 0;
 #endif
+        PSATOMXI32(tgx) = 0;
+        PSATOMYI32(tgx) = 0;
+        PSATOMZI32(tgx) = 0;
+        PSATOMVALID(tgx)= 0;
       }
 
       // Translate all atoms into a local coordinate system within one unit
@@ -990,19 +1040,7 @@
       PSATOMZ(tgx) += cSim.cellOffset[cell][2];
 #endif
 
-#ifndef PME_IS_ORTHOGONAL
-#  ifdef PME_VIRIAL
-      PSATOMX(tgx) = sUcellf[0]*PSATOMX(tgx) + sUcellf[1]*PSATOMY(tgx) +
-                     sUcellf[2]*PSATOMZ(tgx);
-      PSATOMY(tgx) = sUcellf[4]*PSATOMY(tgx) + sUcellf[5]*PSATOMZ(tgx);
-      PSATOMZ(tgx) = sUcellf[8]*PSATOMZ(tgx);
-#  else
-      PSATOMX(tgx) = cSim.ucellf[0][0]*PSATOMX(tgx) + cSim.ucellf[0][1]*PSATOMY(tgx) +
-                     cSim.ucellf[0][2]*PSATOMZ(tgx);
-      PSATOMY(tgx) = cSim.ucellf[1][1]*PSATOMY(tgx) + cSim.ucellf[1][2]*PSATOMZ(tgx);
-      PSATOMZ(tgx) = cSim.ucellf[2][2]*PSATOMZ(tgx);
-#  endif
-#endif
+
       int j = tgx;
       int shIdx = sNext[tgx];
 #ifdef PME_SCTI
@@ -1013,13 +1051,19 @@
 #endif
         do {
           unsigned int TIj = PSATOMTI(j);
+          int valid_j = PSATOMVALID(j);
 
           // If (TIi | TIj) evaluates to 111 or 110, the interaction cross two TI regions.
           // Mask those interactions out, we don't want them here.
-          if ((TIi | TIj) < 6) {
-            PMEFloat xij = xi - PSATOMX(j);
-            PMEFloat yij = yi - PSATOMY(j);
-            PMEFloat zij = zi - PSATOMZ(j);
+          if ((TIi | TIj) < 6 && validAtom && valid_j) {
+            PMEFloat xij = (PMEFloat)0.0;
+            PMEFloat yij = (PMEFloat)0.0;
+            PMEFloat zij = (PMEFloat)0.0;
+                          PMEFloat xij_frac = (PMEFloat)((double)(xi_i32 - PSATOMXI32(j)) * cSim.mint32_inv_scale.x);
+                          PMEFloat yij_frac = (PMEFloat)((double)(yi_i32 - PSATOMYI32(j)) * cSim.mint32_inv_scale.y);
+                          PMEFloat zij_frac = (PMEFloat)((double)(zi_i32 - PSATOMZI32(j)) * cSim.mint32_inv_scale.z);            xij = xij_frac;
+            yij = yij_frac;
+            zij = zij_frac;
             PMEFloat r2  = xij * xij + yij * yij + zij * zij;
             if (r2 < cSim.cut2) {
               PMEFloat rinv       = rsqrt(r2);
@@ -1464,6 +1508,10 @@
           shAtom.x    = __SHFL(mask1, shAtom.x, shIdx);
           shAtom.y    = __SHFL(mask1, shAtom.y, shIdx);
           shAtom.z    = __SHFL(mask1, shAtom.z, shIdx);
+          shAtom.xi32 = __SHFL(mask1, shAtom.xi32, shIdx);
+          shAtom.yi32 = __SHFL(mask1, shAtom.yi32, shIdx);
+          shAtom.zi32 = __SHFL(mask1, shAtom.zi32, shIdx);
+          shAtom.valid= __SHFL(mask1, shAtom.valid, shIdx);
           shAtom.q    = __SHFL(mask1, shAtom.q, shIdx);
           shAtom.LJID = __SHFL(mask1, shAtom.LJID, shIdx);
           shAtom.TI   = __SHFL(mask1, shAtom.TI, shIdx);
@@ -1479,11 +1527,21 @@
 #pragma unroll 2
 #endif
           do {
-            PMEFloat xij = xi - PSATOMX(j);
-            PMEFloat yij = yi - PSATOMY(j);
-            PMEFloat zij = zi - PSATOMZ(j);
-            PMEFloat r2  = xij*xij + yij*yij + zij*zij;
-            if (r2 < cSim.cut2) {
+            int valid_j = PSATOMVALID(j);
+            PMEFloat xij = (PMEFloat)0.0;
+            PMEFloat yij = (PMEFloat)0.0;
+            PMEFloat zij = (PMEFloat)0.0;
+            PMEFloat r2  = (PMEFloat)0.0;
+            bool valid_pair = (validAtom && valid_j);
+            if (valid_pair) {
+                            PMEFloat xij_frac = (PMEFloat)((double)(xi_i32 - PSATOMXI32(j)) * cSim.mint32_inv_scale.x);
+                            PMEFloat yij_frac = (PMEFloat)((double)(yi_i32 - PSATOMYI32(j)) * cSim.mint32_inv_scale.y);
+                            PMEFloat zij_frac = (PMEFloat)((double)(zi_i32 - PSATOMZI32(j)) * cSim.mint32_inv_scale.z);              xij = xij_frac;
+              yij = yij_frac;
+              zij = zij_frac;
+              r2  = xij*xij + yij*yij + zij*zij;
+            }
+            if (valid_pair && (r2 < cSim.cut2)) {
               PMEFloat rinv       = rsqrt(r2);
               PMEFloat r          = r2 * rinv;
               PMEFloat r2inv      = rinv * rinv;
@@ -1612,6 +1670,10 @@
             shAtom.x    = __SHFL(mask1, shAtom.x, shIdx);
             shAtom.y    = __SHFL(mask1, shAtom.y, shIdx);
             shAtom.z    = __SHFL(mask1, shAtom.z, shIdx);
+            shAtom.xi32 = __SHFL(mask1, shAtom.xi32, shIdx);
+            shAtom.yi32 = __SHFL(mask1, shAtom.yi32, shIdx);
+            shAtom.zi32 = __SHFL(mask1, shAtom.zi32, shIdx);
+            shAtom.valid= __SHFL(mask1, shAtom.valid, shIdx);
             shAtom.q    = __SHFL(mask1, shAtom.q, shIdx);
             shAtom.LJID = __SHFL(mask1, shAtom.LJID, shIdx);
             j = sNext[j];
@@ -1624,11 +1686,21 @@
 #pragma unroll 2
 #endif
           do {
-            PMEFloat xij = xi - PSATOMX(j);
-            PMEFloat yij = yi - PSATOMY(j);
-            PMEFloat zij = zi - PSATOMZ(j);
-            PMEFloat r2  = xij*xij + yij*yij + zij*zij;
-            if (r2 < cSim.cut2) {
+            int valid_j = PSATOMVALID(j);
+            PMEFloat xij = (PMEFloat)0.0;
+            PMEFloat yij = (PMEFloat)0.0;
+            PMEFloat zij = (PMEFloat)0.0;
+            PMEFloat r2  = (PMEFloat)0.0;
+            bool valid_pair = (validAtom && valid_j);
+            if (valid_pair) {
+                            PMEFloat xij_frac = (PMEFloat)((double)(xi_i32 - PSATOMXI32(j)) * cSim.mint32_inv_scale.x);
+                            PMEFloat yij_frac = (PMEFloat)((double)(yi_i32 - PSATOMYI32(j)) * cSim.mint32_inv_scale.y);
+                            PMEFloat zij_frac = (PMEFloat)((double)(zi_i32 - PSATOMZI32(j)) * cSim.mint32_inv_scale.z);              xij = xij_frac;
+              yij = yij_frac;
+              zij = zij_frac;
+              r2  = xij*xij + yij*yij + zij*zij;
+            }
+            if (valid_pair && (r2 < cSim.cut2)) {
               PMEFloat rinv      = rsqrt(r2);
               PMEFloat r         = r2 * rinv;
               PMEFloat r2inv     = rinv * rinv;
diff --git "a/C:\\amber\\amber_master\\src\\pmemd\\src/cuda/kNLCPNE_AMD.h" "b/C:\\amber\\amber_mint\\src\\pmemd\\src/cuda/kNLCPNE_AMD.h"
index 0768945..637e21f 100644
--- "a/C:\\amber\\amber_master\\src\\pmemd\\src/cuda/kNLCPNE_AMD.h"
+++ "b/C:\\amber\\amber_mint\\src\\pmemd\\src/cuda/kNLCPNE_AMD.h"
@@ -23,6 +23,10 @@
     PMEFloat x;
     PMEFloat y;
     PMEFloat z;
+    int xi32;
+    int yi32;
+    int zi32;
+    int valid;
     PMEFloat q;
     unsigned int LJID;
     unsigned int ID;
@@ -127,36 +131,43 @@
     uint offset = psWarp->nlEntry.NL.offset;
 
     // Read y atoms into registers
-    PMEFloat xi;
-    PMEFloat yi;
-    PMEFloat zi;
-    PMEFloat qi;
-    unsigned int LJIDi;
+        PMEFloat xi = (PMEFloat)0.0;
+        PMEFloat yi = (PMEFloat)0.0;
+        PMEFloat zi = (PMEFloat)0.0;
+        PMEFloat qi = (PMEFloat)0.0;
+        unsigned int LJIDi = 0;
+        int xi_i32 = 0;
+        int yi_i32 = 0;
+        int zi_i32 = 0;
+        int validAtom = 0;
     unsigned int index = psWarp->nlEntry.NL.ypos + (tgx & cSim.NLAtomsPerWarpBitsMask);
     if (index < psWarp->nlEntry.NL.ymax) {
+      int4 coord = cSim.pAtomCoord_M32[index];
+      xi_i32 = coord.x;
+      yi_i32 = coord.y;
+      zi_i32 = coord.z;
+      PMEFloat xi_frac = (PMEFloat)xi_i32 * cSim.mint32_inv_scale.x;
+      PMEFloat yi_frac = (PMEFloat)yi_i32 * cSim.mint32_inv_scale.y;
+      PMEFloat zi_frac = (PMEFloat)zi_i32 * cSim.mint32_inv_scale.z;
 #ifndef use_DPFP
 #  if defined(__CUDA_ARCH__) && ((__CUDA_ARCH__ == 700) || (__CUDA_ARCH__ >= 800))
-      PMEFloat2 xy = cSim.pAtomXYSP[index];
       PMEFloat2 qljid = cSim.pAtomChargeSPLJID[index];
-      zi = cSim.pAtomZSP[index];
 #  else
-      PMEFloat2 xy = tex1Dfetch<float2>(cSim.texAtomXYSP, index);
       PMEFloat2 qljid = tex1Dfetch<float2>(cSim.texAtomChargeSPLJID, index);
-      zi = tex1Dfetch<float>(cSim.texAtomZSP, index);
 #  endif
 #else
-      PMEFloat2 xy = cSim.pAtomXYSP[index];
       PMEFloat2 qljid = cSim.pAtomChargeSPLJID[index];
-      zi = cSim.pAtomZSP[index];
 #endif
-      xi = xy.x;
-      yi = xy.y;
       qi = qljid.x;
 #ifdef use_DPFP
       LJIDi = __double_as_longlong(qljid.y);
 #else
       LJIDi = __float_as_uint(qljid.y);
 #endif
+      xi = xi_frac;
+      yi = yi_frac;
+      zi = zi_frac;
+      validAtom = 1;
     }
     else {
       xi = (PMEFloat)10000.0 * index;
@@ -164,19 +175,12 @@
       zi = (PMEFloat)10000.0 * index;
       qi = (PMEFloat)0.0;
       LJIDi = 0;
+      xi_i32 = 0;
+      yi_i32 = 0;
+      zi_i32 = 0;
+      validAtom = 0;
     }
-#ifndef PME_IS_ORTHOGONAL
-    // Transform into cartesian space
-#  ifdef PME_VIRIAL
-    xi = sUcellf[0]*xi + sUcellf[1]*yi + sUcellf[2]*zi;
-    yi =                 sUcellf[4]*yi + sUcellf[5]*zi;
-    zi =                                 sUcellf[8]*zi;
-#  else
-    xi = cSim.ucellf[0][0]*xi + cSim.ucellf[0][1]*yi + cSim.ucellf[0][2]*zi;
-    yi =                        cSim.ucellf[1][1]*yi + cSim.ucellf[1][2]*zi;
-    zi =                                               cSim.ucellf[2][2]*zi;
-#  endif
-#endif
+
 
     // Tile accumulators for forces
     PMEFloat TLx_i = (PMEFloat)0.0;
@@ -218,29 +222,35 @@
       // is necessary--they are different atoms.
       // Fast-forward iftlim iterations in the final PME_ATOMS_PER_WARP lanes for the first
       // tile computation.
-      int j = ((tgx + iftlim * jgroup + 1) & cSim.NLAtomsPerWarpBitsMask) | joffset;
-      int jrec = ((PME_ATOMS_PER_WARP + tgx - (iftlim * jgroup + 1))
-                  & cSim.NLAtomsPerWarpBitsMask) | joffset;
-      PMEFloat xj = __SHFL(WARP_MASK, xi, j);
-      PMEFloat yj = __SHFL(WARP_MASK, yi, j);
-      PMEFloat zj = __SHFL(WARP_MASK, zi, j);
-      PMEFloat qj = __SHFL(WARP_MASK, qi, j);
-      LJIDj       = __SHFL(WARP_MASK, LJIDj, j);
+            int j = ((tgx + iftlim * jgroup + 1) & cSim.NLAtomsPerWarpBitsMask) | joffset;
+            int jrec = ((PME_ATOMS_PER_WARP + tgx - (iftlim * jgroup + 1))
+            & cSim.NLAtomsPerWarpBitsMask) | joffset;
+            int xj_i32 = __SHFL(WARP_MASK, xi_i32, j);
+            int yj_i32 = __SHFL(WARP_MASK, yi_i32, j);
+            int zj_i32 = __SHFL(WARP_MASK, zi_i32, j);
+            PMEFloat qj = __SHFL(WARP_MASK, qi, j);
+            unsigned int LJIDj_val = __SHFL(WARP_MASK, LJIDj, j);
+            int valid_j = __SHFL(WARP_MASK, validAtom, j);
       #pragma unroll 2
       for (int ift = 0; ift < iftlim; ift++) {
-        PMEFloat xij = xi - xj;
-        PMEFloat yij = yi - yj;
-        PMEFloat zij = zi - zj;
-        xj = WarpRotateLeft<PME_ATOMS_PER_WARP>(xj);
-        yj = WarpRotateLeft<PME_ATOMS_PER_WARP>(yj);
-        zj = WarpRotateLeft<PME_ATOMS_PER_WARP>(zj);
+                PMEFloat xij_frac = (PMEFloat)((double)(xi_i32 - xj_i32) * cSim.mint32_inv_scale.x);
+                PMEFloat yij_frac = (PMEFloat)((double)(yi_i32 - yj_i32) * cSim.mint32_inv_scale.y);
+                PMEFloat zij_frac = (PMEFloat)((double)(zi_i32 - zj_i32) * cSim.mint32_inv_scale.z);        PMEFloat xij, yij, zij;
+        xij = xij_frac;
+        yij = yij_frac;
+        zij = zij_frac;
+        xj_i32 = WarpRotateLeft<PME_ATOMS_PER_WARP>(xj_i32);
+        yj_i32 = WarpRotateLeft<PME_ATOMS_PER_WARP>(yj_i32);
+        zj_i32 = WarpRotateLeft<PME_ATOMS_PER_WARP>(zj_i32);
         PMEFloat r2 = xij*xij + yij*yij + zij*zij;
-        unsigned int index = LJIDi + LJIDj;
-        LJIDj = WarpRotateLeft<PME_ATOMS_PER_WARP>(LJIDj);
+        unsigned int index = LJIDi + LJIDj_val;
+        LJIDj_val = WarpRotateLeft<PME_ATOMS_PER_WARP>(LJIDj_val);
         PMEFloat qiqj = qi * qj;
         qj = WarpRotateLeft<PME_ATOMS_PER_WARP>(qj);
+        int valid_pair = validAtom && valid_j;
+        valid_j = WarpRotateLeft<PME_ATOMS_PER_WARP>(valid_j);
         PMEFloat df = (PMEFloat)0.0;
-        bool inrange = ((r2 < cSim.cut2) && r2 > delta);
+        bool inrange = (valid_pair && (r2 < cSim.cut2) && (r2 > delta));
         if (inrange) {
 #if defined(use_SPFP) && !defined(PME_FSWITCH) && !defined(PME_ENERGY)
           PMEFloat r2inv = (PMEFloat)1.0 / r2;
@@ -445,27 +455,27 @@
         unsigned int atom = shAtom.ID >> NLATOM_CELL_SHIFT;
 #ifndef use_DPFP
 #  if defined(__CUDA_ARCH__) && ((__CUDA_ARCH__ == 700) || (__CUDA_ARCH__ >= 800))
-        PMEFloat2 xy = cSim.pAtomXYSP[atom];
         PMEFloat2 qljid = cSim.pAtomChargeSPLJID[atom];
-        shAtom.z = cSim.pAtomZSP[atom];
 #  else
-        PMEFloat2 xy = tex1Dfetch<float2>(cSim.texAtomXYSP, atom);
         PMEFloat2 qljid = tex1Dfetch<float2>(cSim.texAtomChargeSPLJID, atom);
-        shAtom.z = tex1Dfetch<float>(cSim.texAtomZSP, atom);
 #  endif
 #else
-        PMEFloat2 xy = cSim.pAtomXYSP[atom];
         PMEFloat2 qljid = cSim.pAtomChargeSPLJID[atom];
-        shAtom.z = cSim.pAtomZSP[atom];
 #endif
-        shAtom.x = xy.x;
-        shAtom.y = xy.y;
+        int4 coord = cSim.pAtomCoord_M32[atom];
+        shAtom.xi32 = coord.x;
+        shAtom.yi32 = coord.y;
+        shAtom.zi32 = coord.z;
+        shAtom.x = (PMEFloat)shAtom.xi32 * cSim.mint32_inv_scale.x;
+        shAtom.y = (PMEFloat)shAtom.yi32 * cSim.mint32_inv_scale.y;
+        shAtom.z = (PMEFloat)shAtom.zi32 * cSim.mint32_inv_scale.z;
         shAtom.q = qljid.x;
 #ifdef use_DPFP
         shAtom.LJID = __double_as_longlong(qljid.y);
 #else
         shAtom.LJID = __float_as_uint(qljid.y);
 #endif
+        shAtom.valid = 1;
       }
       else {
         shAtom.x = (PMEFloat)-10000.0 * tgx;
@@ -473,6 +483,10 @@
         shAtom.z = (PMEFloat)-10000.0 * tgx;
         shAtom.q = (PMEFloat)0.0;
         shAtom.LJID = 0;
+        shAtom.xi32 = 0;
+        shAtom.yi32 = 0;
+        shAtom.zi32 = 0;
+        shAtom.valid = 0;
       }
 
       // Translate all atoms into a local coordinate system within one unit
@@ -511,21 +525,36 @@
 #endif
 
       if (__ANY(WARP_MASK, exclusion)) {
+        int xj_i32 = shAtom.xi32;
+        int yj_i32 = shAtom.yi32;
+        int zj_i32 = shAtom.zi32;
+        PMEFloat qj = shAtom.q;
+        unsigned int LJIDj_val = shAtom.LJID;
+        int valid_j = shAtom.valid;
         #pragma unroll 2
         for (int it = 0; it < PME_ATOMS_PER_WARP; it++) {
-          PMEFloat xij = xi - shAtom.x;
-          PMEFloat yij = yi - shAtom.y;
-          PMEFloat zij = zi - shAtom.z;
-          shAtom.x = WarpRotateLeft<PME_ATOMS_PER_WARP>(shAtom.x);
-          shAtom.y = WarpRotateLeft<PME_ATOMS_PER_WARP>(shAtom.y);
-          shAtom.z = WarpRotateLeft<PME_ATOMS_PER_WARP>(shAtom.z);
+          PMEFloat xij = (PMEFloat)0.0;
+          PMEFloat yij = (PMEFloat)0.0;
+          PMEFloat zij = (PMEFloat)0.0;
+          int valid_pair = validAtom && valid_j;
+          if (valid_pair) {
+                    PMEFloat xij_frac = (PMEFloat)((double)(xi_i32 - xj_i32) * cSim.mint32_inv_scale.x);
+                    PMEFloat yij_frac = (PMEFloat)((double)(yi_i32 - yj_i32) * cSim.mint32_inv_scale.y);
+                    PMEFloat zij_frac = (PMEFloat)((double)(zi_i32 - zj_i32) * cSim.mint32_inv_scale.z);            xij = xij_frac;
+            yij = yij_frac;
+            zij = zij_frac;
+          }
+          xj_i32 = WarpRotateLeft<PME_ATOMS_PER_WARP>(xj_i32);
+          yj_i32 = WarpRotateLeft<PME_ATOMS_PER_WARP>(yj_i32);
+          zj_i32 = WarpRotateLeft<PME_ATOMS_PER_WARP>(zj_i32);
           PMEFloat r2  = xij*xij + yij*yij + zij*zij;
-          unsigned int index = LJIDi + shAtom.LJID;
-          shAtom.LJID = WarpRotateLeft<PME_ATOMS_PER_WARP>(shAtom.LJID);
-          PMEFloat qiqj = qi * shAtom.q;
-          shAtom.q = WarpRotateLeft<PME_ATOMS_PER_WARP>(shAtom.q);
+          unsigned int index = LJIDi + LJIDj_val;
+          LJIDj_val = WarpRotateLeft<PME_ATOMS_PER_WARP>(LJIDj_val);
+          PMEFloat qiqj = qi * qj;
+          qj = WarpRotateLeft<PME_ATOMS_PER_WARP>(qj);
           PMEFloat df = (PMEFloat)0.0;
-          bool inrange = ((r2 < cSim.cut2) && r2 > delta);
+          bool inrange = (valid_pair && (r2 < cSim.cut2) && r2 > delta);
+          valid_j = WarpRotateLeft<PME_ATOMS_PER_WARP>(valid_j);
           if (inrange) {
 #if defined(use_SPFP) && !defined(PME_FSWITCH) && !defined(PME_ENERGY)
             PMEFloat r2inv = (PMEFloat)1.0 / r2;
@@ -648,24 +677,39 @@
         // there IS at least one exclusion somewhere in the pile
       }
       else {
+        int xj_i32 = shAtom.xi32;
+        int yj_i32 = shAtom.yi32;
+        int zj_i32 = shAtom.zi32;
+        PMEFloat qj = shAtom.q;
+        unsigned int LJIDj_val = shAtom.LJID;
+        int valid_j = shAtom.valid;
         #pragma unroll 2
         for (int it = 0; it < PME_ATOMS_PER_WARP; it++) {
           // Read properties for the other atom
-          PMEFloat xij = xi - shAtom.x;
-          PMEFloat yij = yi - shAtom.y;
-          PMEFloat zij = zi - shAtom.z;
-          shAtom.x = WarpRotateLeft<PME_ATOMS_PER_WARP>(shAtom.x);
-          shAtom.y = WarpRotateLeft<PME_ATOMS_PER_WARP>(shAtom.y);
-          shAtom.z = WarpRotateLeft<PME_ATOMS_PER_WARP>(shAtom.z);
+          PMEFloat xij = (PMEFloat)0.0;
+          PMEFloat yij = (PMEFloat)0.0;
+          PMEFloat zij = (PMEFloat)0.0;
+          int valid_pair = validAtom && valid_j;
+          if (valid_pair) {
+                    PMEFloat xij_frac = (PMEFloat)((double)(xi_i32 - xj_i32) * cSim.mint32_inv_scale.x);
+                    PMEFloat yij_frac = (PMEFloat)((double)(yi_i32 - yj_i32) * cSim.mint32_inv_scale.y);
+                    PMEFloat zij_frac = (PMEFloat)((double)(zi_i32 - zj_i32) * cSim.mint32_inv_scale.z);            xij = xij_frac;
+            yij = yij_frac;
+            zij = zij_frac;
+          }
+          xj_i32 = WarpRotateLeft<PME_ATOMS_PER_WARP>(xj_i32);
+          yj_i32 = WarpRotateLeft<PME_ATOMS_PER_WARP>(yj_i32);
+          zj_i32 = WarpRotateLeft<PME_ATOMS_PER_WARP>(zj_i32);
 
           // Perform the range test
           PMEFloat r2  = xij*xij + yij*yij + zij*zij;
-          unsigned int index = LJIDi + shAtom.LJID;
-          shAtom.LJID = WarpRotateLeft<PME_ATOMS_PER_WARP>(shAtom.LJID);
-          PMEFloat qiqj = qi * shAtom.q;
-          shAtom.q = WarpRotateLeft<PME_ATOMS_PER_WARP>(shAtom.q);
-          bool inrange = ((r2 < cSim.cut2) && r2 > delta);
+          unsigned int index = LJIDi + LJIDj_val;
+          LJIDj_val = WarpRotateLeft<PME_ATOMS_PER_WARP>(LJIDj_val);
+          PMEFloat qiqj = qi * qj;
+          qj = WarpRotateLeft<PME_ATOMS_PER_WARP>(qj);
+          bool inrange = (valid_pair && (r2 < cSim.cut2) && r2 > delta);
           PMEFloat df = (PMEFloat)0.0;
+          valid_j = WarpRotateLeft<PME_ATOMS_PER_WARP>(valid_j);
           if (inrange) {
 #if defined(use_SPFP) && !defined(PME_FSWITCH) && !defined(PME_ENERGY)
             PMEFloat r2inv = (PMEFloat)1.0 / r2;
diff --git "a/C:\\amber\\amber_master\\src\\pmemd\\src/cuda/kNL_PencilHash.h" "b/C:\\amber\\amber_mint\\src\\pmemd\\src/cuda/kNL_PencilHash.h"
index 8cce4bb..8bd1973 100644
--- "a/C:\\amber\\amber_master\\src\\pmemd\\src/cuda/kNL_PencilHash.h"
+++ "b/C:\\amber\\amber_mint\\src\\pmemd\\src/cuda/kNL_PencilHash.h"
@@ -46,13 +46,11 @@
     double dfz =                                           cSim.recip[2][2]*z;
 #endif
 
-    // Re-image all fractional coordinates to within [0.0, 1.0)
-    dfx = dfx - round(dfx) + 0.5;
-    dfy = dfy - round(dfy) + 0.5;
-    dfz = dfz - round(dfz) + 0.5;
-    dfx = (dfx < 1.0) ? dfx : 0.0;
-    dfy = (dfy < 1.0) ? dfy : 0.0;
-    dfz = (dfz < 1.0) ? dfz : 0.0;
+    // Re-image all fractional coordinates to within [-0.5, 0.5)
+    // MINT32 requires coordinates in [-0.5, 0.5), not [0, 1)
+    dfx = dfx - round(dfx);
+    dfy = dfy - round(dfy);
+    dfz = dfz - round(dfz);
 #ifdef CALCULATE_HASH_CELL
     PMEFloat fx = dfx;
     PMEFloat fy = dfy;
@@ -135,7 +133,9 @@
       besty = iy + 1;
       bestz = iz + 1;
     }
+    // MINT32 [-0.5, 0.5) coordinates can produce negative best indices
     besty += cSim.nypencils * (besty < 0);
+    bestz += cSim.nzpencils * (bestz < 0);
 #  else
     // Easier case: the pencils are arranged on a regular grid (pencils are laid out
     // regularly in fractional space).  Find the nearest four, translate them into real
@@ -146,7 +146,7 @@
     PMEFloat izf    = iz;
     PMEFloat iyp1f  = iyf + (PMEFloat)1.0;
     PMEFloat izp1f  = izf + (PMEFloat)1.0;
-#    ifdef PME_VIRAL
+#    ifdef PME_VIRIAL
     iyf             = (iyf   * cSim.invypencils * sUcell[4]) - ry;
     iyp1f           = (iyp1f * cSim.invypencils * sUcell[4]) - ry;
 #    else
@@ -200,6 +200,9 @@
       bestz = iz + 1;
     }
 #  endif
+    // MINT32 [-0.5, 0.5) coordinates can produce negative best indices
+    besty += cSim.nypencils * (besty < 0);
+    bestz += cSim.nzpencils * (bestz < 0);
     besty -= cSim.nypencils * (besty == cSim.nypencils);
     bestz -= cSim.nzpencils * (bestz == cSim.nzpencils);
 
@@ -208,7 +211,10 @@
     // for each atom that happens during neighbor list construction.  It is
     // skipped when the neighbor list is actually used.
     cSim.pHcmbIndex[pos]  = pos;
-    unsigned int ix       = fx * XDIR_HASH_CELLS;
+    int ix_signed         = fx * XDIR_HASH_CELLS;
+    // MINT32 [-0.5, 0.5) coordinates can produce negative indices
+    ix_signed += XDIR_HASH_CELLS * (ix_signed < 0);
+    unsigned int ix       = ix_signed;
     unsigned int cellid   = besty + (bestz * cSim.nypencils);
     cellid                = (cellid << 2) | ((fx >= cSim.UpperCapBoundary) << 1) |
                             (fx < cSim.LowerCapBoundary);
diff --git "a/C:\\amber\\amber_master\\src\\pmemd\\src/cuda/kNeighborList.cu" "b/C:\\amber\\amber_mint\\src\\pmemd\\src/cuda/kNeighborList.cu"
index 9220d5f..31c5416 100644
--- "a/C:\\amber\\amber_master\\src\\pmemd\\src/cuda/kNeighborList.cu"
+++ "b/C:\\amber\\amber_mint\\src\\pmemd\\src/cuda/kNeighborList.cu"
@@ -10,13 +10,18 @@
 #ifndef AMBER_PLATFORM_AMD
 #include <cuda.h>
 #endif
+#include <cstdlib>
 #include "gpu.h"
 #include "bondRemapDS.h"
 #include "ptxmacros.h"
+#define MINT32_NL_CHECKSUM 1
+extern __device__ int dMint32BNLDebug[4];
 
 // Use global instance instead of a local copy
 #include "simulationConst.h"
 CSIM_STO simulationConst cSim;
+__device__ __constant__ int cBNLDebugStage;
+__device__ __constant__ int cBNLDebugCellLimit;
 
 struct Atom {
   PMEFloat xmin;
@@ -101,10 +106,129 @@ extern "C" void kNLResetCounter()
   kNLResetCounter_kernel<<<GRID, 1>>>();
 }
 
+//---------------------------------------------------------------------------------------------
+// kVelSnapshot_kernel: emit a small velocity sample (sorted + unsorted) for debugging.
+//---------------------------------------------------------------------------------------------
+__global__ void __LAUNCH_BOUNDS__(THREADS_PER_BLOCK, 1) kVelSnapshot_kernel(int tag)
+{
+  unsigned int pos = blockIdx.x * blockDim.x + threadIdx.x;
+  if (blockIdx.x == 0 && pos < 4 && pos < cSim.atoms) {
+    unsigned int atom = cSim.pImageAtom[pos];
+    double vsx = cSim.pImageVelX[pos];
+    double vsy = cSim.pImageVelY[pos];
+    double vsz = cSim.pImageVelZ[pos];
+    double vux = (atom < cSim.atoms) ? cSim.pVelX[atom] : 0.0;
+    double vuy = (atom < cSim.atoms) ? cSim.pVelY[atom] : 0.0;
+    double vuz = (atom < cSim.atoms) ? cSim.pVelZ[atom] : 0.0;
+    // printf("VEL_SNAPSHOT tag=%d pos=%u atom=%u v_sorted=(%.6f,%.6f,%.6f) v_unsorted=(%.6f,%.6f,%.6f)\n",
+    //        tag, pos, atom, vsx, vsy, vsz, vux, vuy, vuz);
+  }
+}
+
+extern "C" void kVelSnapshot(int tag)
+{
+  kVelSnapshot_kernel<<<1, THREADS_PER_BLOCK>>>(tag);
+  LAUNCHERROR("kVelSnapshot");
+}
+
+//---------------------------------------------------------------------------------------------
+// kNLPrepareHashCoordinates_kernel: Convert current MINT32 coordinates to float for spatial
+// hashing. MINT32 is always self-wrapped via integer overflow, so no explicit wrapping needed.
+// The spatial hash kernel handles the frame shift internally when computing cell indices.
+//---------------------------------------------------------------------------------------------
+__global__ void __LAUNCH_BOUNDS__(128, 8) kNLPrepareHashCoordinates_kernel()
+{
+  int idx = blockIdx.x * blockDim.x + threadIdx.x;
+  if (idx < cSim.atoms) {
+    // Read current MINT32 coords in sorted / neighbor-list order
+    // MINT32 is always self-wrapped - no explicit wrapping needed
+    int4 coord = cSim.pAtomCoord_M32[idx];
+
+    // Direct conversion: MINT32 -> float in [-box/2, box/2) frame
+    // The spatial hash kernel adds +0.5 shift internally to get [0, box) for cell indices
+    cSim.pImageX[idx] = (PMEDouble)((double)coord.x * cSim.mint32_inv_scale.x);
+    cSim.pImageY[idx] = (PMEDouble)((double)coord.y * cSim.mint32_inv_scale.y);
+    cSim.pImageZ[idx] = (PMEDouble)((double)coord.z * cSim.mint32_inv_scale.z);
+  }
+}
+
+//---------------------------------------------------------------------------------------------
+// kNLPrepareHashCoordinates: C wrapper to launch the imaging/quantization kernel above.
+//---------------------------------------------------------------------------------------------
+extern "C" void kNLPrepareHashCoordinates(gpuContext gpu)
+{
+  int atoms = gpu->sim.atoms;
+  int threads = 128;
+  int blocks = (atoms + threads - 1) / threads;
+
+  kNLPrepareHashCoordinates_kernel<<<blocks, threads>>>();
+
+  cudaError_t status = cudaGetLastError();
+  RTERROR(status, "kNLPrepareHashCoordinates kernel launch failed");
+}
+
+//---------------------------------------------------------------------------------------------
+// kMINT32ToFloat_kernel: Convert MINT32 wrapped Cartesian coords -> FP32 arrays for kBNL
+// MINT32 coords are in range [-box/2, box/2) Cartesian, same as what kBNL expects.
+//---------------------------------------------------------------------------------------------
+__global__ void __LAUNCH_BOUNDS__(128, 8) kMINT32ToFloat_kernel(
+    const int4* __restrict__ pAtomCoord_M32,
+    const PMEFloat inv_scale_x,
+    const PMEFloat inv_scale_y,
+    const PMEFloat inv_scale_z,
+    PMEFloat2* __restrict__ pAtomXYSP,
+    PMEFloat* __restrict__ pAtomZSP,
+    int atoms)
+{
+  int idx = blockIdx.x * blockDim.x + threadIdx.x;
+  if (idx < atoms) {
+    int4 coord = pAtomCoord_M32[idx];  // MINT32 wrapped Cartesian (authoritative)
+    // MINT32 coords are in [-box/2, box/2) Cartesian, same as kBNL expects
+    // No shift needed - kBNL computes cell-relative distances which work with centered coords
+    pAtomXYSP[idx].x = (PMEFloat)coord.x * inv_scale_x;
+    pAtomXYSP[idx].y = (PMEFloat)coord.y * inv_scale_y;
+    pAtomZSP[idx]    = (PMEFloat)coord.z * inv_scale_z;
+  }
+}
+
+//---------------------------------------------------------------------------------------------
+// kMINT32ToFloat: C wrapper to copy MINT32 coords to FP32 pAtomXYSP/ZSP
+//---------------------------------------------------------------------------------------------
+extern "C" void kMINT32ToFloat(gpuContext gpu, int convertOld)
+{
+  int atoms = gpu->sim.atoms;
+  int threads = 128;
+  int blocks = (atoms + threads - 1) / threads;
+
+  if (convertOld) {
+    kMINT32ToFloat_kernel<<<blocks, threads>>>(
+        gpu->sim.pOldAtomCoord_M32,
+        gpu->sim.mint32_inv_scale.x,
+        gpu->sim.mint32_inv_scale.y,
+        gpu->sim.mint32_inv_scale.z,
+        gpu->sim.pOldAtomXYSP,
+        gpu->sim.pOldAtomZSP,
+        atoms);
+  } else {
+    kMINT32ToFloat_kernel<<<blocks, threads>>>(
+        gpu->sim.pAtomCoord_M32,
+        gpu->sim.mint32_inv_scale.x,
+        gpu->sim.mint32_inv_scale.y,
+        gpu->sim.mint32_inv_scale.z,
+        gpu->sim.pAtomXYSP,
+        gpu->sim.pAtomZSP,
+        atoms);
+  }
+
+  cudaError_t status = cudaGetLastError();
+  RTERROR(status, "kMINT32ToFloat kernel launch failed");
+}
+
 //---------------------------------------------------------------------------------------------
 // kNLGenerateSpatialHash_kernel: generates the spatial hash grid based on the fractional
 //                                coordinates of each atom.  Fractional coordinates are taken
-//                                from the Cartesian coordinates.
+//                                from pImageX/Y/Z, which are refreshed from current positions
+//                                by kNLPrepareHashCoordinates before this kernel runs.
 //
 // The spatial decomposition occurs at two levels.  First, atoms are separated into cells of
 // thickness at least the non-bonded cutoff plus the neighbor list buffer zone (nb skin).
@@ -116,6 +240,9 @@ extern "C" void kNLResetCounter()
 // in gpu.cpp (search for cellHash).  By ordering atoms along this weaving, twisting curve,
 // they can be accessed linearly to give a list of atoms that are near one another even within
 // the confines of the cell interior.
+//
+// MINT32 FIX: Now reads from pAtomCoord (MINT32 source of truth) instead of pImageX/Y/Z
+// which may be stale after integration. Converts MINT32 -> wrapped Cartesian on-the-fly.
 //---------------------------------------------------------------------------------------------
 __global__ void
 __LAUNCH_BOUNDS__(THREADS_PER_BLOCK, 1)
@@ -143,9 +270,10 @@ kNLGenerateSpatialHash_kernel()
     }
     __syncthreads();
     while (pos < cSim.atoms) {
-      PMEFloat x = cSim.pImageX[pos];
-      PMEFloat y = cSim.pImageY[pos];
-      PMEFloat z = cSim.pImageZ[pos];
+      // Read wrapped Cartesian coordinates prepared for hashing
+      PMEFloat x = (PMEFloat)cSim.pImageX[pos];
+      PMEFloat y = (PMEFloat)cSim.pImageY[pos];
+      PMEFloat z = (PMEFloat)cSim.pImageZ[pos];
 
       // Orthogonal/nonorthogonal handled in the same code (3 single precision
       // multiplies and adds? Who cares and why?)
@@ -182,15 +310,26 @@ kNLGenerateSpatialHash_kernel()
       unsigned int hash =
         (((iz * cSim.ycells + iy) * cSim.xcells + ix) << CELL_HASH_BITS) | cellHash;
       cSim.pImageHash2[pos] = hash;
+
+      // DEBUG: sample hash input for first few atoms (shifted into [0,box] frame to match master logs)
+      if ((blockIdx.x == 0) && (threadIdx.x < 2)) {
+        double xs = (double)x + 0.5 * cSim.a;
+        double ys = (double)y + 0.5 * cSim.b;
+        double zs = (double)z + 0.5 * cSim.c;
+        // printf("HASH_DEBUG pos=%u x=%.6f y=%.6f z=%.6f xs=%.6f ys=%.6f zs=%.6f fx=%.6f fy=%.6f fz=%.6f ix=%u iy=%u iz=%u hash=%u cellID=%u\n",
+        //        pos, (double)x, (double)y, (double)z, xs, ys, zs,
+        //        (double)fx, (double)fy, (double)fz, ix, iy, iz, hash, cSim.pImageCellID[pos]);
+      }
       pos += increment;
     }
   }
   else {
     __syncthreads();
     while (pos < cSim.atoms) {
-      PMEFloat x = cSim.pImageX[pos];
-      PMEFloat y = cSim.pImageY[pos];
-      PMEFloat z = cSim.pImageZ[pos];
+      // Read wrapped Cartesian coordinates prepared for hashing
+      PMEFloat x = (PMEFloat)cSim.pImageX[pos];
+      PMEFloat y = (PMEFloat)cSim.pImageY[pos];
+      PMEFloat z = (PMEFloat)cSim.pImageZ[pos];
 
       // Orthogonal/nonorthogonal handled in the same code (3 single precision
       // multiplies and adds? Who cares and why?)
@@ -227,6 +366,16 @@ kNLGenerateSpatialHash_kernel()
       unsigned int hash     = (((iz*cSim.ycells + iy)*cSim.xcells + ix) << CELL_HASH_BITS) |
                               cellHash;
       cSim.pImageHash2[pos] = hash;
+
+      // DEBUG: sample hash input for first few atoms (shifted into [0,box] frame to match master logs)
+      if ((blockIdx.x == 0) && (threadIdx.x < 2)) {
+        double xs = (double)x + 0.5 * cSim.a;
+        double ys = (double)y + 0.5 * cSim.b;
+        double zs = (double)z + 0.5 * cSim.c;
+        // printf("HASH_DEBUG pos=%u x=%.6f y=%.6f z=%.6f xs=%.6f ys=%.6f zs=%.6f fx=%.6f fy=%.6f fz=%.6f ix=%u iy=%u iz=%u hash=%u cellID=%u\n",
+        //        pos, (double)x, (double)y, (double)z, xs, ys, zs,
+        //        (double)fx, (double)fy, (double)fz, ix, iy, iz, hash, cSim.pImageCellID[pos]);
+      }
       pos += increment;
     }
   }
@@ -253,6 +402,10 @@ extern "C" void kNLGenerateSpatialHash(gpuContext gpu)
 //                       follow the radix sort of the atoms that occurs when a new neighborlist
 //                       is built
 //
+// MINT32 FIX: Coordinates are now read from pAtomCoord (MINT32 source of truth) and
+// converted to double-precision. This ensures coordinates are current after integration,
+// rather than stale from the previous neighbor list build.
+//
 // Arguments:
 //   pImageIndex: buffer containing pointers to all remapped atom indexes. Provides the
 //                connection from the previous position in the atom list to the position in the
@@ -278,11 +431,18 @@ kNLRemapImage_kernel(unsigned int* pImageIndex)
       newindex = pImageIndex[newpos];
     }
 
-    // Read new data
-    unsigned int atom   = cSim.pImageAtom[index];
-    double x            = cSim.pImageX[index];
-    double y            = cSim.pImageY[index];
-    double z            = cSim.pImageZ[index];
+    // pImageIndex stores the previous neighbor-list position; map through pImageAtom
+    // to recover the original atom ID.
+    unsigned int atom = cSim.pImageAtom[index];
+
+    // Read MINT32 from unsorted array for remapping pAtomCoord_M32, and derive
+    // fresh float coords from the same source to keep frames in sync.
+    int4 coord_m32      = cSim.pAtomCoordUnsorted_M32[atom];
+    double x            = (double)coord_m32.x * cSim.mint32_inv_scale.x;
+    double y            = (double)coord_m32.y * cSim.mint32_inv_scale.y;
+    double z            = (double)coord_m32.z * cSim.mint32_inv_scale.z;
+
+    // Read velocities from OLD sorted position like baseline does
     double vx           = cSim.pImageVelX[index];
     double vy           = cSim.pImageVelY[index];
     double vz           = cSim.pImageVelZ[index];
@@ -323,22 +483,50 @@ kNLRemapImage_kernel(unsigned int* pImageIndex)
       fpsg=cSim.pImageFPsg[index];
       ppsg=cSim.pImagePPsg[index];
     }
+    // Lennard-Jones type from previous ordering
 #ifdef use_DPFP
     long long int LJID  = cSim.pImageLJID[index];
 #else
     unsigned int LJID   = cSim.pImageLJID[index];
 #endif
     unsigned int cellID = cSim.pImageCellID[index];
+    
     cSim.pImageX2[pos] = x;
     cSim.pImageY2[pos] = y;
     cSim.pImageZ2[pos] = z;
     cSim.pImageAtom2[pos] = atom;
     cSim.pImageAtomLookup[atom] = pos;
 
-    // Have to form the tuple separately before committing it to the array
-    PMEFloat2 xy = {(PMEFloat)x, (PMEFloat)y};
-    cSim.pAtomXYSaveSP[pos]  = xy;
-    cSim.pAtomZSaveSP[pos]   = z;
+    // DEBUG: broader mapping sample to catch higher-index corruption
+    if ((blockIdx.x == 0) && (pos < 8)) {
+      // printf("MAP_SAMPLE pos=%u atom=%u oldIndex=%u lookup=%u coord=(%d,%d,%d)\n",
+      //        pos, atom, index, cSim.pImageAtomLookup[atom], coord_m32.x, coord_m32.y, coord_m32.z);
+    }
+
+    // DEBUG: velocity mapping sample (sorted old -> sorted new, plus unsorted stash)
+    if ((blockIdx.x == 0) && (pos < 4)) {
+      double unsorted_vx = cSim.pVelX[atom];
+      double unsorted_vy = cSim.pVelY[atom];
+      double unsorted_vz = cSim.pVelZ[atom];
+      // printf("VEL_SAMPLE pos=%u atom=%u oldIndex=%u newPos=%u v_old_sorted=(%.6f,%.6f,%.6f) "
+      //        "v_new_sorted=(%.6f,%.6f,%.6f) v_unsorted=(%.6f,%.6f,%.6f)\n",
+      //        pos, atom, index, pos, vx, vy, vz,
+      //        cSim.pImageVelX2[pos], cSim.pImageVelY2[pos], cSim.pImageVelZ2[pos],
+      //        unsorted_vx, unsorted_vy, unsorted_vz);
+    }
+
+    // NOTE: pAtomXYSP/ZSP are populated by kMINT32ToFloat AFTER remap completes
+    // to avoid race condition (pAtomXYSP has no double buffer)
+
+    // MINT32 FIX: Since we read from pAtomCoord (already MINT32), just copy directly
+    // to both the active sorted buffer and the save buffer.
+    cSim.pAtomCoord_M32[pos]     = coord_m32;
+    cSim.pAtomCoordSave_M32[pos] = coord_m32;
+
+    // MINT32 FIX: Remap old coordinates using the unsorted source of truth (race-free)
+    int4 old_coord_m32 = cSim.pOldAtomCoordUnsorted_M32[atom];
+    cSim.pOldAtomCoord_M32[pos] = old_coord_m32;
+
     cSim.pImageVelX2[pos]    = vx;
     cSim.pImageVelY2[pos]    = vy;
     cSim.pImageVelZ2[pos]    = vz;
@@ -353,6 +541,56 @@ kNLRemapImage_kernel(unsigned int* pImageIndex)
     cSim.pImageLJID2[pos]    = LJID;
     cSim.pImageCellID2[pos]  = cellID;
 
+    // DEBUG: Sample velocity remap mapping (old sorted index -> new sorted position)
+    if ((blockIdx.x == 0) && (pos < 2)) {
+      // printf("VEL_REMAP oldIndex=%u newPos=%u atom=%u vx=%f vy=%f vz=%f\n",
+      //        index, pos, atom, vx, vy, vz);
+    }
+
+    // DEBUG: velocity mapping sample (sorted old -> sorted new, plus unsorted stash)
+    if ((blockIdx.x == 0) && (pos < 4)) {
+      double unsorted_vx = cSim.pVelX[atom];
+      double unsorted_vy = cSim.pVelY[atom];
+      double unsorted_vz = cSim.pVelZ[atom];
+      // printf("VEL_SAMPLE pos=%u atom=%u oldIndex=%u newPos=%u v_old_sorted=(%.6f,%.6f,%.6f) "
+      //        "v_new_sorted=(%.6f,%.6f,%.6f) v_unsorted=(%.6f,%.6f,%.6f)\n",
+      //        pos, atom, index, pos, vx, vy, vz,
+      //        cSim.pImageVelX2[pos], cSim.pImageVelY2[pos], cSim.pImageVelZ2[pos],
+      //        unsorted_vx, unsorted_vy, unsorted_vz);
+    }
+
+    // DEBUG: distance check for a known bond (atom IDs 0 and 1)
+    if ((atom == 0) || (atom == 1)) {
+      int other = (atom == 0) ? 1 : 0;
+      int4 other_m32 = cSim.pAtomCoordUnsorted_M32[other];
+      // Integer-wrapped distance
+      int dx_i = coord_m32.x - other_m32.x;
+      int dy_i = coord_m32.y - other_m32.y;
+      int dz_i = coord_m32.z - other_m32.z;
+      double r_i =
+        sqrt((double)dx_i * cSim.mint32_inv_scale.x * (double)dx_i * cSim.mint32_inv_scale.x +
+             (double)dy_i * cSim.mint32_inv_scale.y * (double)dy_i * cSim.mint32_inv_scale.y +
+             (double)dz_i * cSim.mint32_inv_scale.z * (double)dz_i * cSim.mint32_inv_scale.z);
+      // Float distance using current float coords (centered)
+      double ofx = (double)other_m32.x * cSim.mint32_inv_scale.x;
+      double ofy = (double)other_m32.y * cSim.mint32_inv_scale.y;
+      double ofz = (double)other_m32.z * cSim.mint32_inv_scale.z;
+      double r_f = sqrt((x - ofx) * (x - ofx) + (y - ofy) * (y - ofy) + (z - ofz) * (z - ofz));
+      double xs  = x   + 0.5 * cSim.a;
+      double ys  = y   + 0.5 * cSim.b;
+      double zs  = z   + 0.5 * cSim.c;
+      double ofxs = ofx + 0.5 * cSim.a;
+      double ofys = ofy + 0.5 * cSim.b;
+      double ofzs = ofz + 0.5 * cSim.c;
+      // printf("DIST_SAMPLE atom=%u other=%d pos=%u lookup=%u coord=(%d,%d,%d) "
+      //        "other_coord=(%d,%d,%d) r_i=%.6f r_f=%.6f xyz=(%.6f,%.6f,%.6f) "
+      //        "xyz_shift=(%.6f,%.6f,%.6f) other_xyz=(%.6f,%.6f,%.6f) other_shift=(%.6f,%.6f,%.6f)\n",
+      //        atom, other, pos, cSim.pImageAtomLookup[atom],
+      //        coord_m32.x, coord_m32.y, coord_m32.z,
+      //        other_m32.x, other_m32.y, other_m32.z,
+      //        r_i, r_f, x, y, z, xs, ys, zs, ofx, ofy, ofz, ofxs, ofys, ofzs);
+    }
+
     if(cSim.iphmd == 3) {
       cSim.pImageGrplist2[pos] = grplist;
       cSim.pImageQstate12[pos] = qstate1;
@@ -389,6 +627,12 @@ kNLRemapImage_kernel(unsigned int* pImageIndex)
 #endif
     cSim.pAtomChargeSPLJID[pos] = qljid;
 
+    // DEBUG: Sample a couple of velocity remaps (old sorted index -> new sorted position)
+    if ((blockIdx.x == 0) && (pos < 2)) {
+      // printf("VEL_REMAP oldIndex=%u newPos=%u atom=%u vx=%f vy=%f vz=%f\n",
+      //        index, pos, atom, vx, vy, vz);
+    }
+
     // Advance to next atom
     index = newindex;
     pos = newpos;
@@ -399,6 +643,9 @@ kNLRemapImage_kernel(unsigned int* pImageIndex)
 // kNLTIRemapImage_kernel: same as kNLRemapImage_kernel, but works in the context of alchemical
 //                         free energy transformations or thermodynamic integration.
 //
+// MINT32 FIX: Coordinates are now read from pAtomCoord (MINT32 source of truth) and
+// converted to double-precision. This ensures coordinates are current after integration.
+//
 // Arguments:
 //   pImageIndex: buffer containing pointers to all remapped atom indexes. Provides the
 //                connection from the previous position in the atom list to the position in the
@@ -422,11 +669,17 @@ kNLTIRemapImage_kernel(unsigned int* pImageIndex)
       newindex = pImageIndex[newpos];
     }
 
-    // Read new data
+    // Read new data from previous ordering via pImageAtom
     unsigned int atom           = cSim.pImageAtom[index];
-    double x                    = cSim.pImageX[index];
-    double y                    = cSim.pImageY[index];
-    double z                    = cSim.pImageZ[index];
+    
+    // Read MINT32 from unsorted array for remapping pAtomCoord_M32, and derive
+    // float coords from the same source to keep float/MINT32 frames aligned.
+    int4 coord_m32              = cSim.pAtomCoordUnsorted_M32[atom];
+    double x                    = (double)coord_m32.x * cSim.mint32_inv_scale.x;
+    double y                    = (double)coord_m32.y * cSim.mint32_inv_scale.y;
+    double z                    = (double)coord_m32.z * cSim.mint32_inv_scale.z;
+
+    // Read velocities from OLD sorted position like baseline does
     double vx                   = cSim.pImageVelX[index];
     double vy                   = cSim.pImageVelY[index];
     double vz                   = cSim.pImageVelZ[index];
@@ -467,6 +720,7 @@ kNLTIRemapImage_kernel(unsigned int* pImageIndex)
       fpsg=cSim.pImageFPsg[index];
       ppsg=cSim.pImagePPsg[index];
     }
+    // Lennard-Jones ID carried over from previous ordering
 #ifdef use_DPFP
     long long int LJID          = cSim.pImageLJID[index];
 #else
@@ -479,9 +733,17 @@ kNLTIRemapImage_kernel(unsigned int* pImageIndex)
     cSim.pImageZ2[pos]          = z;
     cSim.pImageAtom2[pos]       = atom;
     cSim.pImageAtomLookup[atom] = pos;
-    PMEFloat2 xy                = {(PMEFloat)x, (PMEFloat)y};
-    cSim.pAtomXYSaveSP[pos]     = xy;
-    cSim.pAtomZSaveSP[pos]      = z;
+
+
+    // MINT32 FIX: Since we read from pAtomCoord (already MINT32), just copy directly
+    // to both the active sorted buffer and the save buffer.
+    cSim.pAtomCoord_M32[pos]     = coord_m32;
+    cSim.pAtomCoordSave_M32[pos] = coord_m32;
+
+    // MINT32 FIX: Remap old coordinates using the unsorted source of truth
+    int4 old_coord_m32 = cSim.pOldAtomCoordUnsorted_M32[atom];
+    cSim.pOldAtomCoord_M32[pos] = old_coord_m32;
+
     cSim.pImageVelX2[pos]       = vx;
     cSim.pImageVelY2[pos]       = vy;
     cSim.pImageVelZ2[pos]       = vz;
@@ -532,6 +794,16 @@ kNLTIRemapImage_kernel(unsigned int* pImageIndex)
 #endif
     cSim.pAtomChargeSPLJID[pos] = qljid;
 
+    if ((blockIdx.x == 0) && (pos < 2)) {
+      // printf("VEL_REMAP oldIndex=%u newPos=%u atom=%u vx=%f vy=%f vz=%f\n",
+      //        index, pos, atom, vx, vy, vz);
+    }
+
+    if ((blockIdx.x == 0) && (pos < 2)) {
+      // printf("VEL_REMAP oldIndex=%u newPos=%u atom=%u vx=%f vy=%f vz=%f\n",
+      //        index, pos, atom, vx, vy, vz);
+    }
+
     // Advance to next atom
     index = newindex;
     pos   = newpos;
@@ -542,6 +814,9 @@ kNLTIRemapImage_kernel(unsigned int* pImageIndex)
 // kNLTINoLinearAtmRemapImage_kernel: same as kNLTIRemapImage_kernel, but for cases with no
 //                                    linear scaling atoms
 //
+// MINT32 FIX: Coordinates are now read from pAtomCoord (MINT32 source of truth) and
+// converted to double-precision. This ensures coordinates are current after integration.
+//
 // Arguments:
 //   pImageIndex: buffer containing pointers to all remapped atom indexes. Provides the
 //                connection from the previous position in the atom list to the position in the
@@ -565,11 +840,17 @@ kNLTINoLinearAtmRemapImage_kernel(unsigned int* pImageIndex)
       newindex = pImageIndex[newpos];
     }
 
-    // Read new data
+    // Read new data from previous ordering via pImageAtom
     unsigned int atom           = cSim.pImageAtom[index];
-    double x                    = cSim.pImageX[index];
-    double y                    = cSim.pImageY[index];
-    double z                    = cSim.pImageZ[index];
+    
+    // Read MINT32 from unsorted array for remapping pAtomCoord_M32, and derive
+    // float coords from the same source to keep float/MINT32 frames aligned.
+    int4 coord_m32              = cSim.pAtomCoordUnsorted_M32[atom];
+    double x                    = (double)coord_m32.x * cSim.mint32_inv_scale.x;
+    double y                    = (double)coord_m32.y * cSim.mint32_inv_scale.y;
+    double z                    = (double)coord_m32.z * cSim.mint32_inv_scale.z;
+
+    // Read velocities from OLD sorted position like baseline does
     double vx                   = cSim.pImageVelX[index];
     double vy                   = cSim.pImageVelY[index];
     double vz                   = cSim.pImageVelZ[index];
@@ -610,6 +891,7 @@ kNLTINoLinearAtmRemapImage_kernel(unsigned int* pImageIndex)
       fpsg=cSim.pImageFPsg[index];
       ppsg=cSim.pImagePPsg[index];
     }
+    // Lennard-Jones ID carried over from previous ordering
 #ifdef use_DPFP
     long long int LJID          = cSim.pImageLJID[index];
 #else
@@ -622,9 +904,19 @@ kNLTINoLinearAtmRemapImage_kernel(unsigned int* pImageIndex)
     cSim.pImageZ2[pos]          = z;
     cSim.pImageAtom2[pos]       = atom;
     cSim.pImageAtomLookup[atom] = pos;
-    PMEFloat2 xy                = {(PMEFloat)x, (PMEFloat)y};
-    cSim.pAtomXYSaveSP[pos]     = xy;
-    cSim.pAtomZSaveSP[pos]      = z;
+
+    // NOTE: pAtomXYSP/ZSP are populated by kMINT32ToFloat AFTER remap completes
+    // to avoid race condition (pAtomXYSP has no double buffer)
+
+    // MINT32 FIX: Since we read from pAtomCoord (already MINT32), just copy directly
+    // to both the active sorted buffer and the save buffer.
+    cSim.pAtomCoord_M32[pos]     = coord_m32;
+    cSim.pAtomCoordSave_M32[pos] = coord_m32;
+
+    // MINT32 FIX: Remap old coordinates using the unsorted source of truth
+    int4 old_coord_m32 = cSim.pOldAtomCoordUnsorted_M32[atom];
+    cSim.pOldAtomCoord_M32[pos] = old_coord_m32;
+
     cSim.pImageVelX2[pos]       = vx;
     cSim.pImageVelY2[pos]       = vy;
     cSim.pImageVelZ2[pos]       = vz;
@@ -718,6 +1010,9 @@ kNLRemapLinearAtmID_kernel(unsigned int* pImageIndex)
 //---------------------------------------------------------------------------------------------
 extern "C" void kNLRemapImage(gpuContext gpu)
 {
+  // MINT32 FIX: Sync sorted coords to unsorted buffers to serve as race-free source for remapping
+  kNLUnsortCoordinates(gpu);
+
   if (gpu->sim.ti_mode == 0) {
     kNLRemapImage_kernel<<<gpu->blocks, gpu->threadsPerBlock>>>(gpu->sim.pImageIndex);
   }
@@ -733,6 +1028,7 @@ extern "C" void kNLRemapImage(gpuContext gpu)
     }
   }
   LAUNCHERROR("kNLRemapImage");
+  
   unsigned int *pUint;
   int* pInt;
   double* pDouble;
@@ -886,6 +1182,38 @@ extern "C" void kNLRemapImage(gpuContext gpu)
     }
 }
 
+//---------------------------------------------------------------------------------------------
+// kNLUnsortCoordinates_kernel: Sync sorted coords (Array B) back to unsorted (Array A)
+//
+// MINT32 FIX: This runs BEFORE remapping. It syncs the current sorted coordinates (and old coords)
+// to the unsorted arrays. This allows the unsorted arrays to act as a safe, double-buffered
+// source for the remapping operation (avoiding B->B race conditions).
+//---------------------------------------------------------------------------------------------
+__global__ void __LAUNCH_BOUNDS__(128, 8) kNLUnsortCoordinates_kernel()
+{
+  int sortedIdx = blockIdx.x * blockDim.x + threadIdx.x;
+  if (sortedIdx < cSim.atoms) {
+    // Get original atom index from sorted position (using current pImageAtom)
+    int atom = cSim.pImageAtom[sortedIdx];
+
+    // Copy from sorted array (B) to unsorted array (A)
+    cSim.pAtomCoordUnsorted_M32[atom] = cSim.pAtomCoord_M32[sortedIdx];
+    cSim.pOldAtomCoordUnsorted_M32[atom] = cSim.pOldAtomCoord_M32[sortedIdx];
+  }
+}
+
+extern "C" void kNLUnsortCoordinates(gpuContext gpu)
+{
+  int atoms = gpu->sim.atoms;
+  int threads = 128;
+  int blocks = (atoms + threads - 1) / threads;
+
+  kNLUnsortCoordinates_kernel<<<blocks, threads>>>();
+
+  cudaError_t status = cudaGetLastError();
+  RTERROR(status, "kNLUnsortCoordinates kernel launch failed");
+}
+
 //---------------------------------------------------------------------------------------------
 // kNLRemapLocalInteractions_kernel: kernel for remapping local (read: bonded) interactions,
 //                                   delegating them for processing within the groupings of the
@@ -1787,6 +2115,51 @@ kNLRemapBondWorkUnits_kernel()
     if (atomID <= 0x7fffff) {
       cSim.pBwuInstructions[writepos] = cSim.pImageAtomLookup[atomID];
     }
+
+    // DEBUG: sample the first work unit mapping to inspect the coord buffer used by BWU
+    if (wuidx == 0 && threadIdx.x == 0) {
+      int mapped = cSim.pImageAtomLookup[atomID];
+      int4 c = cSim.pAtomCoord_M32[mapped];
+      double fx = (double)c.x * cSim.mint32_inv_scale.x;
+      double fy = (double)c.y * cSim.mint32_inv_scale.y;
+      double fz = (double)c.z * cSim.mint32_inv_scale.z;
+      double sfx = fx + 0.5 * cSim.a;
+      double sfy = fy + 0.5 * cSim.b;
+      double sfz = fz + 0.5 * cSim.c;
+      // printf("BWU_MAP wuidx=%d atomID=%d mapped=%d coord=(%d,%d,%d) xyz=(%.6f,%.6f,%.6f) xyz_shift=(%.6f,%.6f,%.6f)\n",
+      //        wuidx, atomID, mapped, c.x, c.y, c.z, x, y, z, xs, ys, zs);
+
+      // DEBUG: map/coord samples for first few sorted positions
+      for (int i = 0; i < 8 && i < cSim.atoms; i++) {
+        unsigned int a = cSim.pImageAtom[i];
+        unsigned int lk = cSim.pImageAtomLookup[a];
+        int4 cc = cSim.pAtomCoord_M32[lk];
+      // printf("MAP_SAMPLE_BWU pos=%d atom=%u lookup=%u coord=(%d,%d,%d)\n",
+      //        i, atom, cSim.pImageAtomLookup[atom], c.x, c.y, c.z);
+      }
+
+      // DEBUG: distance sample for bond (0,1) using MINT32 and floats
+      if (cSim.atoms > 1) {
+        int4 c0 = cSim.pAtomCoordUnsorted_M32[0];
+        int4 c1 = cSim.pAtomCoordUnsorted_M32[1];
+        int dx_i = c0.x - c1.x;
+        int dy_i = c0.y - c1.y;
+        int dz_i = c0.z - c1.z;
+        double r_i =
+          sqrt((double)dx_i * cSim.mint32_inv_scale.x * (double)dx_i * cSim.mint32_inv_scale.x +
+               (double)dy_i * cSim.mint32_inv_scale.y * (double)dy_i * cSim.mint32_inv_scale.y +
+               (double)dz_i * cSim.mint32_inv_scale.z * (double)dz_i * cSim.mint32_inv_scale.z);
+        double fx0 = (double)c0.x * cSim.mint32_inv_scale.x;
+        double fy0 = (double)c0.y * cSim.mint32_inv_scale.y;
+        double fz0 = (double)c0.z * cSim.mint32_inv_scale.z;
+        double fx1 = (double)c1.x * cSim.mint32_inv_scale.x;
+        double fy1 = (double)c1.y * cSim.mint32_inv_scale.y;
+        double fz1 = (double)c1.z * cSim.mint32_inv_scale.z;
+        double r_f = sqrt((fx0 - fx1)*(fx0 - fx1) + (fy0 - fy1)*(fy0 - fy1) + (fz0 - fz1)*(fz0 - fz1));
+              // printf("DIST_SAMPLE_BWU atoms(0,1): r_i=%.6f r_f=%.6f c0=(%d,%d,%d) c1=(%d,%d,%d) fx0=%.6f fy0=%.6f fz0=%.6f fx1=%.6f fy1=%.6f fz1=%.6f\n",
+              //        r_i, r_f, c0.x, c0.y, c0.z, c1.x, c1.y, c1.z, fx0, fy0, fz0, fx1, fy1, fz1);
+      }
+    }
     wuidx += gridDim.x;
   }
 }
@@ -2134,6 +2507,29 @@ extern "C" void kNeighborListInitKernels(gpuContext gpu)
 //---------------------------------------------------------------------------------------------
 extern "C" void kNLBuildNeighborList(gpuContext gpu)
 {
+  
+  int bnlDebugStage = 0;
+  const char* bnlStageEnv = getenv("MINT32_BNL_DEBUG_STAGE");
+  if (bnlStageEnv != NULL) {
+    bnlDebugStage = atoi(bnlStageEnv);
+  }
+  
+  cudaError_t bnlStageStatus =
+    cudaMemcpyToSymbol(cBNLDebugStage, &bnlDebugStage, sizeof(int));
+  RTERROR(bnlStageStatus, "cudaMemcpyToSymbol cBNLDebugStage failed");
+  
+  int bnlCellLimit = -1;
+  const char* bnlCellLimitEnv = getenv("MINT32_BNL_CELL_LIMIT");
+  if (bnlCellLimitEnv != NULL) {
+    bnlCellLimit = atoi(bnlCellLimitEnv);
+  }
+  
+  cudaError_t bnlCellStatus =
+    cudaMemcpyToSymbol(cBNLDebugCellLimit, &bnlCellLimit, sizeof(int));
+  RTERROR(bnlCellStatus, "cudaMemcpyToSymbol cBNLDebugCellLimit failed");
+  
+
+  
   if (gpu->sim.NLAtomsPerWarp == 32) {
     if ((gpu->sim.ntp > 0) && (gpu->sim.barostat != 2)) {
       if (gpu->sim.is_orthog)
@@ -2229,10 +2625,11 @@ __global__ void __LAUNCH_BOUNDS__(GENERAL_THREADS_PER_BLOCK, 1) kNLSkinTestCount
     PMEFloat xi = cSim.pImageX[i];
     PMEFloat yi = cSim.pImageY[i];
     PMEFloat zi = cSim.pImageZ[i];
-    PMEFloat2 oldxy = cSim.pAtomXYSaveSP[i];
-    PMEFloat xiold  = oldxy.x;
-    PMEFloat yiold  = oldxy.y;
-    PMEFloat ziold  = cSim.pAtomZSaveSP[i];
+    int4 oldcoord = cSim.pAtomCoordSave_M32[i];
+    PMEFloat xiold = (PMEFloat)oldcoord.x * cSim.mint32_inv_scale.x;
+    PMEFloat yiold = (PMEFloat)oldcoord.y * cSim.mint32_inv_scale.y;
+    PMEFloat ziold = (PMEFloat)oldcoord.z * cSim.mint32_inv_scale.z;
+    PMEFloat2 oldxy; oldxy.x = xiold; oldxy.y = yiold;
     while (j < cSim.atoms) {
       PMEFloat dx = cSim.pImageX[j] - xi;
       PMEFloat dy = cSim.pImageY[j] - yi;
@@ -2249,10 +2646,12 @@ __global__ void __LAUNCH_BOUNDS__(GENERAL_THREADS_PER_BLOCK, 1) kNLSkinTestCount
       PMEFloat r2 = dx*dx + dy*dy + dz*dz;
       if (r2 < cSim.cut2) {
         npair++;
-        oldxy           = cSim.pAtomXYSaveSP[j];
+        int4 oldcoordj = cSim.pAtomCoordSave_M32[j];
+        oldxy.x = (PMEFloat)oldcoordj.x * cSim.mint32_inv_scale.x;
+        oldxy.y = (PMEFloat)oldcoordj.y * cSim.mint32_inv_scale.y;
         PMEFloat dxold  = oldxy.x - xiold;
         PMEFloat dyold  = oldxy.y - yiold;
-        PMEFloat dzold  = cSim.pAtomZSaveSP[i] - ziold;
+        PMEFloat dzold  = ((PMEFloat)oldcoordj.z * cSim.mint32_inv_scale.z) - ziold;
         PMEFloat ndxold = sRecipf[0]*dxold + sRecipf[3]*dyold + sRecipf[6]*dzold;
         PMEFloat ndyold =                    sRecipf[4]*dyold + sRecipf[7]*dzold;
         PMEFloat ndzold =                                       sRecipf[8]*dzold;
@@ -2323,9 +2722,12 @@ __global__ void __LAUNCH_BOUNDS__(GENERAL_THREADS_PER_BLOCK, 1) kNLSkinTestCount
   while (pos < cSim.atoms) {
     PMEFloat x      = cSim.pImageX[pos];
     PMEFloat y      = cSim.pImageY[pos];
-    PMEFloat2 oldxy = cSim.pAtomXYSaveSP[pos];
-    PMEFloat z      = cSim.pImageZ[pos];
-    PMEFloat oldz   = cSim.pAtomZSaveSP[pos];
+    int4 oldcoord = cSim.pAtomCoordSave_M32[pos];
+    PMEFloat2 oldxy;
+    oldxy.x = (PMEFloat)oldcoord.x * cSim.mint32_inv_scale.x;
+    oldxy.y = (PMEFloat)oldcoord.y * cSim.mint32_inv_scale.y;
+    PMEFloat z = cSim.pImageZ[pos];
+    PMEFloat oldz = (PMEFloat)oldcoord.z * cSim.mint32_inv_scale.z;
     PMEFloat dx     = x - oldxy.x;
     PMEFloat dy     = y - oldxy.y;
     PMEFloat dz     = z - oldz;
@@ -2360,6 +2762,7 @@ __global__ void __LAUNCH_BOUNDS__(GENERAL_THREADS_PER_BLOCK, 1) kNLSkinTestCount
 //---------------------------------------------------------------------------------------------
 // kNLSkinTest_kernel: kernel to test all atom movements against the current pair list.  If
 //                     one has moved too far from its original position, the "skin" test fails.
+//                     Uses MINT32 integer subtraction for automatic periodic boundary handling.
 //---------------------------------------------------------------------------------------------
 __global__ void
 __LAUNCH_BOUNDS__(GENERAL_THREADS_PER_BLOCK, 1)
@@ -2376,15 +2779,16 @@ kNLSkinTest_kernel()
 
   if ((cSim.ntp > 0) && (cSim.barostat != 2)) {
     while (pos < cSim.atoms) {
-      PMEFloat x      = cSim.pImageX[pos];
-      PMEFloat y      = cSim.pImageY[pos];
-      PMEFloat2 oldxy = cSim.pAtomXYSaveSP[pos];
-      PMEFloat z      = cSim.pImageZ[pos];
-      PMEFloat oldz   = cSim.pAtomZSaveSP[pos];
-      PMEFloat dx     = x - oldxy.x;
-      PMEFloat dy     = y - oldxy.y;
-      PMEFloat dz     = z - oldz;
-      PMEFloat r2     = dx*dx + dy*dy + dz*dz;
+      // Use MINT32 subtraction for automatic periodic boundary handling
+      int4 newcoord = cSim.pAtomCoord_M32[pos];
+      int4 oldcoord = cSim.pAtomCoordSave_M32[pos];
+      int dx_i32 = newcoord.x - oldcoord.x;
+      int dy_i32 = newcoord.y - oldcoord.y;
+      int dz_i32 = newcoord.z - oldcoord.z;
+      PMEFloat dx = (PMEFloat)dx_i32 * cSim.mint32_inv_scale.x;
+      PMEFloat dy = (PMEFloat)dy_i32 * cSim.mint32_inv_scale.y;
+      PMEFloat dz = (PMEFloat)dz_i32 * cSim.mint32_inv_scale.z;
+      PMEFloat r2 = dx*dx + dy*dy + dz*dz;
       if (r2 >= sOne_half_nonbond_skin_squared) {
         sbFail = true;
       }
@@ -2393,22 +2797,23 @@ kNLSkinTest_kernel()
   }
   else {
     while (pos < cSim.atoms) {
-      PMEFloat x      = cSim.pImageX[pos];
-      PMEFloat y      = cSim.pImageY[pos];
-      PMEFloat2 oldxy = cSim.pAtomXYSaveSP[pos];
-      PMEFloat z      = cSim.pImageZ[pos];
-      PMEFloat oldz   = cSim.pAtomZSaveSP[pos];
-      PMEFloat dx     = x - oldxy.x;
-      PMEFloat dy     = y - oldxy.y;
-      PMEFloat dz     = z - oldz;
-      PMEFloat r2     = dx*dx + dy*dy + dz*dz;
+      // Use MINT32 subtraction for automatic periodic boundary handling
+      int4 newcoord = cSim.pAtomCoord_M32[pos];
+      int4 oldcoord = cSim.pAtomCoordSave_M32[pos];
+      int dx_i32 = newcoord.x - oldcoord.x;
+      int dy_i32 = newcoord.y - oldcoord.y;
+      int dz_i32 = newcoord.z - oldcoord.z;
+      PMEFloat dx = (PMEFloat)dx_i32 * cSim.mint32_inv_scale.x;
+      PMEFloat dy = (PMEFloat)dy_i32 * cSim.mint32_inv_scale.y;
+      PMEFloat dz = (PMEFloat)dz_i32 * cSim.mint32_inv_scale.z;
+      PMEFloat r2 = dx*dx + dy*dy + dz*dz;
       if (r2 >= cSim.one_half_nonbond_skin_squared) {
         sbFail = true;
       }
       pos += blockDim.x * gridDim.x;
     }
   }
-  __syncthreads();
+  __syncthreads();;
 #if defined(AMBER_PLATFORM_AMD)
   if (threadIdx.x == 0) {
     // Pack total and failed block counts in one counter
@@ -2440,6 +2845,15 @@ kNLSkinTest_kernel()
 
 extern "C" void kNLSkinTest(gpuContext gpu)
 {
+  // Reset fail flag before launching to avoid stale values triggering rebuilds
+  if (gpu->bCanMapHostMemory) {
+    gpu->pbNLbSkinTestFail->_pSysData[0] = false;
+  }
+  else {
+    *(gpu->pbNLbSkinTestFail->_pSysData) = false;
+    gpu->pbNLbSkinTestFail->Upload();
+  }
+
   kNLSkinTest_kernel<<<gpu->blocks, gpu->generalThreadsPerBlock>>>();
   LAUNCHERROR("kNLSkinTest");
 
diff --git "a/C:\\amber\\amber_master\\src\\pmemd\\src/cuda/kPCCI.h" "b/C:\\amber\\amber_mint\\src\\pmemd\\src/cuda/kPCCI.h"
index 29dfb28..bf6bebb 100644
--- "a/C:\\amber\\amber_master\\src\\pmemd\\src/cuda/kPCCI.h"
+++ "b/C:\\amber\\amber_mint\\src\\pmemd\\src/cuda/kPCCI.h"
@@ -52,10 +52,14 @@
   while (pos < clim.y) {
 
     // Load x, y, and z coordinates from global memory
-    PMEFloat2 xy = cSim.pAtomXYSP[pos];
+    int4 coord = cSim.pAtomCoord_M32[pos];
+    PMEFloat2 xy;
+    xy.x = (PMEFloat)coord.x * cSim.mint32_inv_scale.x;
+    xy.y = (PMEFloat)coord.y * cSim.mint32_inv_scale.y;
+    PMEFloat z   = (PMEFloat)coord.z * cSim.mint32_inv_scale.z;
     double xcrd = xy.x;
     double ycrd = xy.y;
-    double zcrd = cSim.pAtomZSP[pos];
+    double zcrd = z;
 
 #ifdef PME_IS_ORTHOGONAL
     // Convert to fractional coordinates
diff --git "a/C:\\amber\\amber_master\\src\\pmemd\\src/cuda/kPMEInterpolation.cu" "b/C:\\amber\\amber_mint\\src\\pmemd\\src/cuda/kPMEInterpolation.cu"
index 9e37995..b76b8bd 100644
--- "a/C:\\amber\\amber_master\\src\\pmemd\\src/cuda/kPMEInterpolation.cu"
+++ "b/C:\\amber\\amber_mint\\src\\pmemd\\src/cuda/kPMEInterpolation.cu"
@@ -9,6 +9,8 @@
 #ifndef AMBER_PLATFORM_AMD
 #include <cuda.h>
 #endif
+#include <cstdio>
+#include <cstdlib>
 #include "gpu.h"
 #include "ptxmacros.h"
 
@@ -32,6 +34,64 @@ void SetkPMEInterpolationSim(gpuContext gpu)
   RTERROR(status, "cudaMemcpyToSymbol: SetSim copy to cSim failed");
 }
 
+// Optional dump of PME charge-mapping inputs for debugging.
+// Controlled by env var MINT32_PME_SAMPLE (nonzero => dump once to /tmp/mint32_pme_sample.log).
+static void Mint32DumpPMEInputs(gpuContext gpu)
+{
+  static int dumpOnce = -1;
+  if (dumpOnce == -1) {
+    const char* env = getenv("MINT32_PME_SAMPLE");
+    dumpOnce = (env != nullptr && atoi(env) != 0) ? 0 : 1;
+  }
+  if (dumpOnce != 0) return;
+  dumpOnce = 1;
+
+  if (gpu == nullptr || gpu->pbFract == nullptr || gpu->pbAtomChargeSP == nullptr ||
+      gpu->pbAtomCoord_M32 == nullptr) {
+    return;
+  }
+
+  gpu->pbFract->Download();
+  gpu->pbAtomChargeSP->Download();
+  gpu->pbAtomCoord_M32->Download();
+
+  FILE* f = fopen("/tmp/mint32_pme_sample.log", "a");
+  if (f == nullptr) return;
+
+  fprintf(f, "PME_INPUT nfft=(%d,%d,%d) atoms=%d stride=%d mint32_inv_scale=(%g,%g,%g)\n",
+          gpu->sim.nfft1, gpu->sim.nfft2, gpu->sim.nfft3, gpu->sim.atoms, gpu->sim.stride,
+          gpu->sim.mint32_inv_scale.x, gpu->sim.mint32_inv_scale.y, gpu->sim.mint32_inv_scale.z);
+
+  int sample = (gpu->sim.atoms < 8) ? gpu->sim.atoms : 8;
+  PMEFloat total_q = 0.0f;
+  for (int i = 0; i < gpu->sim.atoms; i++) {
+    total_q += gpu->pbAtomChargeSP->_pSysData[i];
+  }
+  fprintf(f, "total_charge=%.6f\n", (double)total_q);
+
+  for (int i = 0; i < sample; i++) {
+    PMEFloat fx = gpu->pbFract->_pSysData[i];
+    PMEFloat fy = gpu->pbFract->_pSysData[i + gpu->sim.stride];
+    PMEFloat fz = gpu->pbFract->_pSysData[i + gpu->sim.stride2];
+    PMEFloat q  = gpu->pbAtomChargeSP->_pSysData[i];
+    int4 ic     = gpu->pbAtomCoord_M32->_pSysData[i];
+    double x_int = (double)ic.x * gpu->sim.mint32_inv_scale.x;
+    double y_int = (double)ic.y * gpu->sim.mint32_inv_scale.y;
+    double z_int = (double)ic.z * gpu->sim.mint32_inv_scale.z;
+    PMEFloat fx_fixed = (fx < 0) ? fx + gpu->sim.nfft1 : fx;
+    PMEFloat fy_fixed = (fy < 0) ? fy + gpu->sim.nfft2 : fy;
+    PMEFloat fz_fixed = (fz < 0) ? fz + gpu->sim.nfft3 : fz;
+    fprintf(f,
+            "atom %d: frac_raw=(%.6f,%.6f,%.6f) frac_fix=(%.6f,%.6f,%.6f) normed_fix=(%.6f,%.6f,%.6f) charge=%.6f int=(%d,%d,%d) int_unscaled=(%.6f,%.6f,%.6f)\n",
+            i, (double)fx, (double)fy, (double)fz,
+            (double)fx_fixed, (double)fy_fixed, (double)fz_fixed,
+            (double)fx_fixed / gpu->sim.nfft1, (double)fy_fixed / gpu->sim.nfft2,
+            (double)fz_fixed / gpu->sim.nfft3,
+            (double)q, ic.x, ic.y, ic.z, x_int, y_int, z_int);
+  }
+  fclose(f);
+}
+
 //---------------------------------------------------------------------------------------------
 // GetkPMEInterpolationSim: download critical information from the GPU.
 //
@@ -442,6 +502,7 @@ extern "C" void kPMEInterpolationInitKernels(gpuContext gpu)
 //---------------------------------------------------------------------------------------------
 extern "C" void kPMEFillChargeGridBuffer(gpuContext gpu)
 {
+  Mint32DumpPMEInputs(gpu);
   // ti_mode == 3 corresponds to vdw only change, no difference in charges between
   // the two regions.  Skip the extra reciprocal space calculation in that case.
 #if !defined(AMBER_PLATFORM_AMD)
diff --git "a/C:\\amber\\amber_mint\\src\\pmemd\\src/cuda/kRattle.cu" "b/C:\\amber\\amber_mint\\src\\pmemd\\src/cuda/kRattle.cu"
new file mode 100644
index 0000000..e69de29
diff --git "a/C:\\amber\\amber_master\\src\\pmemd\\src/cuda/kRattle.h" "b/C:\\amber\\amber_mint\\src\\pmemd\\src/cuda/kRattle.h"
index 8dbb381..13131af 100644
--- "a/C:\\amber\\amber_master\\src\\pmemd\\src/cuda/kRattle.h"
+++ "b/C:\\amber\\amber_mint\\src\\pmemd\\src/cuda/kRattle.h"
@@ -44,12 +44,24 @@
             double invMassH = cSim.pShakeInvMassH[pos];
 #endif
 
+            // MINT32 FIX: Use pAtomCoord_M32 for distance calculation to handle PBC correctly via integer wrapping
+#ifdef RATTLE_NEIGHBORLIST
+            int4 c_i = cSim.pAtomCoord_M32[shakeID.x];
+            int4 c_j = cSim.pAtomCoord_M32[shakeID.y];
+            double xpxx = (double)(c_i.x - c_j.x) * cSim.mint32_inv_scale.x;
+            double ypxx = (double)(c_i.y - c_j.y) * cSim.mint32_inv_scale.y;
+            double zpxx = (double)(c_i.z - c_j.z) * cSim.mint32_inv_scale.z;
+#else
             double xpi = PATOMX(shakeID.x);
             double ypi = PATOMY(shakeID.x);
             double zpi = PATOMZ(shakeID.x);
             double xpj = PATOMX(shakeID.y);
             double ypj = PATOMY(shakeID.y);
             double zpj = PATOMZ(shakeID.y);
+            double xpxx = xpi - xpj;
+            double ypxx = ypi - ypj;
+            double zpxx = zpi - zpj;
+#endif
             double vxpi = VELX(shakeID.x);
             double vypi = VELY(shakeID.x);
             double vzpi = VELZ(shakeID.x);
@@ -63,9 +75,11 @@
             double xpk, ypk, zpk, vxpk, vypk, vzpk;
 
             if (shakeID.z != -1) {
+#ifndef RATTLE_NEIGHBORLIST
                 xpk = PATOMX(shakeID.z);
                 ypk = PATOMY(shakeID.z);
                 zpk = PATOMZ(shakeID.z);
+#endif
                 vxpk = VELX(shakeID.z);
                 vypk = VELY(shakeID.z);
                 vzpk = VELZ(shakeID.z);
@@ -73,9 +87,11 @@
 
             double xpl, ypl, zpl, vxpl, vypl, vzpl;
             if (shakeID.w != -1) {
+#ifndef RATTLE_NEIGHBORLIST
                 xpl = PATOMX(shakeID.w);
                 ypl = PATOMY(shakeID.w);
                 zpl = PATOMZ(shakeID.w);
+#endif
                 vxpl = VELX(shakeID.w);
                 vypl = VELY(shakeID.w);
                 vzpl = VELZ(shakeID.w);
@@ -87,9 +103,12 @@
                 done = true;
 
                 //calculate rvdot
-                double xpxx = xpi - xpj;
-                double ypxx = ypi - ypj;
-                double zpxx = zpi - zpj;
+                // For MINT32, xpxx is already calculated correctly above
+#ifndef RATTLE_NEIGHBORLIST
+                xpxx = xpi - xpj;
+                ypxx = ypi - ypj;
+                zpxx = zpi - zpj;
+#endif
                 double vxpxx = vxpi - vxpj;
                 double vypxx = vypi - vypj;
                 double vzpxx = vzpi - vzpj;
@@ -97,6 +116,11 @@
 
                 double acor = -rvdot / (toler * (invMassI + INVMASSH));
 
+                // if (pos == 0 && i == 0) {
+                //     printf("RATTLE: pos=%d i=%d rvdot=%e acor=%e tol2=%e\n", pos, i, rvdot, acor, tol2);
+                // }
+
+
                 if (abs(acor) >= tol2) {
                     done = false;
                     double h = xpxx * acor;
@@ -112,9 +136,19 @@
 
                 // Second bond if present
                 if (shakeID.z != -1) {
+#ifdef RATTLE_NEIGHBORLIST
+                    int4 c_k = cSim.pAtomCoord_M32[shakeID.z];
+                    xpxx = (double)(c_i.x - c_k.x) * cSim.mint32_inv_scale.x;
+                    ypxx = (double)(c_i.y - c_k.y) * cSim.mint32_inv_scale.y;
+                    zpxx = (double)(c_i.z - c_k.z) * cSim.mint32_inv_scale.z;
+#else
+                    xpk = PATOMX(shakeID.z);
+                    ypk = PATOMY(shakeID.z);
+                    zpk = PATOMZ(shakeID.z);
                     xpxx = xpi - xpk;
                     ypxx = ypi - ypk;
                     zpxx = zpi - zpk;
+#endif
                     vxpxx = vxpi - vxpk;
                     vypxx = vypi - vypk;
                     vzpxx = vzpi - vzpk;
@@ -139,9 +173,19 @@
 
                 // Third bond if present
                 if (shakeID.w != -1) {
+#ifdef RATTLE_NEIGHBORLIST
+                    int4 c_l = cSim.pAtomCoord_M32[shakeID.w];
+                    xpxx = (double)(c_i.x - c_l.x) * cSim.mint32_inv_scale.x;
+                    ypxx = (double)(c_i.y - c_l.y) * cSim.mint32_inv_scale.y;
+                    zpxx = (double)(c_i.z - c_l.z) * cSim.mint32_inv_scale.z;
+#else
+                    xpl = PATOMX(shakeID.w);
+                    ypl = PATOMY(shakeID.w);
+                    zpl = PATOMZ(shakeID.w);
                     xpxx = xpi - xpl;
                     ypxx = ypi - ypl;
                     zpxx = zpi - zpl;
+#endif
                     vxpxx = vxpi - vxpl;
                     vypxx = vypi - vypl;
                     vzpxx = vzpi - vzpl;
@@ -190,6 +234,39 @@
                     VELY(shakeID.w) = vypl;
                     VELZ(shakeID.w) = vzpl;
                 }
+
+#ifdef RATTLE_NEIGHBORLIST
+                // MINT32 FIX: Also update unsorted velocity arrays to keep them synchronized with sorted arrays
+                // This is critical for NL remapping - both arrays must stay in sync
+                unsigned int origI = cSim.pImageAtom[shakeID.x];
+                unsigned int origJ = cSim.pImageAtom[shakeID.y];
+                if (origI < cSim.atoms) {
+                    cSim.pVelX[origI] = vxpi;
+                    cSim.pVelY[origI] = vypi;
+                    cSim.pVelZ[origI] = vzpi;
+                }
+                if (origJ < cSim.atoms) {
+                    cSim.pVelX[origJ] = vxpj;
+                    cSim.pVelY[origJ] = vypj;
+                    cSim.pVelZ[origJ] = vzpj;
+                }
+                if (shakeID.z != -1) {
+                    unsigned int origK = cSim.pImageAtom[shakeID.z];
+                    if (origK < cSim.atoms) {
+                        cSim.pVelX[origK] = vxpk;
+                        cSim.pVelY[origK] = vypk;
+                        cSim.pVelZ[origK] = vzpk;
+                    }
+                }
+                if (shakeID.w != -1) {
+                    unsigned int origL = cSim.pImageAtom[shakeID.w];
+                    if (origL < cSim.atoms) {
+                        cSim.pVelX[origL] = vxpl;
+                        cSim.pVelY[origL] = vypl;
+                        cSim.pVelZ[origL] = vzpl;
+                    }
+                }
+#endif
             }
         }
     }
@@ -223,25 +300,30 @@
             double wohwo = woh * wo;
             double whhwh = whh * wh;
 
-
+#ifdef RATTLE_NEIGHBORLIST
+            // MINT32 FIX: Use pAtomCoord_M32 for distance calculation to handle PBC correctly
+            int4 c1 = cSim.pAtomCoord_M32[shakeID.x];
+            int4 c2 = cSim.pAtomCoord_M32[shakeID.y];
+            int4 c3 = cSim.pAtomCoord_M32[shakeID.z];
+            double xab = (double)(c2.x - c1.x) * cSim.mint32_inv_scale.x;
+            double yab = (double)(c2.y - c1.y) * cSim.mint32_inv_scale.y;
+            double zab = (double)(c2.z - c1.z) * cSim.mint32_inv_scale.z;
+            double xbc = (double)(c3.x - c2.x) * cSim.mint32_inv_scale.x;
+            double ybc = (double)(c3.y - c2.y) * cSim.mint32_inv_scale.y;
+            double zbc = (double)(c3.z - c2.z) * cSim.mint32_inv_scale.z;
+            double xca = (double)(c1.x - c3.x) * cSim.mint32_inv_scale.x;
+            double yca = (double)(c1.y - c3.y) * cSim.mint32_inv_scale.y;
+            double zca = (double)(c1.z - c3.z) * cSim.mint32_inv_scale.z;
+#else
             double xp1  = PATOMX(shakeID.x);
             double yp1  = PATOMY(shakeID.x);
             double zp1  = PATOMZ(shakeID.x);
-            double vxp1 = VELX(shakeID.x);
-            double vyp1 = VELY(shakeID.x);
-            double vzp1 = VELZ(shakeID.x);
             double xp2  = PATOMX(shakeID.y);
             double yp2  = PATOMY(shakeID.y);
             double zp2  = PATOMZ(shakeID.y);
-            double vxp2 = VELX(shakeID.y);
-            double vyp2 = VELY(shakeID.y);
-            double vzp2 = VELZ(shakeID.y);
             double xp3  = PATOMX(shakeID.z);
             double yp3  = PATOMY(shakeID.z);
             double zp3  = PATOMZ(shakeID.z);
-            double vxp3 = VELX(shakeID.z);
-            double vyp3 = VELY(shakeID.z);
-            double vzp3 = VELZ(shakeID.z);
 
             // Step1 AB, VAB
             double xab = xp2 - xp1;
@@ -253,6 +335,17 @@
             double xca = xp1 - xp3;
             double yca = yp1 - yp3;
             double zca = zp1 - zp3;
+#endif
+
+            double vxp1 = VELX(shakeID.x);
+            double vyp1 = VELY(shakeID.x);
+            double vzp1 = VELZ(shakeID.x);
+            double vxp2 = VELX(shakeID.y);
+            double vyp2 = VELY(shakeID.y);
+            double vzp2 = VELZ(shakeID.y);
+            double vxp3 = VELX(shakeID.z);
+            double vyp3 = VELY(shakeID.z);
+            double vzp3 = VELZ(shakeID.z);
 
             double xvab = vxp2 - vxp1;
             double yvab = vyp2 - vyp1;
@@ -303,15 +396,47 @@
                 - wohwo * (cosb * cosb + cosc * cosc);
 
             // Step5 v
-            VELX(shakeID.x) = vxp1 + (xeab * tabd - xeca * tcad) * wh / deno;
-            VELY(shakeID.x) = vyp1 + (yeab * tabd - yeca * tcad) * wh / deno;
-            VELZ(shakeID.x) = vzp1 + (zeab * tabd - zeca * tcad) * wh / deno;
-            VELX(shakeID.y) = vxp2 + (xebc * tbcd - xeab * tabd * wo) / deno;
-            VELY(shakeID.y) = vyp2 + (yebc * tbcd - yeab * tabd * wo) / deno;
-            VELZ(shakeID.y) = vzp2 + (zebc * tbcd - zeab * tabd * wo) / deno;
-            VELX(shakeID.z) = vxp3 + (xeca * tcad * wo - xebc * tbcd) / deno;
-            VELY(shakeID.z) = vyp3 + (yeca * tcad * wo - yebc * tbcd) / deno;
-            VELZ(shakeID.z) = vzp3 + (zeca * tcad * wo - zebc * tbcd) / deno;
+            double vx1 = vxp1 + (xeab * tabd - xeca * tcad) * wh / deno;
+            double vy1 = vyp1 + (yeab * tabd - yeca * tcad) * wh / deno;
+            double vz1 = vzp1 + (zeab * tabd - zeca * tcad) * wh / deno;
+            double vx2 = vxp2 + (xebc * tbcd - xeab * tabd * wo) / deno;
+            double vy2 = vyp2 + (yebc * tbcd - yeab * tabd * wo) / deno;
+            double vz2 = vzp2 + (zebc * tbcd - zeab * tabd * wo) / deno;
+            double vx3 = vxp3 + (xeca * tcad * wo - xebc * tbcd) / deno;
+            double vy3 = vyp3 + (yeca * tcad * wo - yebc * tbcd) / deno;
+            double vz3 = vzp3 + (zeca * tcad * wo - zebc * tbcd) / deno;
+
+            VELX(shakeID.x) = vx1;
+            VELY(shakeID.x) = vy1;
+            VELZ(shakeID.x) = vz1;
+            VELX(shakeID.y) = vx2;
+            VELY(shakeID.y) = vy2;
+            VELZ(shakeID.y) = vz2;
+            VELX(shakeID.z) = vx3;
+            VELY(shakeID.z) = vy3;
+            VELZ(shakeID.z) = vz3;
+
+#ifdef RATTLE_NEIGHBORLIST
+            // MINT32 FIX: Also update unsorted velocity arrays to keep them synchronized with sorted arrays
+            unsigned int orig1 = cSim.pImageAtom[shakeID.x];
+            unsigned int orig2 = cSim.pImageAtom[shakeID.y];
+            unsigned int orig3 = cSim.pImageAtom[shakeID.z];
+            if (orig1 < cSim.atoms) {
+                cSim.pVelX[orig1] = vx1;
+                cSim.pVelY[orig1] = vy1;
+                cSim.pVelZ[orig1] = vz1;
+            }
+            if (orig2 < cSim.atoms) {
+                cSim.pVelX[orig2] = vx2;
+                cSim.pVelY[orig2] = vy2;
+                cSim.pVelZ[orig2] = vz2;
+            }
+            if (orig3 < cSim.atoms) {
+                cSim.pVelX[orig3] = vx3;
+                cSim.pVelY[orig3] = vy3;
+                cSim.pVelZ[orig3] = vz3;
+            }
+#endif
         }
     }
     else if (pos < cSim.slowShakeOffset) {
@@ -494,6 +619,40 @@
                 VELX(shakeID2.w) = vxpm;
                 VELY(shakeID2.w) = vypm;
                 VELZ(shakeID2.w) = vzpm;
+
+#ifdef RATTLE_NEIGHBORLIST
+                // MINT32 FIX: Also update unsorted velocity arrays to keep them synchronized with sorted arrays
+                unsigned int origI = cSim.pImageAtom[shakeID1];
+                unsigned int origJ = cSim.pImageAtom[shakeID2.x];
+                unsigned int origK = cSim.pImageAtom[shakeID2.y];
+                unsigned int origL = cSim.pImageAtom[shakeID2.z];
+                unsigned int origM = cSim.pImageAtom[shakeID2.w];
+                if (origI < cSim.atoms) {
+                    cSim.pVelX[origI] = vxpi;
+                    cSim.pVelY[origI] = vypi;
+                    cSim.pVelZ[origI] = vzpi;
+                }
+                if (origJ < cSim.atoms) {
+                    cSim.pVelX[origJ] = vxpj;
+                    cSim.pVelY[origJ] = vypj;
+                    cSim.pVelZ[origJ] = vzpj;
+                }
+                if (origK < cSim.atoms) {
+                    cSim.pVelX[origK] = vxpk;
+                    cSim.pVelY[origK] = vypk;
+                    cSim.pVelZ[origK] = vzpk;
+                }
+                if (origL < cSim.atoms) {
+                    cSim.pVelX[origL] = vxpl;
+                    cSim.pVelY[origL] = vypl;
+                    cSim.pVelZ[origL] = vzpl;
+                }
+                if (origM < cSim.atoms) {
+                    cSim.pVelX[origM] = vxpm;
+                    cSim.pVelY[origM] = vypm;
+                    cSim.pVelZ[origM] = vzpm;
+                }
+#endif
             }
         }
     }
diff --git "a/C:\\amber\\amber_master\\src\\pmemd\\src/cuda/kReImageCoord.h" "b/C:\\amber\\amber_mint\\src\\pmemd\\src/cuda/kReImageCoord.h"
index b5b5efb..c863433 100644
--- "a/C:\\amber\\amber_master\\src\\pmemd\\src/cuda/kReImageCoord.h"
+++ "b/C:\\amber\\amber_mint\\src\\pmemd\\src/cuda/kReImageCoord.h"
@@ -50,6 +50,7 @@
 #endif
 
     // Account for minimum image convention
+    // Use [0, 1) convention to match cellID calculation in kNLHashPrepare
     dfx = (dfx - round(dfx) + 0.5);
     dfy = (dfy - round(dfy) + 0.5);
     dfz = (dfz - round(dfz) + 0.5);
@@ -82,20 +83,24 @@
 #endif
 #ifdef PME_ORTHOGONAL
 #  ifdef PME_NTP
+    // Write cell-relative FP32 coords for neighbor list (kBNL reads pAtomXYSP/pAtomZSP)
     PMEFloat2 xy = {(PMEFloat)(sUcell[0] * x), (PMEFloat)(sUcell[4] * y)};
     cSim.pAtomXYSP[pos] = xy;
     cSim.pAtomZSP[pos]  = (PMEFloat)(sUcell[8] * z);
 #  else
+    // Write cell-relative FP32 coords for neighbor list (kBNL reads pAtomXYSP/pAtomZSP)
     PMEFloat2 xy = {(PMEFloat)(cSim.a * x), (PMEFloat)(cSim.b * y)};
     cSim.pAtomXYSP[pos] = xy;
     cSim.pAtomZSP[pos]  = (PMEFloat)(cSim.c * z);
 #  endif
 #else
-    PMEFloat2 xy = {(PMEFloat)x, (PMEFloat)y};
+    // Write cell-relative FP32 coords for neighbor list (kBNL reads pAtomXYSP/pAtomZSP)
+    PMEFloat2 xy = {x, y};
     cSim.pAtomXYSP[pos] = xy;
     cSim.pAtomZSP[pos]  = z;
 #endif
 #ifdef COMP_FRACTIONAL_COORD
+    // dfx/dfy/dfz are already in [0,1) crystallographic convention
     PMEFloat fx = dfx;
     PMEFloat fy = dfy;
     PMEFloat fz = dfz;
diff --git "a/C:\\amber\\amber_master\\src\\pmemd\\src/cuda/kShake.cu" "b/C:\\amber\\amber_mint\\src\\pmemd\\src/cuda/kShake.cu"
index 50eab3e..2843f48 100644
--- "a/C:\\amber\\amber_master\\src\\pmemd\\src/cuda/kShake.cu"
+++ "b/C:\\amber\\amber_mint\\src\\pmemd\\src/cuda/kShake.cu"
@@ -302,6 +302,29 @@ void kShakeInitKernels(gpuContext gpu)
   }
 }
 
+//---------------------------------------------------------------------------------------------
+// kScatterMINT32Coordinates_kernel: Scatter constrained coordinates from sorted pAtomCoord_M32
+//                                   back to unsorted pAtomCoordUnsorted_M32.
+//---------------------------------------------------------------------------------------------
+__global__ void
+__launch_bounds__(UPDATE_THREADS_PER_BLOCK, 1)
+kScatterMINT32Coordinates_kernel()
+{
+    unsigned int pos = blockIdx.x * blockDim.x + threadIdx.x;
+      if (pos < cSim.atoms) {
+        // Read constrained coordinate from sorted array (MINT32)
+        int4 coord = cSim.pAtomCoord_M32[pos];
+        
+        // Find original atom index
+        unsigned int originalIndex = cSim.pImageAtom[pos];
+        
+        // Update unsorted MINT32 array (this is what remap reads from!)
+        if (originalIndex < cSim.atoms) {
+          cSim.pAtomCoordUnsorted_M32[originalIndex] = coord;
+        }
+    }
+}
+
 //---------------------------------------------------------------------------------------------
 // kShake: implement the various SHAKE kernels enumerated above.
 //
@@ -412,6 +435,9 @@ void kShake(gpuContext gpu)
       totalBlocks -= blocks;
     }
   }
+  // MINT32: Scatter constrained coordinates back to unsorted array
+  kScatterMINT32Coordinates_kernel<<<gpu->updateBlocks, gpu->updateThreadsPerBlock>>>();
+  LAUNCHERROR("kScatterMINT32Coordinates");
 }
 
 //---------------------------------------------------------------------------------------------
diff --git "a/C:\\amber\\amber_master\\src\\pmemd\\src/cuda/kShake.h" "b/C:\\amber\\amber_mint\\src\\pmemd\\src/cuda/kShake.h"
index 24006dc..b2ac769 100644
--- "a/C:\\amber\\amber_master\\src\\pmemd\\src/cuda/kShake.h"
+++ "b/C:\\amber\\amber_mint\\src\\pmemd\\src/cuda/kShake.h"
@@ -95,72 +95,99 @@
         double invMassI = shakeParm.x;
         double toler = shakeParm.y;
 
-      // Optionally read 2nd hydrogen
-      double xpk, ypk, zpk, xik, yik, zik;
+        // Read current and old MINT32 coordinates for Pure MINT32 SHAKE
+        int4 coord_i = cSim.pAtomCoord_M32[shakeID.x];
+        int4 coord_j = cSim.pAtomCoord_M32[shakeID.y];
+        int xi_i32 = coord_i.x;
+        int yi_i32 = coord_i.y;
+        int zi_i32 = coord_i.z;
+        int xj_i32 = coord_j.x;
+        int yj_i32 = coord_j.y;
+        int zj_i32 = coord_j.z;
+
+        int4 old_coord_i = cSim.pOldAtomCoord_M32[shakeID.x];
+        int4 old_coord_j = cSim.pOldAtomCoord_M32[shakeID.y];
+        int old_xi_i32 = old_coord_i.x;
+        int old_yi_i32 = old_coord_i.y;
+        int old_zi_i32 = old_coord_i.z;
+        int old_xj_i32 = old_coord_j.x;
+        int old_yj_i32 = old_coord_j.y;
+        int old_zj_i32 = old_coord_j.z;
+
+      // Optionally read 2nd hydrogen (Pure MINT32)
+      double xik, yik, zik;
+      int xk_i32, yk_i32, zk_i32;
       if (shakeID.z != -1) {
-#ifdef NODPTEXTURE
-        xik = cSim.pOldAtomX[shakeID.z];
-        yik = cSim.pOldAtomY[shakeID.z];
-        zik = cSim.pOldAtomZ[shakeID.z];
-#else
-        int2 ixik = tex1Dfetch<int2>(cSim.texOldAtomX, shakeID.z);
-        int2 iyik = tex1Dfetch<int2>(cSim.texOldAtomX, shakeID.z + cSim.stride);
-        int2 izik = tex1Dfetch<int2>(cSim.texOldAtomX, shakeID.z + cSim.stride2);
-        xik = __hiloint2double(ixik.y, ixik.x);
-        yik = __hiloint2double(iyik.y, iyik.x);
-        zik = __hiloint2double(izik.y, izik.x);
-#endif
-        xpk = PATOMX(shakeID.z);
-        ypk = PATOMY(shakeID.z);
-        zpk = PATOMZ(shakeID.z);
+        // Read current and old MINT32 coordinates for 2nd hydrogen
+        int4 coord_k = cSim.pAtomCoord_M32[shakeID.z];
+        xk_i32 = coord_k.x;
+        yk_i32 = coord_k.y;
+        zk_i32 = coord_k.z;
+
+        int4 old_coord_k = cSim.pOldAtomCoord_M32[shakeID.z];
+        int old_xk_i32 = old_coord_k.x;
+        int old_yk_i32 = old_coord_k.y;
+        int old_zk_i32 = old_coord_k.z;
+
+        // Calculate reference vector from old MINT32
+        int xik_i32 = old_xi_i32 - old_xk_i32;
+        int yik_i32 = old_yi_i32 - old_yk_i32;
+        int zik_i32 = old_zi_i32 - old_zk_i32;
+        xik = (double)xik_i32 * cSim.mint32_inv_scale.x;
+        yik = (double)yik_i32 * cSim.mint32_inv_scale.y;
+        zik = (double)zik_i32 * cSim.mint32_inv_scale.z;
       }
 
-      // Optionally read 3rd hydrogen into shared memory
-      double xpl, ypl, zpl, xil, yil, zil;
+      // Optionally read 3rd hydrogen (Pure MINT32)
+      double xil, yil, zil;
+      int xl_i32, yl_i32, zl_i32;
       if (shakeID.w != -1) {
-#ifdef NODPTEXTURE
-        xil = cSim.pOldAtomX[shakeID.w];
-        yil = cSim.pOldAtomY[shakeID.w];
-        zil = cSim.pOldAtomZ[shakeID.w];
-#else
-        int2 ixil = tex1Dfetch<int2>(cSim.texOldAtomX, shakeID.w);
-        int2 iyil = tex1Dfetch<int2>(cSim.texOldAtomX, shakeID.w + cSim.stride);
-        int2 izil = tex1Dfetch<int2>(cSim.texOldAtomX, shakeID.w + cSim.stride2);
-        xil = __hiloint2double(ixil.y, ixil.x);
-        yil = __hiloint2double(iyil.y, iyil.x);
-        zil = __hiloint2double(izil.y, izil.x);
-#endif
-        xpl = PATOMX(shakeID.w);
-        ypl = PATOMY(shakeID.w);
-        zpl = PATOMZ(shakeID.w);
+        // Read current and old MINT32 coordinates for 3rd hydrogen
+        int4 coord_l = cSim.pAtomCoord_M32[shakeID.w];
+        xl_i32 = coord_l.x;
+        yl_i32 = coord_l.y;
+        zl_i32 = coord_l.z;
+
+        int4 old_coord_l = cSim.pOldAtomCoord_M32[shakeID.w];
+        int old_xl_i32 = old_coord_l.x;
+        int old_yl_i32 = old_coord_l.y;
+        int old_zl_i32 = old_coord_l.z;
+
+        // Calculate reference vector from old MINT32
+        int xil_i32 = old_xi_i32 - old_xl_i32;
+        int yil_i32 = old_yi_i32 - old_yl_i32;
+        int zil_i32 = old_zi_i32 - old_zl_i32;
+        xil = (double)xil_i32 * cSim.mint32_inv_scale.x;
+        yil = (double)yil_i32 * cSim.mint32_inv_scale.y;
+        zil = (double)zil_i32 * cSim.mint32_inv_scale.z;
       }
 
-      // Calculate unchanging quantities
+      // Calculate reference vector for first bond using MINT32 (Pure MINT32)
       if (shakeID.y != -1) {
-        xij = xi - xij;
-        yij = yi - yij;
-        zij = zi - zij;
-        if (shakeID.z != -1) {
-          xik = xi - xik;
-          yik = yi - yik;
-          zik = zi - zik;
-        }
-        if (shakeID.w != -1) {
-          xil = xi - xil;
-          yil = yi - yil;
-          zil = zi - zil;
-        }
+        // Reference vector from old MINT32 coordinates
+        int xij_i32 = old_xi_i32 - old_xj_i32;
+        int yij_i32 = old_yi_i32 - old_yj_i32;
+        int zij_i32 = old_zi_i32 - old_zj_i32;
+        // Convert to float for dot product calculations (xij, yij, zij already declared earlier)
+        xij = (double)xij_i32 * cSim.mint32_inv_scale.x;
+        yij = (double)yij_i32 * cSim.mint32_inv_scale.y;
+        zij = (double)zij_i32 * cSim.mint32_inv_scale.z;
       }
+
       bool done = false;
 
       for (int i = 0; i < 3000; i++) {
         done = true;
         if (shakeID.y == -1) break;
 
-        // Calculate nominal distance squared
-        double xpxx = xpi - xpj;
-        double ypxx = ypi - ypj;
-        double zpxx = zpi - zpj;
+        // Pure MINT32: Calculate distance using MINT32 coordinates directly
+        // No requantization needed - MINT32 is the authoritative source
+        int dxp_i32 = xi_i32 - xj_i32;  // INT32 overflow wraps for periodic boundaries
+        int dyp_i32 = yi_i32 - yj_i32;
+        int dzp_i32 = zi_i32 - zj_i32;
+        double xpxx = (double)dxp_i32 * cSim.mint32_inv_scale.x;
+        double ypxx = (double)dyp_i32 * cSim.mint32_inv_scale.y;
+        double zpxx = (double)dzp_i32 * cSim.mint32_inv_scale.z;
         double rpxx2 = xpxx * xpxx + ypxx * ypxx + zpxx * zpxx;
 
 #ifdef TISHAKE2
@@ -176,34 +203,37 @@
         }
 #endif
 
-        // Apply correction
+        // Pure MINT32: Apply correction directly to MINT32 coordinates
         double diff = toler - rpxx2;
         if (abs(diff) >= toler * cSim.tol) {
           done = false;
 
-          // Shake resetting of coordinate is done here
+          // Calculate correction magnitude
           double rrpr = xij * xpxx + yij * ypxx + zij * zpxx;
           if (rrpr >= toler * (double)1.0e-06) {
             double acor = diff / (rrpr * (double)2.0 * (invMassI + INVMASSH));
-            double h = xij * acor;
-            xpi += h * invMassI;
-            xpj -= h * INVMASSH;
-            h = yij * acor;
-            ypi += h * invMassI;
-            ypj -= h * INVMASSH;
-            h = zij * acor;
-            zpi += h * invMassI;
-            zpj -= h * INVMASSH;
+
+            // Calculate deltas in float, then apply as MINT32 additions
+            xi_i32 += (int)rint(xij * acor * invMassI * cSim.mint32_scale.x);
+            yi_i32 += (int)rint(yij * acor * invMassI * cSim.mint32_scale.y);
+            zi_i32 += (int)rint(zij * acor * invMassI * cSim.mint32_scale.z);
+            xj_i32 -= (int)rint(xij * acor * INVMASSH * cSim.mint32_scale.x);
+            yj_i32 -= (int)rint(yij * acor * INVMASSH * cSim.mint32_scale.y);
+            zj_i32 -= (int)rint(zij * acor * INVMASSH * cSim.mint32_scale.z);
           }
         }
         
 
         // Second bond if present
         if (shakeID.z != -1) {
-          xpxx  = xpi - xpk;
-          ypxx  = ypi - ypk;
-          zpxx  = zpi - zpk;
-          rpxx2 = xpxx * xpxx + ypxx * ypxx + zpxx * zpxx;
+          // 2nd hydrogen distance using MINT32 INT32 arithmetic
+        int dxpk_i32 = xi_i32 - xk_i32;
+        int dypk_i32 = yi_i32 - yk_i32;
+        int dzpk_i32 = zi_i32 - zk_i32;
+        xpxx  = (double)dxpk_i32 * cSim.mint32_inv_scale.x;
+        ypxx  = (double)dypk_i32 * cSim.mint32_inv_scale.y;
+        zpxx  = (double)dzpk_i32 * cSim.mint32_inv_scale.z;
+        rpxx2 = xpxx * xpxx + ypxx * ypxx + zpxx * zpxx;
 
 #ifdef TISHAKE2
           if(refShakeID.z < -1)
@@ -218,33 +248,36 @@
           }
 #endif
 
-          // Apply correction
+          // Pure MINT32: Apply correction to second bond
           diff = toler - rpxx2;
           if (abs(diff) >= toler * cSim.tol) {
             done = false;
 
-            // Shake resetting of coordinate is done here
+            // Calculate correction magnitude
             double rrpr = xik * xpxx + yik * ypxx + zik * zpxx;
             if (rrpr >= toler * (double)1.0e-06) {
               double acor = diff / (rrpr * (double)2.0 * (invMassI + INVMASSH));
-              double h    = xik * acor;
-              xpi += h * invMassI;
-              xpk -= h * INVMASSH;
-              h    = yik * acor;
-              ypi += h * invMassI;
-              ypk -= h * INVMASSH;
-              h    = zik * acor;
-              zpi += h * invMassI;
-              zpk -= h * INVMASSH;
+
+              // Apply as MINT32 additions
+              xi_i32 += (int)rint(xik * acor * invMassI * cSim.mint32_scale.x);
+              yi_i32 += (int)rint(yik * acor * invMassI * cSim.mint32_scale.y);
+              zi_i32 += (int)rint(zik * acor * invMassI * cSim.mint32_scale.z);
+              xk_i32 -= (int)rint(xik * acor * INVMASSH * cSim.mint32_scale.x);
+              yk_i32 -= (int)rint(yik * acor * INVMASSH * cSim.mint32_scale.y);
+              zk_i32 -= (int)rint(zik * acor * INVMASSH * cSim.mint32_scale.z);
             }
           }
         }
 
         // Third bond if present
         if (shakeID.w != -1) {
-          xpxx  = xpi - xpl;
-          ypxx  = ypi - ypl;
-          zpxx  = zpi - zpl;
+          // Use MINT32 coordinates for third bond distance calculation
+          int dxp_i32 = xi_i32 - xl_i32;  // Automatic periodic wrapping
+          int dyp_i32 = yi_i32 - yl_i32;
+          int dzp_i32 = zi_i32 - zl_i32;
+          xpxx  = (double)dxp_i32 * cSim.mint32_inv_scale.x;
+          ypxx  = (double)dyp_i32 * cSim.mint32_inv_scale.y;
+          zpxx  = (double)dzp_i32 * cSim.mint32_inv_scale.z;
           rpxx2 = xpxx * xpxx + ypxx * ypxx + zpxx * zpxx;
 
 #ifdef TISHAKE2
@@ -260,80 +293,64 @@
           }
 #endif
 
-          // Apply correction
+          // Pure MINT32: Apply correction to third bond
           diff = toler - rpxx2;
           if (abs(diff) >= toler * cSim.tol) {
             done = false;
 
-            // Shake resetting of coordinate is done here
+            // Calculate correction magnitude
             double rrpr = xil * xpxx + yil * ypxx + zil * zpxx;
             if (rrpr >= toler * (double)1.0e-06) {
               double acor = diff / (rrpr * (double)2.0 * (invMassI + INVMASSH));
-              double h    = xil * acor;
-              xpi += h * invMassI;
-              xpl -= h * INVMASSH;
-              h    = yil * acor;
-              ypi += h * invMassI;
-              ypl -= h * INVMASSH;
-              h    = zil * acor;
-              zpi += h * invMassI;
-              zpl -= h * INVMASSH;
+
+              // Apply as MINT32 additions
+              xi_i32 += (int)rint(xil * acor * invMassI * cSim.mint32_scale.x);
+              yi_i32 += (int)rint(yil * acor * invMassI * cSim.mint32_scale.y);
+              zi_i32 += (int)rint(zil * acor * invMassI * cSim.mint32_scale.z);
+              xl_i32 -= (int)rint(xil * acor * INVMASSH * cSim.mint32_scale.x);
+              yl_i32 -= (int)rint(yil * acor * INVMASSH * cSim.mint32_scale.y);
+              zl_i32 -= (int)rint(zil * acor * INVMASSH * cSim.mint32_scale.z);
             }
           }
         }
 
+        // Pure MINT32: No requantization needed
+        // MINT32 coordinates are already updated directly in the correction steps
+
         // Check for convergence
         if (done) {
           break;
         }
       }
 
-      // Write out results if converged, but there's no really good
-      // way to indicate failure so we'll let the simulation heading
-      // off to Neptune do that for us.  Wish there were a better way,
-      // but until the CPU needs something from the GPU, those are the
-      // the breaks.  I guess, technically, we could just set a flag to NOP
-      // the simulation from here and then carry that result through upon
-      // the next ntpr, ntwc, or ntwx update, but I leave that up to you
-      // guys to implement that (or not).
+      // Pure MINT32: Write back MINT32 coordinates and derive float from them
       if (done) {
         if (shakeID.y != -1) {
-          PATOMX(shakeID.x) = xpi;
-          PATOMY(shakeID.x) = ypi;
-          PATOMZ(shakeID.x) = zpi;
-#ifndef SHAKE_NEIGHBORLIST
-          PMEFloat2 xyi = { (PMEFloat)xpi, (PMEFloat)ypi };
-          cSim.pAtomXYSP[shakeID.x] = xyi;
-          cSim.pAtomZSP[shakeID.x] = zpi;
-#endif
-          PATOMX(shakeID.y) = xpj;
-          PATOMY(shakeID.y) = ypj;
-          PATOMZ(shakeID.y) = zpj;
-#ifndef SHAKE_NEIGHBORLIST
-          PMEFloat2 xyj = { (PMEFloat)xpj, (PMEFloat)ypj };
-          cSim.pAtomXYSP[shakeID.y] = xyj;
-          cSim.pAtomZSP[shakeID.y] = zpj;
-#endif
+          // Write MINT32 coordinates (authoritative)
+          cSim.pAtomCoord_M32[shakeID.x] = make_int4(xi_i32, yi_i32, zi_i32, 0);
+          cSim.pAtomCoord_M32[shakeID.y] = make_int4(xj_i32, yj_i32, zj_i32, 0);
+
+          // Derive float coordinates from MINT32
+          PATOMX(shakeID.x) = (double)xi_i32 * cSim.mint32_inv_scale.x;
+          PATOMY(shakeID.x) = (double)yi_i32 * cSim.mint32_inv_scale.y;
+          PATOMZ(shakeID.x) = (double)zi_i32 * cSim.mint32_inv_scale.z;
+          PATOMX(shakeID.y) = (double)xj_i32 * cSim.mint32_inv_scale.x;
+          PATOMY(shakeID.y) = (double)yj_i32 * cSim.mint32_inv_scale.y;
+          PATOMZ(shakeID.y) = (double)zj_i32 * cSim.mint32_inv_scale.z;
         }
         if (shakeID.z != -1) {
-          PATOMX(shakeID.z) = xpk;
-          PATOMY(shakeID.z) = ypk;
-          PATOMZ(shakeID.z) = zpk;
-#ifndef SHAKE_NEIGHBORLIST
-          PMEFloat2 xyk = {(PMEFloat)xpk, (PMEFloat)ypk};
-          cSim.pAtomXYSP[shakeID.z] = xyk;
-          cSim.pAtomZSP[shakeID.z]  = zpk;
-#endif
+          // Write MINT32 and derive float for atom k
+          cSim.pAtomCoord_M32[shakeID.z] = make_int4(xk_i32, yk_i32, zk_i32, 0);
+          PATOMX(shakeID.z) = (double)xk_i32 * cSim.mint32_inv_scale.x;
+          PATOMY(shakeID.z) = (double)yk_i32 * cSim.mint32_inv_scale.y;
+          PATOMZ(shakeID.z) = (double)zk_i32 * cSim.mint32_inv_scale.z;
         }
         if (shakeID.w != -1) {
-          PATOMX(shakeID.w)           = xpl;
-          PATOMY(shakeID.w)           = ypl;
-          PATOMZ(shakeID.w)           = zpl;
-#ifndef SHAKE_NEIGHBORLIST
-          PMEFloat2 xyl               = {(PMEFloat)xpl, (PMEFloat)ypl};
-          cSim.pAtomXYSP[shakeID.w]   = xyl;
-          cSim.pAtomZSP[shakeID.w]    = zpl;
-#endif
+          // Write MINT32 and derive float for atom l
+          cSim.pAtomCoord_M32[shakeID.w] = make_int4(xl_i32, yl_i32, zl_i32, 0);
+          PATOMX(shakeID.w) = (double)xl_i32 * cSim.mint32_inv_scale.x;
+          PATOMY(shakeID.w) = (double)yl_i32 * cSim.mint32_inv_scale.y;
+          PATOMZ(shakeID.w) = (double)zl_i32 * cSim.mint32_inv_scale.z;
         }
       }
     }
@@ -345,68 +362,117 @@
       // Read atom data
 #ifdef SHAKE_NEIGHBORLIST
       int4 shakeID = cSim.pImageFastShakeID[pos];
+      int4* pCoord = cSim.pAtomCoord_M32;
+      int4* pOldCoord = cSim.pOldAtomCoord_M32;
 #else
       int4 shakeID = cSim.pFastShakeID[pos];
+      int4* pCoord = cSim.pAtomCoordUnsorted_M32;
+      int4* pOldCoord = cSim.pOldAtomCoordUnsorted_M32;
 #endif
-#ifdef NODPTEXTURE
-      double x1 = cSim.pOldAtomX[shakeID.x];
-      double y1 = cSim.pOldAtomY[shakeID.x];
-      double z1 = cSim.pOldAtomZ[shakeID.x];
-      double x2 = cSim.pOldAtomX[shakeID.y];
-      double y2 = cSim.pOldAtomY[shakeID.y];
-      double z2 = cSim.pOldAtomZ[shakeID.y];
-      double x3 = cSim.pOldAtomX[shakeID.z];
-      double y3 = cSim.pOldAtomY[shakeID.z];
-      double z3 = cSim.pOldAtomZ[shakeID.z];
-#else
-      int2 ix1  = tex1Dfetch<int2>(cSim.texOldAtomX, shakeID.x);
-      int2 iy1  = tex1Dfetch<int2>(cSim.texOldAtomX, shakeID.x + cSim.stride);
-      int2 iz1  = tex1Dfetch<int2>(cSim.texOldAtomX, shakeID.x + cSim.stride2);
-      int2 ix2  = tex1Dfetch<int2>(cSim.texOldAtomX, shakeID.y);
-      int2 iy2  = tex1Dfetch<int2>(cSim.texOldAtomX, shakeID.y + cSim.stride);
-      int2 iz2  = tex1Dfetch<int2>(cSim.texOldAtomX, shakeID.y + cSim.stride2);
-      int2 ix3  = tex1Dfetch<int2>(cSim.texOldAtomX, shakeID.z);
-      int2 iy3  = tex1Dfetch<int2>(cSim.texOldAtomX, shakeID.z + cSim.stride);
-      int2 iz3  = tex1Dfetch<int2>(cSim.texOldAtomX, shakeID.z + cSim.stride2);
-      double x1 = __hiloint2double(ix1.y, ix1.x);
-      double y1 = __hiloint2double(iy1.y, iy1.x);
-      double z1 = __hiloint2double(iz1.y, iz1.x);
-      double x2 = __hiloint2double(ix2.y, ix2.x);
-      double y2 = __hiloint2double(iy2.y, iy2.x);
-      double z2 = __hiloint2double(iz2.y, iz2.x);
-      double x3 = __hiloint2double(ix3.y, ix3.x);
-      double y3 = __hiloint2double(iy3.y, iy3.x);
-      double z3 = __hiloint2double(iz3.y, iz3.x);
-#endif
-      double xp1 = PATOMX(shakeID.x);
-      double yp1 = PATOMY(shakeID.x);
-      double zp1 = PATOMZ(shakeID.x);
-      double xp2 = PATOMX(shakeID.y);
-      double yp2 = PATOMY(shakeID.y);
-      double zp2 = PATOMZ(shakeID.y);
-      double xp3 = PATOMX(shakeID.z);
-      double yp3 = PATOMY(shakeID.z);
-      double zp3 = PATOMZ(shakeID.z);
+
+      // Read MINT32 coordinates
+      int4 coord1 = pCoord[shakeID.x];
+      int4 coord2 = pCoord[shakeID.y];
+      int4 coord3 = pCoord[shakeID.z];
+      
+      int xp1_i32 = coord1.x;
+      int yp1_i32 = coord1.y;
+      int zp1_i32 = coord1.z;
+      
+      int xp2_i32 = coord2.x;
+      int yp2_i32 = coord2.y;
+      int zp2_i32 = coord2.z;
+      
+      int xp3_i32 = coord3.x;
+      int yp3_i32 = coord3.y;
+      int zp3_i32 = coord3.z;
+
+      // Read OLD MINT32 coordinates
+      int4 old_coord1 = pOldCoord[shakeID.x];
+      int4 old_coord2 = pOldCoord[shakeID.y];
+      int4 old_coord3 = pOldCoord[shakeID.z];
+      
+      int old_xp1_i32 = old_coord1.x;
+      int old_yp1_i32 = old_coord1.y;
+      int old_zp1_i32 = old_coord1.z;
+      
+      int old_xp2_i32 = old_coord2.x;
+      int old_yp2_i32 = old_coord2.y;
+      int old_zp2_i32 = old_coord2.z;
+      
+      int old_xp3_i32 = old_coord3.x;
+      int old_yp3_i32 = old_coord3.y;
+      int old_zp3_i32 = old_coord3.z;
+
+      // Calculate reference geometry (old vectors) using MINT32 differences
+      // This handles PBC automatically via integer overflow
+      
+      // xb0 = x2 - x1 (old)
+      int xb0_i32 = old_xp2_i32 - old_xp1_i32;
+      int yb0_i32 = old_yp2_i32 - old_yp1_i32;
+      int zb0_i32 = old_zp2_i32 - old_zp1_i32;
+      
+      // xc0 = x3 - x1 (old)
+      int xc0_i32 = old_xp3_i32 - old_xp1_i32;
+      int yc0_i32 = old_yp3_i32 - old_yp1_i32;
+      int zc0_i32 = old_zp3_i32 - old_zp1_i32;
+
+      // Convert to double for SETTLE
+      double xb0 = (double)xb0_i32 * cSim.mint32_inv_scale.x;
+      double yb0 = (double)yb0_i32 * cSim.mint32_inv_scale.y;
+      double zb0 = (double)zb0_i32 * cSim.mint32_inv_scale.z;
+      
+      double xc0 = (double)xc0_i32 * cSim.mint32_inv_scale.x;
+      double yc0 = (double)yc0_i32 * cSim.mint32_inv_scale.y;
+      double zc0 = (double)zc0_i32 * cSim.mint32_inv_scale.z;
+
+      // Calculate current vectors relative to COM using MINT32 differences
+      
+      // xa1 = cSim.wh_div_wohh * ( (xp1-xp2) + (xp1-xp3) )
+      // We calculate differences in int32 first
+      
+      int dx12_i32 = xp1_i32 - xp2_i32;
+      int dy12_i32 = yp1_i32 - yp2_i32;
+      int dz12_i32 = zp1_i32 - zp2_i32;
+      
+      int dx13_i32 = xp1_i32 - xp3_i32;
+      int dy13_i32 = yp1_i32 - yp3_i32;
+      int dz13_i32 = zp1_i32 - zp3_i32;
+      
+      // Optimization: Sum in MINT32 domain first
+      double xa1 = cSim.wh_div_wohh * (double)(dx12_i32 + dx13_i32) * cSim.mint32_inv_scale.x;
+      double ya1 = cSim.wh_div_wohh * (double)(dy12_i32 + dy13_i32) * cSim.mint32_inv_scale.y;
+      double za1 = cSim.wh_div_wohh * (double)(dz12_i32 + dz13_i32) * cSim.mint32_inv_scale.z;
+
+      // xb1 = cSim.wo_div_wohh * (xp2-xp1) + cSim.wh_div_wohh * (xp2-xp3)
+      int dx21_i32 = -dx12_i32; // xp2 - xp1
+      int dy21_i32 = -dy12_i32;
+      int dz21_i32 = -dz12_i32;
+      
+      int dx23_i32 = xp2_i32 - xp3_i32;
+      int dy23_i32 = yp2_i32 - yp3_i32;
+      int dz23_i32 = zp2_i32 - zp3_i32;
+      
+      // Optimization: Factor out inv_scale
+      double xb1 = (cSim.wo_div_wohh * (double)dx21_i32 + cSim.wh_div_wohh * (double)dx23_i32) * cSim.mint32_inv_scale.x;
+      double yb1 = (cSim.wo_div_wohh * (double)dy21_i32 + cSim.wh_div_wohh * (double)dy23_i32) * cSim.mint32_inv_scale.y;
+      double zb1 = (cSim.wo_div_wohh * (double)dz21_i32 + cSim.wh_div_wohh * (double)dz23_i32) * cSim.mint32_inv_scale.z;
+
+      // xc1 = cSim.wo_div_wohh * (xp3-xp1) + cSim.wh_div_wohh * (xp3-xp2)
+      int dx31_i32 = -dx13_i32; // xp3 - xp1
+      int dy31_i32 = -dy13_i32;
+      int dz31_i32 = -dz13_i32;
+      
+      int dx32_i32 = -dx23_i32; // xp3 - xp2
+      int dy32_i32 = -dy23_i32;
+      int dz32_i32 = -dz23_i32;
+      
+      // Optimization: Factor out inv_scale
+      double xc1 = (cSim.wo_div_wohh * (double)dx31_i32 + cSim.wh_div_wohh * (double)dx32_i32) * cSim.mint32_inv_scale.x;
+      double yc1 = (cSim.wo_div_wohh * (double)dy31_i32 + cSim.wh_div_wohh * (double)dy32_i32) * cSim.mint32_inv_scale.y;
+      double zc1 = (cSim.wo_div_wohh * (double)dz31_i32 + cSim.wh_div_wohh * (double)dz32_i32) * cSim.mint32_inv_scale.z;
 
       // Step1  A1_prime:
-      double xb0  = x2 - x1;
-      double yb0  = y2 - y1;
-      double zb0  = z2 - z1;
-      double xc0  = x3 - x1;
-      double yc0  = y3 - y1;
-      double zc0  = z3 - z1;
-      double xcom = xp1*cSim.wo_div_wohh + (xp2 + xp3)*cSim.wh_div_wohh;
-      double ycom = yp1*cSim.wo_div_wohh + (yp2 + yp3)*cSim.wh_div_wohh;
-      double zcom = zp1*cSim.wo_div_wohh + (zp2 + zp3)*cSim.wh_div_wohh;
-      double xa1 = xp1 - xcom;
-      double ya1 = yp1 - ycom;
-      double za1 = zp1 - zcom;
-      double xb1 = xp2 - xcom;
-      double yb1 = yp2 - ycom;
-      double zb1 = zp2 - zcom;
-      double xc1 = xp3 - xcom;
-      double yc1 = yp3 - ycom;
-      double zc1 = zp3 - zcom;
       double xakszd = yb0*zc0 - zb0*yc0;
       double yakszd = zb0*xc0 - xb0*zc0;
       double zakszd = xb0*yc0 - yb0*xc0;
@@ -472,16 +538,52 @@
       double yc3d   = -xb2d*sinthe + yc2d*costhe;
       double zc3d   =  zc1d;
 
-      // Step5  A3:
-      PATOMX(shakeID.x) = xcom + trns11*xa3d + trns12*ya3d + trns13*za3d;
-      PATOMY(shakeID.x) = ycom + trns21*xa3d + trns22*ya3d + trns23*za3d;
-      PATOMZ(shakeID.x) = zcom + trns31*xa3d + trns32*ya3d + trns33*za3d;
-      PATOMX(shakeID.y) = xcom + trns11*xb3d + trns12*yb3d + trns13*zb3d;
-      PATOMY(shakeID.y) = ycom + trns21*xb3d + trns22*yb3d + trns23*zb3d;
-      PATOMZ(shakeID.y) = zcom + trns31*xb3d + trns32*yb3d + trns33*zb3d;
-      PATOMX(shakeID.z) = xcom + trns11*xc3d + trns12*yc3d + trns13*zc3d;
-      PATOMY(shakeID.z) = ycom + trns21*xc3d + trns22*yc3d + trns23*zc3d;
-      PATOMZ(shakeID.z) = zcom + trns31*xc3d + trns32*yc3d + trns33*zc3d;
+      // Step5  A3: Update positions
+      // Calculate displacement for atom 1
+      double dx1 = (trns11*xa3d + trns12*ya3d + trns13*za3d) - xa1;
+      double dy1 = (trns21*xa3d + trns22*ya3d + trns23*za3d) - ya1;
+      double dz1 = (trns31*xa3d + trns32*ya3d + trns33*za3d) - za1;
+      
+      // Update MINT32
+      xp1_i32 += (int)rint(dx1 * cSim.mint32_scale.x);
+      yp1_i32 += (int)rint(dy1 * cSim.mint32_scale.y);
+      zp1_i32 += (int)rint(dz1 * cSim.mint32_scale.z);
+      
+      // Atom 2
+      double dx2 = (trns11*xb3d + trns12*yb3d + trns13*zb3d) - xb1;
+      double dy2 = (trns21*xb3d + trns22*yb3d + trns23*zb3d) - yb1;
+      double dz2 = (trns31*xb3d + trns32*yb3d + trns33*zb3d) - zb1;
+      
+      xp2_i32 += (int)rint(dx2 * cSim.mint32_scale.x);
+      yp2_i32 += (int)rint(dy2 * cSim.mint32_scale.y);
+      zp2_i32 += (int)rint(dz2 * cSim.mint32_scale.z);
+      
+      // Atom 3
+      double dx3 = (trns11*xc3d + trns12*yc3d + trns13*zc3d) - xc1;
+      double dy3 = (trns21*xc3d + trns22*yc3d + trns23*zc3d) - yc1;
+      double dz3 = (trns31*xc3d + trns32*yc3d + trns33*zc3d) - zc1;
+      
+      xp3_i32 += (int)rint(dx3 * cSim.mint32_scale.x);
+      yp3_i32 += (int)rint(dy3 * cSim.mint32_scale.y);
+      zp3_i32 += (int)rint(dz3 * cSim.mint32_scale.z);
+      
+      // Write back MINT32
+      pCoord[shakeID.x] = make_int4(xp1_i32, yp1_i32, zp1_i32, 0);
+      pCoord[shakeID.y] = make_int4(xp2_i32, yp2_i32, zp2_i32, 0);
+      pCoord[shakeID.z] = make_int4(xp3_i32, yp3_i32, zp3_i32, 0);
+      
+      // Update float mirrors
+      PATOMX(shakeID.x) = (double)xp1_i32 * cSim.mint32_inv_scale.x;
+      PATOMY(shakeID.x) = (double)yp1_i32 * cSim.mint32_inv_scale.y;
+      PATOMZ(shakeID.x) = (double)zp1_i32 * cSim.mint32_inv_scale.z;
+      
+      PATOMX(shakeID.y) = (double)xp2_i32 * cSim.mint32_inv_scale.x;
+      PATOMY(shakeID.y) = (double)yp2_i32 * cSim.mint32_inv_scale.y;
+      PATOMZ(shakeID.y) = (double)zp2_i32 * cSim.mint32_inv_scale.z;
+      
+      PATOMX(shakeID.z) = (double)xp3_i32 * cSim.mint32_inv_scale.x;
+      PATOMY(shakeID.z) = (double)yp3_i32 * cSim.mint32_inv_scale.y;
+      PATOMZ(shakeID.z) = (double)zp3_i32 * cSim.mint32_inv_scale.z;
     }
   }
   else if (pos < cSim.slowShakeOffset) {
@@ -492,12 +594,16 @@
 #ifdef SHAKE_NEIGHBORLIST
       int  shakeID1 = cSim.pImageSlowShakeID1[pos];
       int4 shakeID2 = cSim.pImageSlowShakeID2[pos];
+      int4* pCoord = cSim.pAtomCoord_M32;
+      int4* pOldCoord = cSim.pOldAtomCoord_M32;
 #ifdef TISHAKE2
       int4 refShakeID  = cSim.pImageSlowShakeID2[pos];
 #endif
 #else
       int  shakeID1 = cSim.pSlowShakeID1[pos];
       int4 shakeID2 = cSim.pSlowShakeID2[pos];
+      int4* pCoord = cSim.pAtomCoordUnsorted_M32;
+      int4* pOldCoord = cSim.pOldAtomCoordUnsorted_M32;
 #ifdef TISHAKE2
       int4 refShakeID = cSim.pSlowShakeID2[pos];
 #endif
@@ -525,95 +631,118 @@
 #ifdef SHAKE_HMR
       double invMassH = cSim.pSlowShakeInvMassH[pos];
 #endif
-      // Read SHAKE network components
-#ifdef NODPTEXTURE
-      double xi  = cSim.pOldAtomX[shakeID1];
-      double yi  = cSim.pOldAtomY[shakeID1];
-      double zi  = cSim.pOldAtomZ[shakeID1];
-      double xij = cSim.pOldAtomX[shakeID2.x];
-      double yij = cSim.pOldAtomY[shakeID2.x];
-      double zij = cSim.pOldAtomZ[shakeID2.x];
-      double xik = cSim.pOldAtomX[shakeID2.y];
-      double yik = cSim.pOldAtomY[shakeID2.y];
-      double zik = cSim.pOldAtomZ[shakeID2.y];
-      double xil = cSim.pOldAtomX[shakeID2.z];
-      double yil = cSim.pOldAtomY[shakeID2.z];
-      double zil = cSim.pOldAtomZ[shakeID2.z];
-      double xim = cSim.pOldAtomX[shakeID2.w];
-      double yim = cSim.pOldAtomY[shakeID2.w];
-      double zim = cSim.pOldAtomZ[shakeID2.w];
-#else
-      int2 ixi   = tex1Dfetch<int2>(cSim.texOldAtomX, shakeID1);
-      int2 iyi   = tex1Dfetch<int2>(cSim.texOldAtomX, shakeID1 + cSim.stride);
-      int2 izi   = tex1Dfetch<int2>(cSim.texOldAtomX, shakeID1 + cSim.stride2);
-      int2 ixij  = tex1Dfetch<int2>(cSim.texOldAtomX, shakeID2.x);
-      int2 iyij  = tex1Dfetch<int2>(cSim.texOldAtomX, shakeID2.x + cSim.stride);
-      int2 izij  = tex1Dfetch<int2>(cSim.texOldAtomX, shakeID2.x + cSim.stride2);
-      int2 ixik  = tex1Dfetch<int2>(cSim.texOldAtomX, shakeID2.y);
-      int2 iyik  = tex1Dfetch<int2>(cSim.texOldAtomX, shakeID2.y + cSim.stride);
-      int2 izik  = tex1Dfetch<int2>(cSim.texOldAtomX, shakeID2.y + cSim.stride2);
-      int2 ixil  = tex1Dfetch<int2>(cSim.texOldAtomX, shakeID2.z);
-      int2 iyil  = tex1Dfetch<int2>(cSim.texOldAtomX, shakeID2.z + cSim.stride);
-      int2 izil  = tex1Dfetch<int2>(cSim.texOldAtomX, shakeID2.z + cSim.stride2);
-      int2 ixim  = tex1Dfetch<int2>(cSim.texOldAtomX, shakeID2.w);
-      int2 iyim  = tex1Dfetch<int2>(cSim.texOldAtomX, shakeID2.w + cSim.stride);
-      int2 izim  = tex1Dfetch<int2>(cSim.texOldAtomX, shakeID2.w + cSim.stride2);
-      double xi  = __hiloint2double(ixi.y, ixi.x);
-      double yi  = __hiloint2double(iyi.y, iyi.x);
-      double zi  = __hiloint2double(izi.y, izi.x);
-      double xij = __hiloint2double(ixij.y, ixij.x);
-      double yij = __hiloint2double(iyij.y, iyij.x);
-      double zij = __hiloint2double(izij.y, izij.x);
-      double xik = __hiloint2double(ixik.y, ixik.x);
-      double yik = __hiloint2double(iyik.y, iyik.x);
-      double zik = __hiloint2double(izik.y, izik.x);
-      double xil = __hiloint2double(ixil.y, ixil.x);
-      double yil = __hiloint2double(iyil.y, iyil.x);
-      double zil = __hiloint2double(izil.y, izil.x);
-      double xim = __hiloint2double(ixim.y, ixim.x);
-      double yim = __hiloint2double(iyim.y, iyim.x);
-      double zim = __hiloint2double(izim.y, izim.x);
-#endif
-      double xpi = PATOMX(shakeID1);
-      double ypi = PATOMY(shakeID1);
-      double zpi = PATOMZ(shakeID1);
-      double xpj = PATOMX(shakeID2.x);
-      double ypj = PATOMY(shakeID2.x);
-      double zpj = PATOMZ(shakeID2.x);
-      double xpk = PATOMX(shakeID2.y);
-      double ypk = PATOMY(shakeID2.y);
-      double zpk = PATOMZ(shakeID2.y);
-      double xpl = PATOMX(shakeID2.z);
-      double ypl = PATOMY(shakeID2.z);
-      double zpl = PATOMZ(shakeID2.z);
-      double xpm = PATOMX(shakeID2.w);
-      double ypm = PATOMY(shakeID2.w);
-      double zpm = PATOMZ(shakeID2.w);
-      double invMassI = shakeParm.x;
-      double toler    = shakeParm.y;
+      // Read MINT32 coordinates
+      int4 coord1 = pCoord[shakeID1];
+      int4 coord2 = pCoord[shakeID2.x];
+      int4 coord3 = pCoord[shakeID2.y];
+      int4 coord4 = pCoord[shakeID2.z];
+      int4 coord5 = pCoord[shakeID2.w];
+
+      int xi_i32 = coord1.x;
+      int yi_i32 = coord1.y;
+      int zi_i32 = coord1.z;
+      
+      int xj_i32 = coord2.x;
+      int yj_i32 = coord2.y;
+      int zj_i32 = coord2.z;
+      
+      int xk_i32 = coord3.x;
+      int yk_i32 = coord3.y;
+      int zk_i32 = coord3.z;
+      
+      int xl_i32 = coord4.x;
+      int yl_i32 = coord4.y;
+      int zl_i32 = coord4.z;
+      
+      int xm_i32 = coord5.x;
+      int ym_i32 = coord5.y;
+      int zm_i32 = coord5.z;
+
+      // Read OLD MINT32 coordinates
+      int4 old_coord1 = pOldCoord[shakeID1];
+      int4 old_coord2 = pOldCoord[shakeID2.x];
+      int4 old_coord3 = pOldCoord[shakeID2.y];
+      int4 old_coord4 = pOldCoord[shakeID2.z];
+      int4 old_coord5 = pOldCoord[shakeID2.w];
+      
+      int old_xi_i32 = old_coord1.x;
+      int old_yi_i32 = old_coord1.y;
+      int old_zi_i32 = old_coord1.z;
+      
+      int old_xj_i32 = old_coord2.x;
+      int old_yj_i32 = old_coord2.y;
+      int old_zj_i32 = old_coord2.z;
+      
+      int old_xk_i32 = old_coord3.x;
+      int old_yk_i32 = old_coord3.y;
+      int old_zk_i32 = old_coord3.z;
+      
+      int old_xl_i32 = old_coord4.x;
+      int old_yl_i32 = old_coord4.y;
+      int old_zl_i32 = old_coord4.z;
+      
+      int old_xm_i32 = old_coord5.x;
+      int old_ym_i32 = old_coord5.y;
+      int old_zm_i32 = old_coord5.z;
 
-      // Calculate unchanging quantities
-      xij = xi - xij;
-      yij = yi - yij;
-      zij = zi - zij;
-      xik = xi - xik;
-      yik = yi - yik;
-      zik = zi - zik;
-      xil = xi - xil;
-      yil = yi - yil;
-      zil = zi - zil;
-      xim = xi - xim;
-      yim = yi - yim;
-      zim = zi - zim;
+      // Calculate reference geometry (old vectors) using MINT32 differences
+      // This handles PBC automatically via integer overflow
+      
+      // xij = xj - xi (old)
+      int dxij_i32 = old_xj_i32 - old_xi_i32;
+      int dyij_i32 = old_yj_i32 - old_yi_i32;
+      int dzij_i32 = old_zj_i32 - old_zi_i32;
+      
+      // xik = xk - xi (old)
+      int dxik_i32 = old_xk_i32 - old_xi_i32;
+      int dyik_i32 = old_yk_i32 - old_yi_i32;
+      int dzik_i32 = old_zk_i32 - old_zi_i32;
+      
+      // xil = xl - xi (old)
+      int dxil_i32 = old_xl_i32 - old_xi_i32;
+      int dyil_i32 = old_yl_i32 - old_yi_i32;
+      int dzil_i32 = old_zl_i32 - old_zi_i32;
+      
+      // xim = xm - xi (old)
+      int dxim_i32 = old_xm_i32 - old_xi_i32;
+      int dyim_i32 = old_ym_i32 - old_yi_i32;
+      int dzim_i32 = old_zm_i32 - old_zi_i32;
+
+      // Convert to double for SHAKE
+      double xij = (double)dxij_i32 * cSim.mint32_inv_scale.x;
+      double yij = (double)dyij_i32 * cSim.mint32_inv_scale.y;
+      double zij = (double)dzij_i32 * cSim.mint32_inv_scale.z;
+      
+      double xik = (double)dxik_i32 * cSim.mint32_inv_scale.x;
+      double yik = (double)dyik_i32 * cSim.mint32_inv_scale.y;
+      double zik = (double)dzik_i32 * cSim.mint32_inv_scale.z;
+      
+      double xil = (double)dxil_i32 * cSim.mint32_inv_scale.x;
+      double yil = (double)dyil_i32 * cSim.mint32_inv_scale.y;
+      double zil = (double)dzil_i32 * cSim.mint32_inv_scale.z;
+      
+      double xim = (double)dxim_i32 * cSim.mint32_inv_scale.x;
+      double yim = (double)dyim_i32 * cSim.mint32_inv_scale.y;
+      double zim = (double)dzim_i32 * cSim.mint32_inv_scale.z;
 
+      // Initialize loop variables
+      double invMassI = shakeParm.x;
+      double toler    = shakeParm.y;
       bool done = false;
+
+      // Iterative SHAKE loop
       for (int i = 0; i < 3000; i++) {
         done = true;
 
-        // Calculate nominal distance squared
-        double xpxx  = xpi - xpj;
-        double ypxx  = ypi - ypj;
-        double zpxx  = zpi - zpj;
+        // Calculate nominal distance squared using MINT32 INT32 arithmetic
+        // xpxx = xj - xi (current)
+        int dxp_i32 = xj_i32 - xi_i32;
+        int dyp_i32 = yj_i32 - yi_i32;
+        int dzp_i32 = zj_i32 - zi_i32;
+        
+        double xpxx = (double)dxp_i32 * cSim.mint32_inv_scale.x;
+        double ypxx = (double)dyp_i32 * cSim.mint32_inv_scale.y;
+        double zpxx = (double)dzp_i32 * cSim.mint32_inv_scale.z;
         double rpxx2 = xpxx*xpxx + ypxx*ypxx + zpxx*zpxx;
 
 #ifdef TISHAKE2
@@ -627,6 +756,7 @@
           invMassI = shakeParm.x;
           toler    = shakeParm.y;
         }
+#endif
 
         // Apply correction to first hydrogen
         double diff = toler - rpxx2;
@@ -637,22 +767,34 @@
           double rrpr = xij * xpxx + yij * ypxx + zij * zpxx;
           if (rrpr >= toler * (double)1.0e-06) {
             double acor = diff / (rrpr * (double)2.0 * (invMassI + INVMASSH));
-            double h    = xij * acor;
-            xpi += h * invMassI;
-            xpj -= h * INVMASSH;
-            h    = yij * acor;
-            ypi += h * invMassI;
-            ypj -= h * INVMASSH;
-            h    = zij * acor;
-            zpi += h * invMassI;
-            zpj -= h * INVMASSH;
+            
+            // Apply correction directly to MINT32 coordinates
+            // Correction vector is along the bond (xij, yij, zij) scaled by acor
+            // But we need to be careful with units.
+            // h = xij * acor (displacement in length units)
+            // displacement in MINT32 = h * scale
+            
+            xi_i32 -= (int)rint(xij * acor * invMassI * cSim.mint32_scale.x);
+            yi_i32 -= (int)rint(yij * acor * invMassI * cSim.mint32_scale.y);
+            zi_i32 -= (int)rint(zij * acor * invMassI * cSim.mint32_scale.z);
+            
+            xj_i32 += (int)rint(xij * acor * INVMASSH * cSim.mint32_scale.x);
+            yj_i32 += (int)rint(yij * acor * INVMASSH * cSim.mint32_scale.y);
+            zj_i32 += (int)rint(zij * acor * INVMASSH * cSim.mint32_scale.z);
           }
         }
-        xpxx  = xpi - xpk;
-        ypxx  = ypi - ypk;
-        zpxx  = zpi - zpk;
+        
+        // Second hydrogen
+        dxp_i32 = xk_i32 - xi_i32;
+        dyp_i32 = yk_i32 - yi_i32;
+        dzp_i32 = zk_i32 - zi_i32;
+        
+        xpxx = (double)dxp_i32 * cSim.mint32_inv_scale.x;
+        ypxx = (double)dyp_i32 * cSim.mint32_inv_scale.y;
+        zpxx = (double)dzp_i32 * cSim.mint32_inv_scale.z;
         rpxx2 = xpxx*xpxx + ypxx*ypxx + zpxx*zpxx;
 
+#ifdef TISHAKE2
         if(refShakeID.y < -1)
         {
           invMassI = shakeParm2.x;
@@ -663,32 +805,38 @@
           invMassI = shakeParm.x;
           toler    = shakeParm.y;
         }
+#endif
 
         // Apply correction to second hydrogen
         diff = toler - rpxx2;
         if (abs(diff) >= toler * cSim.tol) {
           done = false;
 
-          // Shake resetting of coordinate is done here
           double rrpr = xik*xpxx + yik*ypxx + zik*zpxx;
           if (rrpr >= toler * (double)1.0e-06) {
-            double acor             = diff / (rrpr * (double)2.0 * (invMassI + INVMASSH));
-            double h                = xik * acor;
-            xpi                    += h * invMassI;
-            xpk                    -= h * INVMASSH;
-            h                       = yik * acor;
-            ypi                    += h * invMassI;
-            ypk                    -= h * INVMASSH;
-            h                       = zik * acor;
-            zpi                    += h * invMassI;
-            zpk                    -= h * INVMASSH;
+            double acor = diff / (rrpr * (double)2.0 * (invMassI + INVMASSH));
+            
+            xi_i32 -= (int)rint(xik * acor * invMassI * cSim.mint32_scale.x);
+            yi_i32 -= (int)rint(yik * acor * invMassI * cSim.mint32_scale.y);
+            zi_i32 -= (int)rint(zik * acor * invMassI * cSim.mint32_scale.z);
+            
+            xk_i32 += (int)rint(xik * acor * INVMASSH * cSim.mint32_scale.x);
+            yk_i32 += (int)rint(yik * acor * INVMASSH * cSim.mint32_scale.y);
+            zk_i32 += (int)rint(zik * acor * INVMASSH * cSim.mint32_scale.z);
           }
         }
-        xpxx  = xpi - xpl;
-        ypxx  = ypi - ypl;
-        zpxx  = zpi - zpl;
+        
+        // Third hydrogen
+        dxp_i32 = xl_i32 - xi_i32;
+        dyp_i32 = yl_i32 - yi_i32;
+        dzp_i32 = zl_i32 - zi_i32;
+        
+        xpxx = (double)dxp_i32 * cSim.mint32_inv_scale.x;
+        ypxx = (double)dyp_i32 * cSim.mint32_inv_scale.y;
+        zpxx = (double)dzp_i32 * cSim.mint32_inv_scale.z;
         rpxx2 = xpxx*xpxx + ypxx*ypxx + zpxx*zpxx;
 
+#ifdef TISHAKE2
         if(refShakeID.z < -1)
         {
           invMassI = shakeParm2.x;
@@ -699,32 +847,38 @@
           invMassI = shakeParm.x;
           toler    = shakeParm.y;
         }
+#endif
 
         // Apply correction to third hydrogen
         diff = toler - rpxx2;
         if (abs(diff) >= toler * cSim.tol) {
           done = false;
 
-          // Shake resetting of coordinate is done here
           double rrpr = xil*xpxx + yil*ypxx + zil*zpxx;
           if (rrpr >= toler * (double)1.0e-06) {
             double acor = diff / (rrpr * (double)2.0 * (invMassI + INVMASSH));
-            double h = xil * acor;
-            xpi += h * invMassI;
-            xpl -= h * INVMASSH;
-            h    = yil * acor;
-            ypi += h * invMassI;
-            ypl -= h * INVMASSH;
-            h    = zil * acor;
-            zpi += h * invMassI;
-            zpl -= h * INVMASSH;
+            
+            xi_i32 -= (int)rint(xil * acor * invMassI * cSim.mint32_scale.x);
+            yi_i32 -= (int)rint(yil * acor * invMassI * cSim.mint32_scale.y);
+            zi_i32 -= (int)rint(zil * acor * invMassI * cSim.mint32_scale.z);
+            
+            xl_i32 += (int)rint(xil * acor * INVMASSH * cSim.mint32_scale.x);
+            yl_i32 += (int)rint(yil * acor * INVMASSH * cSim.mint32_scale.y);
+            zl_i32 += (int)rint(zil * acor * INVMASSH * cSim.mint32_scale.z);
           }
         }
-        xpxx  = xpi - xpm;
-        ypxx  = ypi - ypm;
-        zpxx  = zpi - zpm;
+        
+        // Fourth hydrogen
+        dxp_i32 = xm_i32 - xi_i32;
+        dyp_i32 = ym_i32 - yi_i32;
+        dzp_i32 = zm_i32 - zi_i32;
+        
+        xpxx = (double)dxp_i32 * cSim.mint32_inv_scale.x;
+        ypxx = (double)dyp_i32 * cSim.mint32_inv_scale.y;
+        zpxx = (double)dzp_i32 * cSim.mint32_inv_scale.z;
         rpxx2 = xpxx*xpxx + ypxx*ypxx + zpxx*zpxx;
 
+#ifdef TISHAKE2
         if(refShakeID.w < -1)
         {
           invMassI = shakeParm2.x;
@@ -735,25 +889,24 @@
           invMassI = shakeParm.x;
           toler    = shakeParm.y;
         }
+#endif
 
         // Apply correction to fourth hydrogen
         diff = toler - rpxx2;
         if (abs(diff) >= toler * cSim.tol) {
           done = false;
 
-          // Shake resetting of coordinate is done here
           double rrpr = xim*xpxx + yim*ypxx + zim*zpxx;
           if (rrpr >= toler * (double)1.0e-06) {
             double acor = diff / (rrpr * (double)2.0 * (invMassI + INVMASSH));
-            double h    = xim * acor;
-            xpi += h * invMassI;
-            xpm -= h * INVMASSH;
-            h    = yim * acor;
-            ypi += h * invMassI;
-            ypm -= h * INVMASSH;
-            h    = zim * acor;
-            zpi += h * invMassI;
-            zpm -= h * INVMASSH;
+            
+            xi_i32 -= (int)rint(xim * acor * invMassI * cSim.mint32_scale.x);
+            yi_i32 -= (int)rint(yim * acor * invMassI * cSim.mint32_scale.y);
+            zi_i32 -= (int)rint(zim * acor * invMassI * cSim.mint32_scale.z);
+            
+            xm_i32 += (int)rint(xim * acor * INVMASSH * cSim.mint32_scale.x);
+            ym_i32 += (int)rint(yim * acor * INVMASSH * cSim.mint32_scale.y);
+            zm_i32 += (int)rint(zim * acor * INVMASSH * cSim.mint32_scale.z);
           }
         }
 
@@ -762,56 +915,36 @@
           break;
         }
       }
-
-      // Write out results if converged, but there's no really good
-      // way to indicate failure so we'll let the simulation heading
-      // off to Neptune do that for us.  Wish there were a better way,
-      // but until the CPU needs something from the GPU, those are the
-      // the breaks.  I guess, technically, we could just set a flag to NOP
-      // the simulation from here and then carry that result through upon
-      // the next ntpr, ntwc, or ntwx update, but I leave that up to you
-      // guys to implement that (or not).
+      
+      // Write back MINT32 coordinates and derive float from them
       if (done) {
-        PATOMX(shakeID1) = xpi;
-        PATOMY(shakeID1) = ypi;
-        PATOMZ(shakeID1) = zpi;
-#ifndef SHAKE_NEIGHBORLIST
-        PMEFloat2 xyi = {(PMEFloat)xpi, (PMEFloat)ypi};
-        cSim.pAtomXYSP[shakeID1] = xyi;
-        cSim.pAtomZSP[shakeID1]  = zpi;
-#endif
-        PATOMX(shakeID2.x) = xpj;
-        PATOMY(shakeID2.x) = ypj;
-        PATOMZ(shakeID2.x) = zpj;
-#ifndef SHAKE_NEIGHBORLIST
-        PMEFloat2 xyj = {(PMEFloat)xpj, (PMEFloat)ypj};
-        cSim.pAtomXYSP[shakeID2.x] = xyj;
-        cSim.pAtomZSP[shakeID2.x]  = zpj;
-#endif
-        PATOMX(shakeID2.y)              = xpk;
-        PATOMY(shakeID2.y)              = ypk;
-        PATOMZ(shakeID2.y)              = zpk;
-#ifndef SHAKE_NEIGHBORLIST
-        PMEFloat2 xyk = {(PMEFloat)xpk, (PMEFloat)ypk};
-        cSim.pAtomXYSP[shakeID2.y] = xyk;
-        cSim.pAtomZSP[shakeID2.y]  = zpk;
-#endif
-        PATOMX(shakeID2.z) = xpl;
-        PATOMY(shakeID2.z) = ypl;
-        PATOMZ(shakeID2.z) = zpl;
-#ifndef SHAKE_NEIGHBORLIST
-        PMEFloat2 xyl = {(PMEFloat)xpl, (PMEFloat)ypl};
-        cSim.pAtomXYSP[shakeID2.z] = xyl;
-        cSim.pAtomZSP[shakeID2.z]  = zpl;
-#endif
-        PATOMX(shakeID2.w) = xpm;
-        PATOMY(shakeID2.w) = ypm;
-        PATOMZ(shakeID2.w) = zpm;
-#ifndef SHAKE_NEIGHBORLIST
-        PMEFloat2 xym = {(PMEFloat)xpm, (PMEFloat)ypm};
-        cSim.pAtomXYSP[shakeID2.w] = xym;
-        cSim.pAtomZSP[shakeID2.w]  = zpm;
-#endif
+        // Write MINT32 coordinates (authoritative)
+        pCoord[shakeID1] = make_int4(xi_i32, yi_i32, zi_i32, 0);
+        pCoord[shakeID2.x] = make_int4(xj_i32, yj_i32, zj_i32, 0);
+        pCoord[shakeID2.y] = make_int4(xk_i32, yk_i32, zk_i32, 0);
+        pCoord[shakeID2.z] = make_int4(xl_i32, yl_i32, zl_i32, 0);
+        pCoord[shakeID2.w] = make_int4(xm_i32, ym_i32, zm_i32, 0);
+        
+        // Update float mirrors
+        PATOMX(shakeID1) = (double)xi_i32 * cSim.mint32_inv_scale.x;
+        PATOMY(shakeID1) = (double)yi_i32 * cSim.mint32_inv_scale.y;
+        PATOMZ(shakeID1) = (double)zi_i32 * cSim.mint32_inv_scale.z;
+        
+        PATOMX(shakeID2.x) = (double)xj_i32 * cSim.mint32_inv_scale.x;
+        PATOMY(shakeID2.x) = (double)yj_i32 * cSim.mint32_inv_scale.y;
+        PATOMZ(shakeID2.x) = (double)zj_i32 * cSim.mint32_inv_scale.z;
+        
+        PATOMX(shakeID2.y) = (double)xk_i32 * cSim.mint32_inv_scale.x;
+        PATOMY(shakeID2.y) = (double)yk_i32 * cSim.mint32_inv_scale.y;
+        PATOMZ(shakeID2.y) = (double)zk_i32 * cSim.mint32_inv_scale.z;
+        
+        PATOMX(shakeID2.z) = (double)xl_i32 * cSim.mint32_inv_scale.x;
+        PATOMY(shakeID2.z) = (double)yl_i32 * cSim.mint32_inv_scale.y;
+        PATOMZ(shakeID2.z) = (double)zl_i32 * cSim.mint32_inv_scale.z;
+        
+        PATOMX(shakeID2.w) = (double)xm_i32 * cSim.mint32_inv_scale.x;
+        PATOMY(shakeID2.w) = (double)ym_i32 * cSim.mint32_inv_scale.y;
+        PATOMZ(shakeID2.w) = (double)zm_i32 * cSim.mint32_inv_scale.z;
       }
     }
   }
@@ -1044,6 +1177,37 @@
         ypq = PATOMY(shakeID3.w);
         zpq = PATOMZ(shakeID3.w);
       }
+
+      // Load MINT32 coordinates for TI SHAKE branch
+      int4 coord_i = cSim.pAtomCoord_M32[shakeID1];
+      int4 old_coord_i = cSim.pOldAtomCoord_M32[shakeID1];
+      int xi_i32 = coord_i.x;
+      int yi_i32 = coord_i.y;
+      int zi_i32 = coord_i.z;
+      int old_xi_i32 = old_coord_i.x;
+      int old_yi_i32 = old_coord_i.y;
+      int old_zi_i32 = old_coord_i.z;
+
+      int4 coord_j = cSim.pAtomCoord_M32[shakeID2.x];
+      int xj_i32 = coord_j.x;
+      int yj_i32 = coord_j.y;
+      int zj_i32 = coord_j.z;
+
+      int4 coord_k = cSim.pAtomCoord_M32[shakeID2.y];
+      int xk_i32 = coord_k.x;
+      int yk_i32 = coord_k.y;
+      int zk_i32 = coord_k.z;
+
+      int4 coord_l = cSim.pAtomCoord_M32[shakeID2.z];
+      int xl_i32 = coord_l.x;
+      int yl_i32 = coord_l.y;
+      int zl_i32 = coord_l.z;
+
+      int4 coord_m = cSim.pAtomCoord_M32[shakeID2.w];
+      int xm_i32 = coord_m.x;
+      int ym_i32 = coord_m.y;
+      int zm_i32 = coord_m.z;
+
       double invMassI = shakeParm.x;
       double toler    = shakeParm.y;
 
@@ -1083,12 +1247,33 @@
       for (int i = 0; i < 3000; i++) {
         done = true;
 
-        // Calculate nominal distance squared
-        double xpxx  = xpi - xpj;
-        double ypxx  = ypi - ypj;
-        double zpxx  = zpi - zpj;
+        // Re-quantize updated positions to MINT32 for this iteration
+        xi_i32 = (int)rint(xpi * cSim.mint32_scale.x);
+        yi_i32 = (int)rint(ypi * cSim.mint32_scale.y);
+        zi_i32 = (int)rint(zpi * cSim.mint32_scale.z);
+        xj_i32 = (int)rint(xpj * cSim.mint32_scale.x);
+        yj_i32 = (int)rint(ypj * cSim.mint32_scale.y);
+        zj_i32 = (int)rint(zpj * cSim.mint32_scale.z);
+        xk_i32 = (int)rint(xpk * cSim.mint32_scale.x);
+        yk_i32 = (int)rint(ypk * cSim.mint32_scale.y);
+        zk_i32 = (int)rint(zpk * cSim.mint32_scale.z);
+        xl_i32 = (int)rint(xpl * cSim.mint32_scale.x);
+        yl_i32 = (int)rint(ypl * cSim.mint32_scale.y);
+        zl_i32 = (int)rint(zpl * cSim.mint32_scale.z);
+        xm_i32 = (int)rint(xpm * cSim.mint32_scale.x);
+        ym_i32 = (int)rint(ypm * cSim.mint32_scale.y);
+        zm_i32 = (int)rint(zpm * cSim.mint32_scale.z);
+
+        // Calculate nominal distance squared using MINT32 INT32 arithmetic
+        int dxp_i32 = xi_i32 - xj_i32;
+        int dyp_i32 = yi_i32 - yj_i32;
+        int dzp_i32 = zi_i32 - zj_i32;
+        double xpxx  = (double)dxp_i32 * cSim.mint32_inv_scale.x;
+        double ypxx  = (double)dyp_i32 * cSim.mint32_inv_scale.y;
+        double zpxx  = (double)dzp_i32 * cSim.mint32_inv_scale.z;
         double rpxx2 = xpxx*xpxx + ypxx*ypxx + zpxx*zpxx;
 
+#ifdef TISHAKE2
         if(refShakeID2.x < -1)
         {
           invMassI = shakeParm2.x;
@@ -1390,6 +1575,27 @@
         }
 #endif //TISHAKE2
 
+        // Refresh MINT32 coordinates after any corrections
+        xi_i32 = (int)rint(xpi * cSim.mint32_scale.x);
+        yi_i32 = (int)rint(ypi * cSim.mint32_scale.y);
+        zi_i32 = (int)rint(zpi * cSim.mint32_scale.z);
+
+        xj_i32 = (int)rint(xpj * cSim.mint32_scale.x);
+        yj_i32 = (int)rint(ypj * cSim.mint32_scale.y);
+        zj_i32 = (int)rint(zpj * cSim.mint32_scale.z);
+
+        xk_i32 = (int)rint(xpk * cSim.mint32_scale.x);
+        yk_i32 = (int)rint(ypk * cSim.mint32_scale.y);
+        zk_i32 = (int)rint(zpk * cSim.mint32_scale.z);
+
+        xl_i32 = (int)rint(xpl * cSim.mint32_scale.x);
+        yl_i32 = (int)rint(ypl * cSim.mint32_scale.y);
+        zl_i32 = (int)rint(zpl * cSim.mint32_scale.z);
+
+        xm_i32 = (int)rint(xpm * cSim.mint32_scale.x);
+        ym_i32 = (int)rint(ypm * cSim.mint32_scale.y);
+        zm_i32 = (int)rint(zpm * cSim.mint32_scale.z);
+
         // Check for convergence
         if (done) {
           break;
@@ -1408,81 +1614,81 @@
         PATOMX(shakeID1) = xpi;
         PATOMY(shakeID1) = ypi;
         PATOMZ(shakeID1) = zpi;
-#ifndef SHAKE_NEIGHBORLIST
-        PMEFloat2 xyi = {(PMEFloat)xpi, (PMEFloat)ypi};
-        cSim.pAtomXYSP[shakeID1] = xyi;
-        cSim.pAtomZSP[shakeID1]  = zpi;
-#endif
+        int4 coord = cSim.pAtomCoord_M32[shakeID1];
+        coord.x = (int)(long long)rint(xpi * cSim.mint32_scale.x);
+        coord.y = (int)(long long)rint(ypi * cSim.mint32_scale.y);
+        coord.z = (int)(long long)rint(zpi * cSim.mint32_scale.z);
+        cSim.pAtomCoord_M32[shakeID1] = coord;
         PATOMX(shakeID2.x) = xpj;
         PATOMY(shakeID2.x) = ypj;
         PATOMZ(shakeID2.x) = zpj;
-#ifndef SHAKE_NEIGHBORLIST
-        PMEFloat2 xyj = {(PMEFloat)xpj, (PMEFloat)ypj};
-        cSim.pAtomXYSP[shakeID2.x] = xyj;
-        cSim.pAtomZSP[shakeID2.x]  = zpj;
-#endif
+        coord = cSim.pAtomCoord_M32[shakeID2.x];
+        coord.x = (int)(long long)rint(xpj * cSim.mint32_scale.x);
+        coord.y = (int)(long long)rint(ypj * cSim.mint32_scale.y);
+        coord.z = (int)(long long)rint(zpj * cSim.mint32_scale.z);
+        cSim.pAtomCoord_M32[shakeID2.x] = coord;
         PATOMX(shakeID2.y)              = xpk;
         PATOMY(shakeID2.y)              = ypk;
         PATOMZ(shakeID2.y)              = zpk;
-#ifndef SHAKE_NEIGHBORLIST
-        PMEFloat2 xyk = {(PMEFloat)xpk, (PMEFloat)ypk};
-        cSim.pAtomXYSP[shakeID2.y] = xyk;
-        cSim.pAtomZSP[shakeID2.y]  = zpk;
-#endif
+        coord = cSim.pAtomCoord_M32[shakeID2.y];
+        coord.x = (int)(long long)rint(xpk * cSim.mint32_scale.x);
+        coord.y = (int)(long long)rint(ypk * cSim.mint32_scale.y);
+        coord.z = (int)(long long)rint(zpk * cSim.mint32_scale.z);
+        cSim.pAtomCoord_M32[shakeID2.y] = coord;
         PATOMX(shakeID2.z) = xpl;
         PATOMY(shakeID2.z) = ypl;
         PATOMZ(shakeID2.z) = zpl;
-#ifndef SHAKE_NEIGHBORLIST
-        PMEFloat2 xyl = {(PMEFloat)xpl, (PMEFloat)ypl};
-        cSim.pAtomXYSP[shakeID2.z] = xyl;
-        cSim.pAtomZSP[shakeID2.z]  = zpl;
-#endif
+        coord = cSim.pAtomCoord_M32[shakeID2.z];
+        coord.x = (int)(long long)rint(xpl * cSim.mint32_scale.x);
+        coord.y = (int)(long long)rint(ypl * cSim.mint32_scale.y);
+        coord.z = (int)(long long)rint(zpl * cSim.mint32_scale.z);
+        cSim.pAtomCoord_M32[shakeID2.z] = coord;
         PATOMX(shakeID2.w) = xpm;
         PATOMY(shakeID2.w) = ypm;
         PATOMZ(shakeID2.w) = zpm;
-#ifndef SHAKE_NEIGHBORLIST
-        PMEFloat2 xym = {(PMEFloat)xpm, (PMEFloat)ypm};
-        cSim.pAtomXYSP[shakeID2.w] = xym;
-        cSim.pAtomZSP[shakeID2.w]  = zpm;
-#endif
+        coord = cSim.pAtomCoord_M32[shakeID2.w];
+        coord.x = (int)(long long)rint(xpm * cSim.mint32_scale.x);
+        coord.y = (int)(long long)rint(ypm * cSim.mint32_scale.y);
+        coord.z = (int)(long long)rint(zpm * cSim.mint32_scale.z);
+        cSim.pAtomCoord_M32[shakeID2.w] = coord;
 #ifdef TISHAKE2
         PATOMX(shakeID3.x) = xpn;
         PATOMY(shakeID3.x) = ypn;
         PATOMZ(shakeID3.x) = zpn;
-#ifndef SHAKE_NEIGHBORLIST
-        PMEFloat2 xyn = {(PMEFloat)xpn, (PMEFloat)ypn};
-        cSim.pAtomXYSP[shakeID2.x] = xyn;
-        cSim.pAtomZSP[shakeID2.x]  = zpn;
-#endif
+        coord = cSim.pAtomCoord_M32[shakeID3.x];
+        coord.x = (int)(long long)rint(xpn * cSim.mint32_scale.x);
+        coord.y = (int)(long long)rint(ypn * cSim.mint32_scale.y);
+        coord.z = (int)(long long)rint(zpn * cSim.mint32_scale.z);
+        cSim.pAtomCoord_M32[shakeID3.x] = coord;
         if(shakeID3.y != -1) {
           PATOMX(shakeID3.y) = xpo;
           PATOMY(shakeID3.y) = ypo;
           PATOMZ(shakeID3.y) = zpo;
-#ifndef SHAKE_NEIGHBORLIST
-          PMEFloat2 xyo = {(PMEFloat)xpo, (PMEFloat)ypo};
-          cSim.pAtomXYSP[shakeID2.y] = xyo;
-          cSim.pAtomZSP[shakeID2.y]  = zpo;
-#endif
+          coord = cSim.pAtomCoord_M32[shakeID3.y];
+          coord.x = (int)(long long)rint(xpo * cSim.mint32_scale.x);
+          coord.y = (int)(long long)rint(ypo * cSim.mint32_scale.y);
+          coord.z = (int)(long long)rint(zpo * cSim.mint32_scale.z);
+          cSim.pAtomCoord_M32[shakeID3.y] = coord;
         }
         if(shakeID3.z != -1) {
           PATOMX(shakeID3.z) = xpp;
           PATOMY(shakeID3.z) = ypp;
           PATOMZ(shakeID3.z) = zpp;
-#ifndef SHAKE_NEIGHBORLIST
-          PMEFloat2 xyp = {(PMEFloat)xpp, (PMEFloat)ypp};
-          cSim.pAtomXYSP[shakeID2.z] = xyp;
-          cSim.pAtomZSP[shakeID2.z]  = zpp;
-#endif
+          coord = cSim.pAtomCoord_M32[shakeID3.z];
+          coord.x = (int)(long long)rint(xpp * cSim.mint32_scale.x);
+          coord.y = (int)(long long)rint(ypp * cSim.mint32_scale.y);
+          coord.z = (int)(long long)rint(zpp * cSim.mint32_scale.z);
+          cSim.pAtomCoord_M32[shakeID3.z] = coord;
         }
         if(shakeID3.w != -1) {
           PATOMX(shakeID3.w) = xpq;
           PATOMY(shakeID3.w) = ypq;
           PATOMZ(shakeID3.w) = zpq;
-#ifndef SHAKE_NEIGHBORLIST
-          PMEFloat2 xyq = {(PMEFloat)xpq, (PMEFloat)ypq};
-          cSim.pAtomXYSP[shakeID2.w] = xyq;
-          cSim.pAtomZSP[shakeID2.w]  = zpq;
-#endif
+          coord = cSim.pAtomCoord_M32[shakeID3.w];
+          coord.x = (int)(long long)rint(xpq * cSim.mint32_scale.x);
+          coord.y = (int)(long long)rint(ypq * cSim.mint32_scale.y);
+          coord.z = (int)(long long)rint(zpq * cSim.mint32_scale.z);
+          cSim.pAtomCoord_M32[shakeID3.w] = coord;
         }
 #endif
       }
@@ -1493,4 +1699,3 @@
 #undef PATOMZ
 #undef INVMASSH
 }
-
diff --git "a/C:\\amber\\amber_master\\src\\pmemd\\src/cuda/kU.h" "b/C:\\amber\\amber_mint\\src\\pmemd\\src/cuda/kU.h"
index a4d9272..f15f221 100644
--- "a/C:\\amber\\amber_master\\src\\pmemd\\src/cuda/kU.h"
+++ "b/C:\\amber\\amber_mint\\src\\pmemd\\src/cuda/kU.h"
@@ -129,6 +129,16 @@
     PMEAccumulator fx = cSim.pForceXAccumulator[pos];
     PMEAccumulator fy = cSim.pForceYAccumulator[pos];
     PMEAccumulator fz = cSim.pForceZAccumulator[pos];
+
+    // DEBUG: force sample to catch post-NL corruption
+    // if (blockIdx.x == 0 && threadIdx.x < 2) {
+    //   double fx_d = (double)fx * (double)ONEOVERFORCESCALE;
+    //   double fy_d = (double)fy * (double)ONEOVERFORCESCALE;
+    //   double fz_d = (double)fz * (double)ONEOVERFORCESCALE;
+    //   printf("FORCE_SAMPLE pos=%u fx=%e fy=%e fz=%e fx_raw=%lld fy_raw=%lld fz_raw=%lld\n",
+    //          pos, fx_d, fy_d, fz_d,
+    //          (long long)fx, (long long)fy, (long long)fz);
+    // }
 #if defined(UPDATE_NTP) && !defined(MPI)
     PMEAccumulator nfx = cSim.pNBForceXAccumulator[pos];
     PMEAccumulator nfy = cSim.pNBForceYAccumulator[pos];
@@ -141,9 +151,27 @@
     double forceY = (double)fy * (double)ONEOVERFORCESCALE;
     double forceZ = (double)fz * (double)ONEOVERFORCESCALE;
 #endif
+    // FROZEN MODE: Print forces then force to 0
+    if (cSim.bFrozen) {
+        if (blockIdx.x == 0 && threadIdx.x == 0) {
+             printf("DEBUG: pForceXAcc=%p pNBForceXAcc=%p\n", cSim.pForceXAccumulator, cSim.pNBForceXAccumulator);
+        }
+        if (fabs(forceX) > 5000.0 || fabs(forceY) > 5000.0 || fabs(forceZ) > 5000.0) {
+             printf("LARGE_FORCE pos=%u f=(%f, %f, %f)\n", pos, forceX, forceY, forceZ);
+        }
+        forceX = 0.0;
+        forceY = 0.0;
+        forceZ = 0.0;
+    }
+    if (isnan(forceX) || isinf(forceX) || isnan(forceY) || isinf(forceY) || isnan(forceZ) || isinf(forceZ)) {
+        printf("ERROR kU: pos=%d force=(%f,%f,%f)\n", pos, forceX, forceY, forceZ);
+    }
     double velX  = VELX(pos);
     double velY  = VELY(pos);
     double velZ  = VELZ(pos);
+    if (isnan(velX) || isinf(velX) || isnan(velY) || isinf(velY) || isnan(velZ) || isinf(velZ)) {
+        printf("ERROR kU: pos=%d old_vel=(%f,%f,%f)\n", pos, velX, velY, velZ);
+    }
 #ifdef UPDATE_LANGEVIN
     double gaussX = cSim.pRandomX[pos + rpos];
     double gaussY = cSim.pRandomY[pos + rpos];
@@ -151,6 +179,12 @@
     double rsd    = sqrt(sdfac * aamass);
 #endif
     double wfac = invMass * dtx;
+    if (isnan(invMass) || isinf(invMass)) {
+        printf("ERROR kU: pos=%d invMass=%f\n", pos, invMass);
+    }
+    if (isnan(wfac) || isinf(wfac)) {
+        printf("ERROR kU: pos=%d wfac=%f invMass=%f dtx=%f\n", pos, wfac, invMass, dtx);
+    }
     
     // Save previous velocities and positions
     LVELX(pos) = velX;
@@ -159,6 +193,10 @@
     cSim.pOldAtomX[pos] = atomX;
     cSim.pOldAtomY[pos] = atomY;
     cSim.pOldAtomZ[pos] = atomZ;
+
+    // Save MINT32 coordinates for SHAKE (pure INT32 precision)
+    cSim.pOldAtomCoord_M32[pos] = cSim.pAtomCoord_M32[pos];
+    
     // Update velocities
 #ifdef UPDATE_RELAXMD
     if (index >= cSim.first_update_atom) {
@@ -298,25 +336,100 @@
         velZ = max(min(velZ, cSim.vlimit), -cSim.vlimit);
       }
 
+      // DEBUG: Check what we're writing for first few atoms
+      // if (pos < 1) {
+      //     printf("kU_WRITE: pos=%d writing vel=(%f,%f,%f) lvel=(%f,%f,%f) pImageVelX=%p\n", 
+      //            pos, velX, velY, velZ, LVELX(pos), LVELY(pos), LVELZ(pos), cSim.pImageVelX);
+      // }
+
+      if (isnan(velX) || isinf(velX) || isnan(velY) || isinf(velY) || isnan(velZ) || isinf(velZ)) {
+          printf("ERROR kU: pos=%d vel=(%f,%f,%f) old_vel=(%f,%f,%f) force=(%f,%f,%f) wfac=%f invMass=%f\n", 
+                 pos, velX, velY, velZ, LVELX(pos), LVELY(pos), LVELZ(pos), forceX, forceY, forceZ, wfac, invMass);
+      }
+      
       // Save new velocity
       VELX(pos)= velX;
       VELY(pos)= velY;
       VELZ(pos)= velZ;
 
-      // Update positions for SHAKE and kinetic energy kernel
-      double newAtomX = atomX + velX*dtx;
-      double newAtomY = atomY + velY*dtx;
-      double newAtomZ = atomZ + velZ*dtx;
+#ifdef UPDATE_NEIGHBORLIST
+      // Keep unsorted velocities in original atom order up to date for NL rebuilds
+      unsigned int originalIndex = cSim.pImageAtom[pos];
+
+      if (originalIndex < cSim.atoms) {
+        cSim.pVelX[originalIndex]  = velX;
+        cSim.pVelY[originalIndex]  = velY;
+        cSim.pVelZ[originalIndex]  = velZ;
+        cSim.pLVelX[originalIndex] = LVELX(pos);
+        cSim.pLVelY[originalIndex] = LVELY(pos);
+        cSim.pLVelZ[originalIndex] = LVELZ(pos);
+      }
+#endif
+
+      // MINT32 INTEGRATION: Use pure integer arithmetic to avoid wrapping discontinuities
+      // Read current MINT32 coordinates
+      int4 coord = cSim.pAtomCoord_M32[pos];
+
+      // Convert velocity delta to MINT32 (integer overflow wraps automatically)
+      int dx = (int)rint(velX * dtx * cSim.mint32_scale.x);
+      int dy = (int)rint(velY * dtx * cSim.mint32_scale.y);
+      int dz = (int)rint(velZ * dtx * cSim.mint32_scale.z);
+
+      // Integer addition - overflow wraps automatically (no explicit wrapping needed!)
+      coord.x += dx;
+      coord.y += dy;
+      coord.z += dz;
+
+      // Store updated MINT32 coordinates
+      cSim.pAtomCoord_M32[pos] = coord;
+#ifdef UPDATE_NEIGHBORLIST
+      // MINT32: Also update the unsorted array (Array A) which is the source for the next NL build
+      if (originalIndex < cSim.atoms) {
+        // Save current (old) MINT32 coordinate before updating to new coordinate
+        // This is critical for velocity recalculation after NL rebuild!
+        cSim.pOldAtomCoordUnsorted_M32[originalIndex] = cSim.pAtomCoordUnsorted_M32[originalIndex];
+        cSim.pAtomCoordUnsorted_M32[originalIndex] = coord;
+      }
+
+      // DEBUG: Sample a few updated coords to ensure integration is writing
+      // if ((pos < 2) && (blockIdx.x == 0)) {
+      //   double fx = (double)coord.x * cSim.mint32_inv_scale.x;
+      //   double fy = (double)coord.y * cSim.mint32_inv_scale.y;
+      //   double fz = (double)coord.z * cSim.mint32_inv_scale.z;
+      //   double sfx = fx + 0.5 * cSim.a;
+      //   double sfy = fy + 0.5 * cSim.b;
+      //   double sfz = fz + 0.5 * cSim.c;
+      //   int4 u = {0, 0, 0, 0};
+      //   unsigned int lookup = 0xffffffffu;
+      //   if (originalIndex < cSim.atoms) {
+      //     u = cSim.pAtomCoordUnsorted_M32[originalIndex];
+      //     lookup = cSim.pImageAtomLookup[originalIndex];
+      //   } else {
+      //     printf("KU_BAD_ORIG pos=%u originalIndex=%u atoms=%u\n", pos, originalIndex, cSim.atoms);
+      //   }
+      //   double ufx = (double)u.x * cSim.mint32_inv_scale.x;
+      //   double ufy = (double)u.y * cSim.mint32_inv_scale.y;
+      //   double ufz = (double)u.z * cSim.mint32_inv_scale.z;
+      //   double usfx = ufx + 0.5 * cSim.a;
+      //   double usfy = ufy + 0.5 * cSim.b;
+      //   double usfz = ufz + 0.5 * cSim.c;
+      //   printf("KU_COORD pos=%u coord=(%d,%d,%d) xyz=(%.6f,%.6f,%.6f) xyz_shift=(%.6f,%.6f,%.6f) "
+      //          "unsorted[%u]=(%d,%d,%d) xyz_u=(%.6f,%.6f,%.6f) xyz_u_shift=(%.6f,%.6f,%.6f) lookup=%u\n",
+      //          pos, coord.x, coord.y, coord.z, fx, fy, fz, sfx, sfy, sfz,
+      //          originalIndex, u.x, u.y, u.z, ufx, ufy, ufz, usfx, usfy, usfz, lookup);
+      // }
+#endif
+
+      // Derive floating-point coordinates from MINT32 (always wrapped consistently)
+      double newAtomX = (double)coord.x * cSim.mint32_inv_scale.x;
+      double newAtomY = (double)coord.y * cSim.mint32_inv_scale.y;
+      double newAtomZ = (double)coord.z * cSim.mint32_inv_scale.z;
+
+      // Store in pImageX/Y/Z for SHAKE and kinetic energy kernel
       ATOMX(pos) = newAtomX;
       ATOMY(pos) = newAtomY;
       ATOMZ(pos) = newAtomZ;
-#ifndef UPDATE_NEIGHBORLIST
-      PMEFloat2 xy;
-      xy.x = newAtomX;
-      xy.y = newAtomY;
-      cSim.pAtomXYSP[pos] = xy;
-      cSim.pAtomZSP[pos]  = newAtomZ;
-#endif
+      // NOTE: pAtomXYSP/ZSP are populated by kMINT32ToFloat which reads from pImageX/Y/Z
 
 #ifdef UPDATE_RELAXMD
     }
diff --git "a/C:\\amber\\amber_mint\\src\\pmemd\\src/cuda/mint32_debug.h" "b/C:\\amber\\amber_mint\\src\\pmemd\\src/cuda/mint32_debug.h"
new file mode 100644
index 0000000..52e85b0
--- /dev/null
+++ "b/C:\\amber\\amber_mint\\src\\pmemd\\src/cuda/mint32_debug.h"
@@ -0,0 +1,58 @@
+#ifndef MINT32_DEBUG_H
+#define MINT32_DEBUG_H
+
+#include <stdio.h>
+#include <time.h>
+
+// Enable/disable via environment variable or compile flag
+#ifdef MINT32_DEBUG_ENABLE
+#define MINT32_DEBUG_LOG_FILE "/tmp/mint32_debug.log"
+
+static inline void mint32_log(const char* func, int line, const char* msg) {
+    FILE* f = fopen(MINT32_DEBUG_LOG_FILE, "a");
+    if (f) {
+        time_t now = time(NULL);
+        struct tm* t = localtime(&now);
+        fprintf(f, "[%02d:%02d:%02d] %s:%d - %s\n",
+                t->tm_hour, t->tm_min, t->tm_sec, func, line, msg);
+        fflush(f);
+        fclose(f);
+    }
+}
+
+static inline void mint32_log_int(const char* func, int line, const char* name, int value) {
+    FILE* f = fopen(MINT32_DEBUG_LOG_FILE, "a");
+    if (f) {
+        time_t now = time(NULL);
+        struct tm* t = localtime(&now);
+        fprintf(f, "[%02d:%02d:%02d] %s:%d - %s = %d\n",
+                t->tm_hour, t->tm_min, t->tm_sec, func, line, name, value);
+        fflush(f);
+        fclose(f);
+    }
+}
+
+static inline void mint32_log_double(const char* func, int line, const char* name, double value) {
+    FILE* f = fopen(MINT32_DEBUG_LOG_FILE, "a");
+    if (f) {
+        time_t now = time(NULL);
+        struct tm* t = localtime(&now);
+        fprintf(f, "[%02d:%02d:%02d] %s:%d - %s = %f\n",
+                t->tm_hour, t->tm_min, t->tm_sec, func, line, name, value);
+        fflush(f);
+        fclose(f);
+    }
+}
+
+#define MINT32_LOG(msg) mint32_log(__FUNCTION__, __LINE__, msg)
+#define MINT32_LOG_INT(name, value) mint32_log_int(__FUNCTION__, __LINE__, #name, value)
+#define MINT32_LOG_DOUBLE(name, value) mint32_log_double(__FUNCTION__, __LINE__, #name, value)
+
+#else
+// Disabled - compile to nothing
+#define MINT32_LOG(msg)
+#define MINT32_LOG_INT(name, value)
+#define MINT32_LOG_DOUBLE(name, value)
+#endif
+
+#endif // MINT32_DEBUG_H
diff --git "a/C:\\amber\\amber_mint\\src\\pmemd\\src/cuda/test_mint32_distance" "b/C:\\amber\\amber_mint\\src\\pmemd\\src/cuda/test_mint32_distance"
new file mode 100644
index 0000000..afecfea
Binary files /dev/null and "b/C:\\amber\\amber_mint\\src\\pmemd\\src/cuda/test_mint32_distance" differ
diff --git "a/C:\\amber\\amber_mint\\src\\pmemd\\src/cuda/test_mint32_distance.cu" "b/C:\\amber\\amber_mint\\src\\pmemd\\src/cuda/test_mint32_distance.cu"
new file mode 100644
index 0000000..4253a06
--- /dev/null
+++ "b/C:\\amber\\amber_mint\\src\\pmemd\\src/cuda/test_mint32_distance.cu"
@@ -0,0 +1,93 @@
+#include <stdio.h>
+#include <math.h>
+#include <cuda_runtime.h>
+#include <stdlib.h>
+
+// Emulate the dynamic scaling logic
+__host__ __device__ void get_scale_factors(double box, double* scale, double* inv_scale) {
+    // Map box to full 2^32 range for implicit PBC
+    // scale = 2^32 / box
+    *scale = 4294967296.0 / box;
+    *inv_scale = 1.0 / *scale;
+}
+
+__global__ void test_mint32_dist_kernel(double x1, double x2, double box, double scale, double inv_scale, double* d_out_mint32, double* d_out_ref) {
+    // Reference double calculation with explicit PBC
+    double dx_ref = x1 - x2;
+    while (dx_ref > box * 0.5) dx_ref -= box;
+    while (dx_ref < -box * 0.5) dx_ref += box;
+    *d_out_ref = dx_ref;
+
+    // MINT32 calculation with Implicit PBC
+    // 1. Convert to int using dynamic scale
+    // Use long long cast to handle values that exceed int range (e.g. > box/2)
+    // but fit in the periodic domain.
+    int ix1 = (int)(long long)rint(x1 * scale);
+    int ix2 = (int)(long long)rint(x2 * scale);
+
+    // 2. Distance in integer space (overflow handles PBC)
+    int idx = ix1 - ix2;
+
+    // 3. Convert back to double
+    *d_out_mint32 = (double)idx * inv_scale;
+}
+
+int main() {
+    double h_out_mint32, h_out_ref;
+    double *d_out_mint32, *d_out_ref;
+
+    cudaMalloc(&d_out_mint32, sizeof(double));
+    cudaMalloc(&d_out_ref, sizeof(double));
+
+    struct TestCase {
+        double x1;
+        double x2;
+        double box;
+        const char* name;
+    };
+
+    TestCase tests[] = {
+        {10.5, 12.5, 100.0, "Simple Distance"},
+        {1.0, 99.0, 100.0, "PBC Crossing (Positive)"},
+        {99.0, 1.0, 100.0, "PBC Crossing (Negative)"},
+        {1000.0, 1002.0, 5000.0, "Large Coordinates"},
+        {0.1, 99.9, 100.0, "Small Distance across PBC"},
+        {50.0, 150.0, 100.0, "Coordinates outside primary box"} 
+    };
+
+    int num_tests = sizeof(tests) / sizeof(tests[0]);
+
+    printf("Running MINT32 Dynamic Scaling Tests...\n");
+    printf("--------------------------------------------------------------------------------\n");
+    printf("%-25s | %-15s | %-15s | %-15s | %-10s\n", "Test Name", "Reference", "MINT32", "Diff", "Status");
+    printf("--------------------------------------------------------------------------------\n");
+
+    int passed = 0;
+    for (int i = 0; i < num_tests; i++) {
+        double scale, inv_scale;
+        get_scale_factors(tests[i].box, &scale, &inv_scale);
+
+        test_mint32_dist_kernel<<<1, 1>>>(tests[i].x1, tests[i].x2, tests[i].box, scale, inv_scale, d_out_mint32, d_out_ref);
+        cudaDeviceSynchronize();
+
+        cudaMemcpy(&h_out_mint32, d_out_mint32, sizeof(double), cudaMemcpyDeviceToHost);
+        cudaMemcpy(&h_out_ref, d_out_ref, sizeof(double), cudaMemcpyDeviceToHost);
+
+        double diff = fabs(h_out_ref - h_out_mint32);
+        // Tolerance: 1 unit in int32 is approx box/2^32. For box=100, precision is ~2e-8.
+        // Let's use a safe tolerance of 1e-5 for general cases.
+        const double tolerance = 1e-5; 
+        
+        const char* status = (diff < tolerance) ? "PASS" : "FAIL";
+        if (diff < tolerance) passed++;
+
+        printf("%-25s | %15.10f | %15.10f | %15.10e | %s\n", 
+               tests[i].name, h_out_ref, h_out_mint32, diff, status);
+    }
+    printf("--------------------------------------------------------------------------------\n");
+    printf("Summary: %d/%d tests passed.\n", passed, num_tests);
+
+    cudaFree(d_out_mint32);
+    cudaFree(d_out_ref);
+    return (passed == num_tests) ? 0 : 1;
+}
diff --git "a/C:\\amber\\amber_master\\src\\pmemd\\src/extra_pnts_nb14.F90" "b/C:\\amber\\amber_mint\\src\\pmemd\\src/extra_pnts_nb14.F90"
index becfc68..bb8b412 100644
--- "a/C:\\amber\\amber_master\\src\\pmemd\\src/extra_pnts_nb14.F90"
+++ "b/C:\\amber\\amber_mint\\src\\pmemd\\src/extra_pnts_nb14.F90"
@@ -414,8 +414,10 @@ subroutine nb14_setup(num_ints, num_reals, use_atm_map)
 ! This routine can handle reallocation, and thus can be called multiple
 ! times.
 
+  ! write(6,*) 'FORTRAN_DEBUG: nb14_setup ENTRY, gbl_nb14_cnt=', gbl_nb14_cnt
+
 ! Find all diheds for which this process owns either atom:
-  
+
   cit_nb14_cnt = 0
 
   do nb14_idx = 1, gbl_nb14_cnt
diff --git "a/C:\\amber\\amber_master\\src\\pmemd\\src/pme_alltasks_setup.F90" "b/C:\\amber\\amber_mint\\src\\pmemd\\src/pme_alltasks_setup.F90"
index 70eed68..45a468d 100644
--- "a/C:\\amber\\amber_master\\src\\pmemd\\src/pme_alltasks_setup.F90"
+++ "b/C:\\amber\\amber_mint\\src\\pmemd\\src/pme_alltasks_setup.F90"
@@ -82,7 +82,6 @@ subroutine pme_alltasks_setup(num_ints, num_reals)
   integer               :: omp_get_max_threads
 #endif
 
-
 #ifdef MPI
 #else
   integer               :: use_atm_map(natom)
@@ -90,6 +89,8 @@ subroutine pme_alltasks_setup(num_ints, num_reals)
   use_atm_map(1:natom) = 0
 #endif
 
+  ! write(6,*) 'FORTRAN_DEBUG: pme_alltasks_setup ENTRY'
+
 
 #ifdef MPI
 ! Send and receive common blocks from the master node.  In the non-masters,
@@ -243,12 +244,14 @@ endif
   call bonds_setup(num_ints, num_reals, use_atm_map)
   call angles_setup(num_ints, num_reals, use_atm_map)
   call dihedrals_setup(num_ints, num_reals, use_atm_map)
-#ifdef CUDA 
+#ifdef CUDA
   if (nmropt .ne. 0) then
-    call cuda_nmr_setup()         
+    call cuda_nmr_setup()
   end if
 #endif
+  ! write(6,*) 'FORTRAN_DEBUG: pme_alltasks_setup about to call nb14_setup'
   call nb14_setup(num_ints, num_reals, use_atm_map)
+  ! write(6,*) 'FORTRAN_DEBUG: pme_alltasks_setup returned from nb14_setup'
 
   if (charmm_active) then
     call angles_ub_setup(num_ints, num_reals, use_atm_map)
diff --git "a/C:\\amber\\amber_master\\src\\pmemd\\src/runmd.F90" "b/C:\\amber\\amber_mint\\src\\pmemd\\src/runmd.F90"
index 02dcb2a..dc9dca6 100644
--- "a/C:\\amber\\amber_master\\src\\pmemd\\src/runmd.F90"
+++ "b/C:\\amber\\amber_mint\\src\\pmemd\\src/runmd.F90"
@@ -398,7 +398,7 @@ subroutine runmd(atm_cnt, crd, mass, frc, vel, last_vel)
 #endif /*MPI*/
   else
     if (.not. allocated(atm_igroup)) allocate(atm_igroup(atm_cnt))
-    call degcnt(ibelly, natom, atm_igroup, natom, gbl_bond, gbl_bond(bonda_idx-1), &
+    call degcnt(ibelly, natom, atm_igroup, natom, gbl_bond, gbl_bond(bonda_idx), &
                 ntc, rndfp, rndfs)
   end if
 
